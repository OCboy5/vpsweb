"""
VPSWeb Workflow Adapter for Web Interface Integration

This service layer bridges the existing VPSWeb translation workflow with the web interface,
providing async execution, background tasks, and repository integration.
"""

import asyncio
import logging
import time
import uuid
import threading
from typing import Dict, Any, Optional, List
from datetime import datetime, timezone
from contextlib import asynccontextmanager

from fastapi import BackgroundTasks
import os

from vpsweb.core.workflow import TranslationWorkflow, WorkflowError, StepExecutionError
from vpsweb.models.translation import TranslationInput, TranslationOutput
from vpsweb.models.config import WorkflowMode
from vpsweb.utils.config_loader import load_config
from vpsweb.utils.logger import get_logger
from vpsweb.utils.storage import StorageHandler
from vpsweb.utils.language_mapper import get_language_mapper

from .poem_service import PoemService
from .translation_service import TranslationService
from ..task_models import TaskStatusEnum
from vpsweb.repository.service import RepositoryWebService
from vpsweb.repository.schemas import TaskStatus, WorkflowTaskResult
from vpsweb.repository.database import create_session
from sqlalchemy.orm import Session
from sqlalchemy import create_engine
from sqlalchemy.pool import StaticPool

logger = get_logger(__name__)

# P0.1: Timeout configuration for workflow execution
DEFAULT_WORKFLOW_TIMEOUT = int(
    os.getenv("VPSWEB_WORKFLOW_TIMEOUT", "600")
)  # 10 minutes default
MAX_WORKFLOW_TIMEOUT = int(
    os.getenv("VPSWEB_MAX_WORKFLOW_TIMEOUT", "1800")
)  # 30 minutes maximum


class VPSWebIntegrationError(Exception):
    """Base exception for VPSWeb integration errors."""

    pass


class WorkflowExecutionError(VPSWebIntegrationError):
    """Raised when workflow execution fails."""

    pass


class ConfigurationError(VPSWebIntegrationError):
    """Raised when configuration loading fails."""

    pass


class WorkflowTimeoutError(VPSWebIntegrationError):
    """Raised when workflow execution times out."""

    pass


class VPSWebWorkflowAdapter:
    """
    Adapter service that integrates VPSWeb translation workflow with the web interface.

    This service provides:
    - Async workflow execution with background tasks
    - Repository integration for storing results
    - Error handling and logging
    - Status tracking for long-running tasks
    """

    def __init__(
        self,
        poem_service: PoemService,
        translation_service: TranslationService,
        repository_service: RepositoryWebService,
        config_path: Optional[str] = None,
    ):
        """
        Initialize the VPSWeb workflow adapter.

        Args:
            poem_service: Repository poem service
            translation_service: Repository translation service
            repository_service: Repository service for database operations
            config_path: Optional custom config directory path
        """
        self.poem_service = poem_service
        self.translation_service = translation_service
        self.repository_service = repository_service
        self.config_path = config_path

        # Initialize StorageHandler for JSON file saving (CLI-compatible)
        self.storage_handler = StorageHandler()

        # Note: No longer using in-memory _active_tasks
        # Task tracking is now handled by the database via repository_service.workflow_tasks

        # Lazy-loaded configuration and workflow
        self._config = None
        self._workflow_config = None
        self._providers_config = None

        # Language mapper for ISO code conversion
        self.language_mapper = get_language_mapper()

        logger.info("VPSWebWorkflowAdapter initialized")

    def _convert_language_code(self, lang_code: str) -> str:
        """
        Convert ISO language code to display name for VPSWeb workflow.

        Args:
            lang_code: ISO language code (e.g., 'en', 'zh-CN')

        Returns:
            Display name (e.g., 'English', 'Chinese')
        """
        display_name = self.language_mapper.get_language_name(lang_code)
        if display_name:
            return display_name

        # Fallback mapping for common codes
        fallback_mapping = {
            "en": "English",
            "zh": "Chinese",
            "zh-CN": "Chinese",
            "zh-TW": "Chinese",
            "ja": "Japanese",
            "ko": "Korean",
            "fr": "French",
            "de": "German",
            "es": "Spanish",
            "ru": "Russian",
            "ar": "Arabic",
            "hi": "Hindi",
            "pl": "Polish",
        }

        return fallback_mapping.get(lang_code, lang_code.title())

    async def _load_configuration(self):
        """
        Load VPSWeb configuration lazily.

        Raises:
            ConfigurationError: If configuration loading fails
        """
        if self._config is None:
            try:
                logger.info("Loading VPSWeb configuration...")
                self._config = load_config(self.config_path)
                self._workflow_config = self._config.main.workflow
                self._providers_config = self._config.providers

                logger.info(
                    f"Loaded workflow: {self._workflow_config.name} v{self._workflow_config.version}"
                )
                logger.info(
                    f"Available providers: {list(self._providers_config.providers.keys())}"
                )

            except Exception as e:
                logger.error(f"Failed to load VPSWeb configuration: {e}")
                raise ConfigurationError(f"Configuration loading failed: {e}")

    async def _create_workflow(
        self, workflow_mode: WorkflowMode = WorkflowMode.HYBRID
    ) -> TranslationWorkflow:
        """
        Create a new workflow instance.

        Args:
            workflow_mode: Workflow mode to use

        Returns:
            Initialized workflow instance

        Raises:
            ConfigurationError: If workflow creation fails
        """
        await self._load_configuration()

        try:
            # Create a minimal system config for the adapter
            system_config = {
                "system": {
                    "preview_lengths": {
                        "input_preview": 100,
                        "response_preview": 100,
                        "editor_preview": 200,
                    }
                }
            }

            workflow = TranslationWorkflow(
                config=self._workflow_config,
                providers_config=self._providers_config,
                workflow_mode=workflow_mode,
                system_config=system_config,
            )
            return workflow

        except Exception as e:
            logger.error(f"Failed to create workflow: {e}")
            raise ConfigurationError(f"Workflow creation failed: {e}")

    def _map_iso_to_display_language(self, iso_code: str) -> str:
        """
        Map ISO language codes to display names expected by VPSWeb TranslationInput.

        Args:
            iso_code: ISO language code (e.g., 'en', 'zh-CN')

        Returns:
            Display language name (e.g., 'English', 'Chinese')
        """
        language_mapping = {
            "en": "English",
            "en-US": "English",
            "en-GB": "English",
            "zh": "Chinese",
            "zh-CN": "Chinese",
            "zh-TW": "Chinese",
            "zh-HK": "Chinese",
            "es": "Spanish",
            "es-ES": "Spanish",
            "es-MX": "Spanish",
            "fr": "French",
            "fr-FR": "French",
            "de": "German",
            "de-DE": "German",
            "ja": "Japanese",
            "ja-JP": "Japanese",
            "ru": "Russian",
            "ru-RU": "Russian",
            "pl": "Polish",
            "pl-PL": "Polish",
            "ar": "Arabic",
            "ar-SA": "Arabic",
        }

        # Return the mapped language or default to English if not found
        return language_mapping.get(iso_code, "English")

    def _map_repository_to_workflow_input(
        self, poem_data: Dict[str, Any], source_lang: str, target_lang: str
    ) -> TranslationInput:
        """
        Map repository poem data to VPSWeb TranslationInput.

        Args:
            poem_data: Poem data from repository
            source_lang: Source language code
            target_lang: Target language code

        Returns:
            TranslationInput for VPSWeb workflow
        """
        return TranslationInput(
            original_poem=poem_data["content"],
            source_lang=self._convert_language_code(source_lang),
            target_lang=self._convert_language_code(target_lang),
        )

    def _map_workflow_output_to_repository(
        self, workflow_output: TranslationOutput, poem_id: str, workflow_mode: str
    ) -> Dict[str, Any]:
        """
        Map VPSWeb TranslationOutput to repository translation format.

        Args:
            workflow_output: Output from VPSWeb workflow
            poem_id: ID of the source poem
            workflow_mode: Workflow mode used

        Returns:
            Dictionary format for repository storage
        """
        # Debug: Log the workflow output fields
        logger.info(f"🔍 [MAPPING DEBUG] Mapping workflow output to repository format")
        logger.info(f"🔍 [MAPPING DEBUG] workflow_output.initial_translation.translated_poem_title: '{workflow_output.initial_translation.translated_poem_title}'")
        logger.info(f"🔍 [MAPPING DEBUG] workflow_output.initial_translation.translated_poet_name: '{workflow_output.initial_translation.translated_poet_name}'")
        logger.info(f"🔍 [MAPPING DEBUG] workflow_output.revised_translation.refined_translated_poem_title: '{workflow_output.revised_translation.refined_translated_poem_title}'")
        logger.info(f"🔍 [MAPPING DEBUG] workflow_output.revised_translation.refined_translated_poet_name: '{workflow_output.revised_translation.refined_translated_poet_name}'")

        result = {
            "poem_id": poem_id,
            "workflow_mode": workflow_mode,
            "source_lang": workflow_output.input.source_lang,
            "target_lang": workflow_output.input.target_lang,
            # Workflow steps
            "initial_translation": workflow_output.initial_translation.initial_translation,
            "initial_translation_notes": workflow_output.initial_translation.initial_translation_notes,
            "translated_poem_title": workflow_output.initial_translation.translated_poem_title,
            "translated_poet_name": workflow_output.initial_translation.translated_poet_name,
            "editor_suggestions": workflow_output.editor_review.editor_suggestions,
            "revised_translation": workflow_output.revised_translation.revised_translation,
            "revised_translation_notes": workflow_output.revised_translation.revised_translation_notes,
            "refined_translated_poem_title": workflow_output.revised_translation.refined_translated_poem_title,
            "refined_translated_poet_name": workflow_output.revised_translation.refined_translated_poet_name,
            # Metadata
            "total_tokens": workflow_output.total_tokens,
            "duration_seconds": workflow_output.duration_seconds,
            "total_cost": workflow_output.total_cost,
            "full_log": workflow_output.full_log,
            # Step-specific metadata
            "initial_model_info": workflow_output.initial_translation.model_info,
            "editor_model_info": workflow_output.editor_review.model_info,
            "revision_model_info": workflow_output.revised_translation.model_info,
            # Step-specific metrics
            "initial_tokens": workflow_output.initial_translation.tokens_used,
            "editor_tokens": workflow_output.editor_review.tokens_used,
            "revision_tokens": workflow_output.revised_translation.tokens_used,
            "initial_cost": getattr(workflow_output.initial_translation, "cost", None),
            "editor_cost": getattr(workflow_output.editor_review, "cost", None),
            "revision_cost": getattr(workflow_output.revised_translation, "cost", None),
            "created_at": datetime.now(timezone.utc).isoformat(),
            "updated_at": datetime.now(timezone.utc).isoformat(),
        }

        # Debug: Log what we're putting in the result
        logger.info(f"🔍 [MAPPING DEBUG] Result['translated_poem_title']: '{result.get('translated_poem_title', 'MISSING')}'")
        logger.info(f"🔍 [MAPPING DEBUG] Result['translated_poet_name']: '{result.get('translated_poet_name', 'MISSING')}'")
        logger.info(f"🔍 [MAPPING DEBUG] Result['refined_translated_poem_title']: '{result.get('refined_translated_poem_title', 'MISSING')}'")
        logger.info(f"🔍 [MAPPING DEBUG] Result['refined_translated_poet_name']: '{result.get('refined_translated_poet_name', 'MISSING')}'")

        return result

    async def _execute_workflow_with_timeout(
        self,
        workflow: TranslationWorkflow,
        input_data: TranslationInput,
        timeout: int = DEFAULT_WORKFLOW_TIMEOUT,
    ) -> TranslationOutput:
        """
        Execute VPSWeb workflow with timeout protection.

        Args:
            workflow: VPSWeb workflow instance
            input_data: Translation input data
            timeout: Timeout in seconds

        Returns:
            TranslationOutput: Workflow execution result

        Raises:
            WorkflowTimeoutError: If workflow execution times out
            ConfigurationError: If workflow configuration is invalid
        """
        try:
            logger.info(f"Executing workflow with {timeout}s timeout")
            workflow_output = await asyncio.wait_for(
                workflow.execute(input_data, show_progress=False), timeout=timeout
            )
            logger.info("Workflow execution completed successfully")
            return workflow_output
        except asyncio.TimeoutError:
            logger.error(f"Workflow execution timed out after {timeout}s")
            raise WorkflowTimeoutError(
                f"Translation workflow timed out after {timeout} seconds."
            )
        except Exception as e:
            logger.error(f"Workflow execution failed: {e}")
            raise ConfigurationError(f"Workflow execution failed: {e}")

    async def execute_translation_workflow(
        self,
        poem_id: str,
        source_lang: str,
        target_lang: str,
        workflow_mode: str = "hybrid",
        background_tasks: Optional[BackgroundTasks] = None,
        synchronous: bool = False,
    ) -> Dict[str, Any]:
        """
        Execute VPSWeb translation workflow for a poem with real-time SSE progress updates.

        Args:
            poem_id: ID of the poem to translate
            source_lang: Source language code
            target_lang: Target language code
            workflow_mode: Workflow mode (reasoning, non_reasoning, hybrid)
            background_tasks: FastAPI BackgroundTasks (kept for compatibility)
            synchronous: If True, execute synchronously and return result (not implemented)

        Returns:
            Dictionary with task info for SSE tracking

        Raises:
            WorkflowExecutionError: If workflow execution fails
            ConfigurationError: If configuration is invalid
        """
        logger.info(
            f"execute_translation_workflow called with poem_id={poem_id}, source_lang={source_lang}, target_lang={target_lang}, workflow_mode={workflow_mode}, synchronous={synchronous}"
        )

        # Validate inputs
        if not poem_id:
            raise WorkflowExecutionError("Poem ID is required")

        if source_lang == target_lang:
            raise WorkflowExecutionError(
                "Source and target languages must be different"
            )

        # Validate workflow mode
        try:
            workflow_mode_enum = WorkflowMode(workflow_mode)
        except ValueError:
            raise WorkflowExecutionError(f"Invalid workflow mode: {workflow_mode}")

        # Retrieve poem from repository
        poem = await self.poem_service.get_poem(poem_id)
        if not poem:
            raise WorkflowExecutionError(f"Poem not found: {poem_id}")

        # Create task ID and initialize in app.state (no database storage for personal use system)
        task_id = str(uuid.uuid4())
        logger.info(f"Created task ID: {task_id}")

        # Import task models for app.state initialization
        from ..task_models import TaskStatus as InMemoryTaskStatus, TaskStatusEnum

        # Get FastAPI app instance for app.state access
        import sys
        import os

        sys.path.append(os.path.dirname(os.path.dirname(__file__)))
        from ..main import app

        # Initialize task in app.state
        task_status = InMemoryTaskStatus(task_id=task_id)
        app.state.tasks[task_id] = task_status
        app.state.task_locks[task_id] = threading.Lock()

        # Execute workflows asynchronously with real-time progress callbacks
        # This enables SSE streaming of step-by-step progress updates

        # Use asyncio.create_task to run the new callback-based async function
        # This runs the task concurrently on the main event loop, allowing real-time updates.
        asyncio.create_task(
            self._execute_workflow_task_with_callback(
                task_id,
                poem_id,
                source_lang,
                target_lang,
                workflow_mode,
            )
        )
        logger.info(
            f"Asynchronous workflow task {task_id} with callback has been scheduled."
        )

        return {
            "task_id": task_id,
            "status": TaskStatusEnum.PENDING,
            "message": "Translation workflow started",
        }

    async def _execute_workflow_task_with_callback(
        self,
        task_id: str,
        poem_id: str,
        source_lang: str,
        target_lang: str,
        workflow_mode_str: str,
    ) -> None:
        """
        Asynchronous background task that uses a progress callback to provide real-time updates.
        This method executes the translation workflow step-by-step and updates the task status
        through the callback mechanism for real-time SSE streaming.

        Args:
            task_id: Workflow task ID
            poem_id: Poem ID to retrieve
            source_lang: Source language code
            target_lang: Target language code
            workflow_mode_str: Workflow mode as string
        """
        logger.info(f"Starting callback-based workflow task for task_id: {task_id}")

        # Get FastAPI app instance for app.state access
        from ..main import app

        # Get task status from app.state (should already be initialized)
        task_status = app.state.tasks.get(task_id)
        if not task_status:
            logger.error(f"Task {task_id} not found in app.state for async execution.")
            return

        # Define helper function to check if task was cancelled
        def is_cancelled():
            if task_id not in app.state.tasks:
                return True
            current_status = app.state.tasks[task_id]
            return (
                current_status.status.value == "failed"
                and "cancelled" in str(current_status.message).lower()
            )

        # Check if task was cancelled before starting
        if is_cancelled():
            print(
                f"[WORKFLOW] ❌ Task {task_id} was CANCELLED before execution started"
            )
            logger.info(f"Task {task_id} was cancelled before execution started.")
            return

        logger.info(f"✅ [CALLBACK ENTRY] Task status found for task_id: {task_id}")

        try:
            # Define the async callback function that updates the task state
            async def progress_callback(step_name: str, details: dict):
                # Skip updates if task is already completed or failed (including cancelled)
                current_task_status = app.state.tasks.get(task_id)
                if not current_task_status or current_task_status.status in [
                    "completed",
                    "failed",
                ]:
                    return

                # Skip progress updates if task was cancelled
                if (
                    current_task_status.status.value == "failed"
                    and "cancelled" in str(current_task_status.message).lower()
                ):
                    print(
                        f"[PROGRESS] Task {task_id} was cancelled, skipping progress updates"
                    )
                    return

                with app.state.task_locks[task_id]:
                    # Calculate progress percentage based on step
                    progress_map = {
                        "Initial Translation": 33,  # 1/3
                        "Editor Review": 67,  # 2/3 (updated from 66% for mathematical accuracy)
                        "Translator Revision": 100,  # 3/3
                    }

                    # Update the current step and mark it appropriately
                    step_state = details.get("status", "running")

                    if step_state == "completed":
                        current_task_status.update_step(
                            step_name=step_name,
                            step_details={"step_status": "completed", **details},
                            step_state="completed",
                        )
                        # Set progress to the current step's completion level
                        current_task_status.progress = progress_map.get(step_name, 0)
                        print(
                            f"[PROGRESS] Step '{step_name}' completed - progress: {current_task_status.progress}%"
                        )
                    elif step_state == "failed":
                        current_task_status.update_step(
                            step_name=step_name,
                            step_details={"step_status": "failed", **details},
                            step_state="failed",
                        )
                    else:  # running
                        current_task_status.update_step(
                            step_name=step_name,
                            step_details={"step_status": "running", **details},
                            step_state="running",
                        )
                        # For running steps, show progress corresponding to the previous step's completion
                        # This ensures running steps show their task completion progress, not 100%
                        if step_name == "Initial Translation":
                            # Initial Translation just started, show 0%
                            current_task_status.progress = 0
                        elif step_name == "Editor Review":
                            # Editor Review is running, show Initial Translation completion (33%)
                            current_task_status.progress = 33
                        elif step_name == "Translator Revision":
                            # Translator Revision is running, show Editor Review completion (67%)
                            current_task_status.progress = 67
                        else:
                            # Fallback to step-specific progress
                            current_task_status.progress = progress_map.get(
                                step_name, 0
                            )
                        print(
                            f"[PROGRESS] Step '{step_name}' running - progress: {current_task_status.progress}%"
                        )

                # Yield control to the event loop so the SSE stream can send the update
                await asyncio.sleep(0.01)

            with app.state.task_locks[task_id]:
                # Set task as running with the first step already initialized
                task_status.status = TaskStatusEnum.RUNNING
                task_status.started_at = datetime.now()
                task_status.current_step = "Initial Translation"
                task_status.step_details = {"provider": "AI", "step_status": "running"}
                task_status.step_states = {"Initial Translation": "running"}
                task_status.message = "Initializing workflow..."
                task_status.updated_at = datetime.now()

            # Setup workflow (poem retrieval, input creation, etc.)
            from src.vpsweb.repository.database import create_session
            from src.vpsweb.repository.service import RepositoryWebService
            from vpsweb.models.config import WorkflowMode
            from vpsweb.models.translation import TranslationInput, Language

            db = create_session()
            poem = RepositoryWebService(db).get_poem(poem_id)
            db.close()

            if not poem:
                raise ValueError(f"Poem not found: {poem_id}")

            translation_input = TranslationInput(
                original_poem=poem.original_text,
                source_lang=(
                    Language.CHINESE
                    if poem.source_language.startswith("zh")
                    else Language.ENGLISH
                ),
                target_lang=(
                    Language.ENGLISH if target_lang == "en" else Language.CHINESE
                ),
                metadata={
                    "id": poem.id,
                    "title": poem.poem_title,
                    "author": poem.poet_name,
                },
            )

            # Parse workflow mode
            try:
                workflow_mode = WorkflowMode(workflow_mode_str)
            except ValueError:
                workflow_mode = WorkflowMode.HYBRID  # fallback
                logger.warning(
                    f"Invalid workflow mode '{workflow_mode_str}', using HYBRID"
                )

            # Get workflow (recreate configuration)
            await self._load_configuration()
            system_config = {
                "system": {
                    "preview_lengths": {
                        "input_preview": 100,
                        "response_preview": 100,
                        "editor_preview": 200,
                    }
                }
            }

            workflow = TranslationWorkflow(
                config=self._workflow_config,
                providers_config=self._providers_config,
                workflow_mode=workflow_mode,
                system_config=system_config,
            )

            # CRITICAL: Assign our callback to the workflow instance
            workflow.progress_callback = progress_callback

            # Check if task was cancelled before executing workflow
            if is_cancelled():
                print(
                    f"[WORKFLOW] ❌ Task {task_id} was CANCELLED before workflow execution"
                )
                logger.info(f"Task {task_id} was cancelled before workflow execution.")
                return

            # Now, execute the workflow as a single, encapsulated unit
            # The progress callback will handle real-time SSE updates
            print(f"[WORKFLOW] Starting workflow execution for task {task_id}")

            try:
                final_result = await workflow.execute(
                    translation_input, show_progress=False
                )
            except Exception as e:
                # Check if the workflow was cancelled during execution
                if is_cancelled():
                    print(
                        f"[WORKFLOW] ❌ Task {task_id} was CANCELLED during workflow execution"
                    )
                    logger.info(
                        f"Task {task_id} was cancelled during workflow execution."
                    )
                    return
                else:
                    # Re-raise the exception if it's not due to cancellation
                    raise e

            # Final cancellation check after workflow execution
            if is_cancelled():
                print(
                    f"[WORKFLOW] ❌ Task {task_id} was CANCELLED after workflow execution"
                )
                logger.info(f"Task {task_id} was cancelled after workflow execution.")
                return

            print(f"[WORKFLOW] Workflow execution completed for task {task_id}")

            with app.state.task_locks[task_id]:
                task_status.set_completed(
                    result=final_result.to_dict(),
                    message="Workflow completed successfully.",
                )
                # Ensure final progress is 100%
                task_status.progress = 100
                print(
                    f"[TASK STATUS] Task marked as completed for task {task_id} - progress: 100%"
                )

                # Ensure all step states are properly set to completed for final UI update
                task_status.step_states = {
                    "Initial Translation": "completed",
                    "Editor Review": "completed",
                    "Translator Revision": "completed",
                }
                print(
                    f"[STEP STATES] Final step states updated: all steps marked as completed"
                )

            # Save the final result to the database
            print(f"[DB SAVE] Starting database save for task {task_id}")
            try:
                # We need to get the poem_id and other details from the original input data
                # since workflow tasks are stored in memory, not the database
                poem_id = translation_input.metadata.get("id")
                if not poem_id:
                    print(
                        f"[DB SAVE] ❌ No poem_id found in translation input metadata"
                    )
                    return

                print(f"[DB SAVE] Using poem_id from input: {poem_id}")

                # Map workflow output to repository format
                translation_data = self._map_workflow_output_to_repository(
                    final_result, poem_id, workflow_mode.value
                )
                print(f"[DB SAVE] Mapped workflow output to repository format")

                # Create database session and save translation
                db = create_session()
                repository_service = RepositoryWebService(db)

                # Get source and target languages from input and convert to proper format
                source_lang_raw = (
                    translation_input.source_lang.value
                    if hasattr(translation_input.source_lang, "value")
                    else str(translation_input.source_lang)
                )
                target_lang_raw = (
                    translation_input.target_lang.value
                    if hasattr(translation_input.target_lang, "value")
                    else str(translation_input.target_lang)
                )

                # Convert language names to ISO codes using the same mapping as the repository
                lang_map = {
                    "english": "en",
                    "chinese": "zh-CN",
                    "french": "fr",
                    "spanish": "es",
                    "german": "de",
                    "italian": "it",
                    "portuguese": "pt",
                    "korean": "ko",
                    "russian": "ru",
                    "japanese": "ja",
                }

                source_lang = lang_map.get(source_lang_raw.lower(), source_lang_raw)
                target_lang = lang_map.get(target_lang_raw.lower(), target_lang_raw)

                print(
                    f"[DB SAVE] Language codes converted: {source_lang_raw}->{source_lang}, {target_lang_raw}->{target_lang}"
                )

                print(
                    f"[DB SAVE] Creating translation with poem_id: {poem_id}, {source_lang}->{target_lang}"
                )

                # Save translation to database using RepositoryWebService
                print(f"[DB SAVE] About to create translation...")

                # Import the required schema classes
                from vpsweb.repository.schemas import TranslationCreate, TranslatorType

                # Create TranslationCreate object with proper schema
                # The final translation is stored as 'revised_translation' in the mapped data
                final_translation = translation_data.get("revised_translation", "")
                if not final_translation or len(final_translation.strip()) < 10:
                    # Fallback to initial translation if revised translation is empty
                    final_translation = translation_data.get("initial_translation", "")

                print(
                    f"[DB SAVE] Final translation length: {len(final_translation)} characters"
                )

                translation_create = TranslationCreate(
                    poem_id=poem_id,
                    source_language=source_lang,
                    target_language=target_lang,
                    translated_text=final_translation,
                    translated_poem_title=translation_data.get("refined_translated_poem_title") or translation_data.get("translated_poem_title", ""),
                    translated_poet_name=translation_data.get("refined_translated_poet_name") or translation_data.get("translated_poet_name", ""),
                    translator_type=TranslatorType.AI,  # Use proper enum value
                    translator_info="qwen-max",  # Use actual model name
                    quality_rating=translation_data.get(
                        "quality_rating", None
                    ),  # Fixed field name
                    metadata=translation_data,
                )

                print(
                    f"[DB SAVE] Created TranslationCreate object with {len(translation_create.translated_text)} characters"
                )
                saved_translation = repository_service.create_translation(
                    translation_create
                )
                print(
                    f"[DB SAVE] ✅ Translation created with ID: {saved_translation.id}"
                )

                # Save JSON file using poet-based storage (CLI-compatible)
                print(f"[DB SAVE] 📁 [STEP 9.5] Saving JSON file with poet-based storage")
                try:
                    # Use the same poet-based storage as CLI workflow
                    json_path = self.storage_handler.save_translation_with_poet_dir(
                        output=final_result,
                        poet_name=translation_input.metadata.get("author", "Unknown"),
                        workflow_mode=workflow_mode.value,
                        include_mode_tag=True
                    )
                    print(f"[DB SAVE] ✅ [STEP 9.5 SUCCESS] JSON file saved to poet directory: {json_path}")
                except Exception as json_error:
                    print(f"[DB SAVE] ⚠️ [STEP 9.5 WARNING] Failed to save JSON file: {json_error}")
                    # Continue execution - database save was successful

                db.close()
                print(f"[DB SAVE] Database connection closed")

            except Exception as db_error:
                print(
                    f"[DB SAVE] ❌ Failed to save translation to database: {db_error}"
                )
                import traceback

                traceback.print_exc()
                # Don't fail the task, just log the error

        except Exception as e:
            logger.error(
                f"Error in callback-based workflow task {task_id}: {e}", exc_info=True
            )
            if task_status:
                with app.state.task_locks[task_id]:
                    task_status.set_failed(
                        error=str(e), message="The workflow encountered an error."
                    )

        except Exception as e:
            print(
                f"❌ [STANDALONE] Error in standalone task for task_id {task_id}: {str(e)}"
            )
            import traceback

            traceback.print_exc()

            # Update error status in app.state
            try:
                if hasattr(app.state, "tasks") and task_id in app.state.tasks:
                    with app.state.task_locks[task_id]:
                        app.state.tasks[task_id].set_failed(
                            error=str(e),
                            message=f"Translation workflow failed: {str(e)}",
                        )
                        print(
                            f"❌ [STANDALONE] Error status updated in app.state for task_id: {task_id}"
                        )
            except Exception as app_state_error:
                print(
                    f"❌ [STANDALONE] Failed to update app.state error status for task_id {task_id}: {str(app_state_error)}"
                )

            # Also try to update error status in database for logging
            try:
                error_db = create_session()
                try:
                    error_repository_service = RepositoryWebService(error_db)
                    error_repository_service.update_workflow_task_status(
                        task_id, TaskStatus.FAILED, error_message=str(e)
                    )
                    error_db.commit()
                    print(
                        f"❌ [STANDALONE] Error status updated in database for task_id: {task_id}"
                    )
                finally:
                    error_db.close()
            except Exception as update_error:
                print(
                    f"❌ [STANDALONE] Failed to update database error status for task_id {task_id}: {str(update_error)}"
                )

    def _execute_workflow_task_with_new_session(
        self, task_id: str, poem: Dict[str, Any], workflow_mode: WorkflowMode
    ) -> Dict[str, Any]:
        """
        Execute workflow task in background with separate database connection.

        This prevents SQLite transaction conflicts by creating a completely separate
        database connection for the background task execution.

        Note: This is a synchronous function for FastAPI BackgroundTasks compatibility.
        It uses asyncio.run() to execute async operations internally.
        """
        print(f"🌱 [BACKGROUND ENTRY] BACKGROUND TASK STARTING for task_id: {task_id}")
        logger.info(f"Starting background task execution for task_id: {task_id}")

        # Create async function to run the actual workflow
        async def _run_async_workflow():
            # Create a separate database engine for background task with WAL mode and NullPool
            from vpsweb.repository.settings import settings
            from sqlalchemy import create_engine, text
            from sqlalchemy.orm import sessionmaker
            from sqlalchemy.pool import NullPool
            import time
            import random

            # Extract database file path and add WAL mode parameters
            db_url = settings.database_url
            if "sqlite:///" in db_url:
                # Remove existing parameters and add WAL mode for better concurrent access
                base_db_url = db_url.split("?")[0] if "?" in db_url else db_url
                wal_db_url = (
                    base_db_url + "?timeout=30&mode=rw&cache=shared&journal_mode=WAL"
                )
            else:
                wal_db_url = db_url

            # Use NullPool to prevent connection sharing issues in background tasks
            # Each database operation gets a fresh connection, avoiding locks
            background_engine = create_engine(
                wal_db_url,
                connect_args={"check_same_thread": False, "timeout": 30},
                poolclass=NullPool,  # Critical: No pooling for background tasks
                echo=False,
            )

            # Enable WAL mode and optimize for concurrent access
            with background_engine.connect() as conn:
                conn.execute(
                    text("PRAGMA journal_mode=WAL")
                )  # Enable Write-Ahead Logging
                conn.execute(
                    text("PRAGMA synchronous=NORMAL")
                )  # Balance safety/performance
                conn.execute(text("PRAGMA busy_timeout=30000"))  # 30 seconds timeout
                conn.execute(
                    text("PRAGMA foreign_keys=ON")
                )  # Enable foreign key constraints
                conn.execute(
                    text("PRAGMA temp_store=MEMORY")
                )  # Use memory for temp tables
                conn.execute(
                    text("PRAGMA mmap_size=268435456")
                )  # 256MB memory-mapped I/O
                conn.commit()

            BackgroundSessionLocal = sessionmaker(
                autocommit=False, autoflush=False, bind=background_engine
            )
            db = BackgroundSessionLocal()
            logger.info(f"Created separate database connection for background task")

            # Add retry mechanism for database operations
            def retry_database_operation(max_retries=3, base_delay=0.1):
                def decorator(func):
                    def wrapper(*args, **kwargs):
                        for attempt in range(max_retries):
                            try:
                                return func(*args, **kwargs)
                            except Exception as e:
                                if (
                                    "database is locked" in str(e).lower()
                                    and attempt < max_retries - 1
                                ):
                                    # Exponential backoff with jitter
                                    delay = base_delay * (
                                        2**attempt
                                    ) + random.uniform(0, 0.1)
                                    logger.warning(
                                        f"Database locked on attempt {attempt + 1}, retrying in {delay:.2f}s"
                                    )
                                    time.sleep(delay)
                                else:
                                    raise

                    return wrapper

                return decorator

            # Apply retry to critical database operations
            original_commit = db.commit
            original_flush = db.flush

            @retry_database_operation(max_retries=3, base_delay=0.2)
            def safe_commit():
                original_commit()

            @retry_database_operation(max_retries=2, base_delay=0.1)
            def safe_flush():
                original_flush()

            # Replace methods with retry wrappers
            db.commit = safe_commit
            db.flush = safe_flush

            try:
                # Create new services with fresh database session
                poem_service = PoemService(db)
                translation_service = TranslationService(db)
                repository_service = RepositoryWebService(db)

                # Execute workflow with fresh services
                return await self._execute_workflow_task_with_services(
                    task_id,
                    poem,
                    workflow_mode,
                    poem_service,
                    translation_service,
                    repository_service,
                )

            except Exception as e:
                logger.error(f"Background workflow task {task_id} failed: {e}")
                # Update task status to failed
                try:
                    repository_service.update_workflow_task_status(
                        task_id, TaskStatus.FAILED, error_message=str(e)
                    )
                except:
                    logger.error(f"Failed to update task status to failed: {e}")
                raise
            finally:
                # Always close database session and engine
                try:
                    db.close()
                    background_engine.dispose()
                except:
                    pass

        # Execute the async workflow using asyncio.run()
        return asyncio.run(_run_async_workflow())

    async def _execute_workflow_task(
        self, task_id: str, poem: Dict[str, Any], workflow_mode: WorkflowMode
    ) -> Dict[str, Any]:
        """
        Execute the actual workflow task.

        Args:
            task_id: Unique task identifier
            poem: Poem data from repository
            workflow_mode: Workflow mode to use

        Returns:
            Task result with translation data

        Raises:
            WorkflowExecutionError: If execution fails
        """
        logger.info(
            f"🚀 [STEP 1] Starting background workflow task execution for task_id: {task_id}"
        )

        # Get task from database
        logger.info(f"🔍 [STEP 2] Retrieving task from database for task_id: {task_id}")
        db_task = self.repository_service.get_workflow_task(task_id)
        if not db_task:
            logger.error(f"❌ [STEP 2 FAILED] Task not found in database: {task_id}")
            raise WorkflowExecutionError(f"Task not found: {task_id}")

        logger.info(
            f"✅ [STEP 2 SUCCESS] Task retrieved from database: poem_id={db_task.poem_id}, status={db_task.status}"
        )

        try:
            # Update task status in database
            logger.info(
                f"🔄 [STEP 3] Updating task status to RUNNING for task_id: {task_id}"
            )
            self.repository_service.update_workflow_task_status(
                task_id, TaskStatus.RUNNING, progress_percentage=0
            )
            logger.info(
                f"✅ [STEP 3 SUCCESS] Task status updated to RUNNING for task_id: {task_id}"
            )

            logger.info(
                f"📝 [STEP 4] Starting translation workflow task {task_id} for poem {db_task.poem_id}"
            )

            # Create workflow
            logger.info(
                f"⚙️ [STEP 5] Creating workflow instance for mode: {workflow_mode.value}"
            )
            workflow = await self._create_workflow(workflow_mode)
            logger.info(f"✅ [STEP 5 SUCCESS] Workflow instance created")

            # Prepare input
            logger.info(f"📋 [STEP 6] Preparing workflow input data")
            input_data = self._map_repository_to_workflow_input(
                poem, db_task.source_lang, db_task.target_lang
            )
            logger.info(
                f"✅ [STEP 6 SUCCESS] Workflow input prepared: {len(input_data)} fields"
            )

            # Execute workflow with timeout protection
            logger.info(
                f"🔄 [STEP 7] Executing VPSWeb translation workflow in {workflow_mode.value} mode (timeout: {DEFAULT_WORKFLOW_TIMEOUT}s)"
            )
            workflow_output = await self._execute_workflow_with_timeout(
                workflow, input_data, timeout=DEFAULT_WORKFLOW_TIMEOUT
            )
            logger.info(f"✅ [STEP 7 SUCCESS] Workflow execution completed")

            # Map output to repository format
            logger.info(f"📊 [STEP 8] Mapping workflow output to repository format")
            translation_data = self._map_workflow_output_to_repository(
                workflow_output, db_task.poem_id, db_task.workflow_mode
            )
            logger.info(f"✅ [STEP 8 SUCCESS] Output mapping completed")

            # Save to repository
            logger.info(f"💾 [STEP 9] Saving translation results to repository")
            saved_translation = await self.translation_service.create_translation(
                poem_id=db_task.poem_id,
                source_lang=db_task.source_lang,
                target_lang=db_task.target_lang,
                workflow_mode=db_task.workflow_mode,
                translation_data=translation_data,
            )
            logger.info(
                f"✅ [STEP 9 SUCCESS] Translation saved with ID: {saved_translation['id']}"
            )

            # Save JSON file using poet-based storage (CLI-compatible)
            logger.info(f"📁 [STEP 9.5] Saving JSON file with poet-based storage")
            try:
                # Get poet name from the original poem (use same method as CLI)
                poem = await self.poem_service.get_poem_by_id(db_task.poem_id)
                poet_name = poem["poet_name"] if poem else "Unknown"

                # Use the same poet-based storage as CLI workflow
                json_path = self.storage_handler.save_translation_with_poet_dir(
                    output=workflow_output,
                    poet_name=poet_name,
                    workflow_mode=db_task.workflow_mode,
                    include_mode_tag=True
                )
                logger.info(f"✅ [STEP 9.5 SUCCESS] JSON file saved to poet directory: {json_path}")
            except Exception as e:
                logger.error(f"⚠️ [STEP 9.5 WARNING] Failed to save JSON file: {e}")
                # Continue execution - database save was successful

            # Update task with result in database
            logger.info(f"🏁 [STEP 10] Updating task with final results")
            result_data = {
                "translation_id": saved_translation["id"],
                "total_tokens": workflow_output.total_tokens,
                "duration_seconds": workflow_output.duration_seconds,
                "total_cost": workflow_output.total_cost,
            }
            repository_service.set_workflow_task_result(task_id, result_data)
            repository_service.update_workflow_task_status(
                task_id, TaskStatus.COMPLETED, progress_percentage=100
            )
            logger.info(f"✅ [STEP 10 SUCCESS] Task marked as completed")

            logger.info(
                f"🎉 [WORKFLOW COMPLETE] Translation workflow task {task_id} completed successfully"
            )

            return {
                "task_id": task_id,
                "status": TaskStatus.COMPLETED,
                "translation_id": saved_translation["id"],
                "total_tokens": workflow_output.total_tokens,
                "duration_seconds": workflow_output.duration_seconds,
                "total_cost": workflow_output.total_cost,
                "message": "Translation completed successfully",
            }

        except WorkflowTimeoutError as e:
            logger.error(
                f"⏰ [TIMEOUT ERROR] VPSWeb workflow timed out for task {task_id}: {e}"
            )
            # Update task status in database
            logger.error(f"🔴 [FAILED] Updating task status to FAILED due to timeout")
            self.repository_service.update_workflow_task_status(
                task_id, TaskStatus.FAILED, progress_percentage=0, error_message=str(e)
            )

            raise WorkflowExecutionError(f"Workflow execution timed out: {e}")

        except WorkflowError as e:
            logger.error(
                f"❌ [WORKFLOW ERROR] VPSWeb workflow failed for task {task_id}: {e}"
            )
            # Update task status in database
            logger.error(
                f"🔴 [FAILED] Updating task status to FAILED due to workflow error"
            )
            self.repository_service.update_workflow_task_status(
                task_id, TaskStatus.FAILED, progress_percentage=0, error_message=str(e)
            )

            raise WorkflowExecutionError(f"Workflow execution failed: {e}")

        except Exception as e:
            logger.error(
                f"💥 [UNEXPECTED ERROR] Task {task_id} failed with unexpected error: {e}"
            )
            logger.error(
                f"🔴 [FAILED] Updating task status to FAILED due to unexpected error"
            )
            logger.error(f"Unexpected error in workflow task {task_id}: {e}")
            # Update task status in database
            self.repository_service.update_workflow_task_status(
                task_id, TaskStatus.FAILED, progress_percentage=0, error_message=str(e)
            )

            raise WorkflowExecutionError(f"Unexpected error: {e}")

    async def _execute_workflow_task_with_services(
        self,
        task_id: str,
        poem: Dict[str, Any],
        workflow_mode: WorkflowMode,
        poem_service: "PoemService",
        translation_service: "TranslationService",
        repository_service: "RepositoryWebService",
    ) -> Dict[str, Any]:
        """
        Execute workflow task with separate service instances.

        This method creates fresh service instances to avoid session conflicts.
        """
        print(
            f"🔧 [SERVICES ENTRY] _execute_workflow_task_with_services called for task_id: {task_id}"
        )
        logger.info(
            f"🔧 [SERVICES ENTRY] _execute_workflow_task_with_services called for task_id: {task_id}"
        )

        # Get task from database
        db_task = repository_service.get_workflow_task(task_id)
        if not db_task:
            raise WorkflowExecutionError(f"Task not found: {task_id}")

        try:
            # Update task status in database
            repository_service.update_workflow_task_status(
                task_id, TaskStatus.RUNNING, progress_percentage=0
            )

            logger.info(
                f"Starting translation workflow task {task_id} for poem {db_task.poem_id}"
            )

            # Create workflow
            workflow = await self._create_workflow(workflow_mode)

            # Prepare input
            input_data = self._map_repository_to_workflow_input(
                poem, db_task.source_lang, db_task.target_lang
            )

            # Execute workflow with timeout protection
            logger.info(
                f"Executing VPSWeb translation workflow in {workflow_mode.value} mode"
            )
            workflow_output = await self._execute_workflow_with_timeout(
                workflow, input_data, timeout=DEFAULT_WORKFLOW_TIMEOUT
            )

            # Map output to repository format
            translation_data = self._map_workflow_output_to_repository(
                workflow_output, db_task.poem_id, db_task.workflow_mode
            )

            # Save to repository using fresh translation service
            logger.info(f"Saving translation results to repository")
            # Filter translation_data to only include valid fields for create_translation
            filtered_translation_data = {
                k: v
                for k, v in translation_data.items()
                if k
                not in [
                    "workflow_id",
                    "poem_id",
                    "source_lang",
                    "target_lang",
                    "workflow_mode",
                ]
            }
            saved_translation = await translation_service.create_translation(
                poem_id=translation_data["poem_id"],
                source_lang=translation_data["source_lang"],
                target_lang=translation_data["target_lang"],
                workflow_mode=translation_data["workflow_mode"],
                translation_data=filtered_translation_data,
            )
            logger.info(f"✅ Translation saved to database with ID: {saved_translation['id']}")

            # Save JSON file using poet-based storage (CLI-compatible)
            logger.info(f"📁 Saving JSON file with poet-based storage")
            try:
                # Get poet name from the original poem (use same method as CLI)
                poem = await self.poem_service.get_poem_by_id(translation_data["poem_id"])
                poet_name = poem["poet_name"] if poem else "Unknown"

                # Use the same poet-based storage as CLI workflow
                json_path = self.storage_handler.save_translation_with_poet_dir(
                    output=workflow_output,
                    poet_name=poet_name,
                    workflow_mode=translation_data["workflow_mode"],
                    include_mode_tag=True
                )
                logger.info(f"✅ JSON file saved to poet directory: {json_path}")
            except Exception as e:
                logger.error(f"⚠️ Failed to save JSON file: {e}")
                # Continue execution - database save was successful

            # Update task status in database using fresh repository service
            repository_service.update_workflow_task_status(
                task_id, TaskStatus.COMPLETED, progress_percentage=100
            )

            logger.info(f"Translation workflow task {task_id} completed successfully")

            return {
                "task_id": task_id,
                "status": TaskStatus.COMPLETED,
                "translation_id": saved_translation["id"],
                "total_tokens": workflow_output.total_tokens,
                "duration_seconds": workflow_output.duration_seconds,
                "total_cost": workflow_output.total_cost,
                "message": "Translation completed successfully",
            }

        except WorkflowTimeoutError as e:
            logger.error(f"VPSWeb workflow timed out for task {task_id}: {e}")
            # Update task status in database
            repository_service.update_workflow_task_status(
                task_id, TaskStatus.FAILED, progress_percentage=0, error_message=str(e)
            )
            raise WorkflowExecutionError(f"Workflow execution timed out: {e}")

        except WorkflowError as e:
            logger.error(f"VPSWeb workflow failed for task {task_id}: {e}")
            # Update task status in database
            repository_service.update_workflow_task_status(
                task_id, TaskStatus.FAILED, progress_percentage=0, error_message=str(e)
            )
            raise WorkflowExecutionError(f"Workflow execution failed: {e}")

        except Exception as e:
            logger.error(f"Unexpected error in workflow task {task_id}: {e}")
            # Update task status in database
            repository_service.update_workflow_task_status(
                task_id, TaskStatus.FAILED, progress_percentage=0, error_message=str(e)
            )
            raise WorkflowExecutionError(f"Unexpected error: {e}")

    # Note: Task cleanup is now handled by database - no need for manual cleanup

    async def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """
        Get the status of a background task.

        Args:
            task_id: Task identifier

        Returns:
            Task status information or None if not found
        """
        # Get task from database
        db_task = self.repository_service.get_workflow_task(task_id)
        if not db_task:
            return None

        return {
            "task_id": db_task.id,
            "status": db_task.status.value,
            "poem_id": db_task.poem_id,
            "source_lang": db_task.source_lang,
            "target_lang": db_task.target_lang,
            "workflow_mode": db_task.workflow_mode.value,
            "created_at": (
                db_task.created_at.isoformat() if db_task.created_at else None
            ),
            "started_at": (
                db_task.started_at.isoformat() if db_task.started_at else None
            ),
            "completed_at": (
                db_task.completed_at.isoformat() if db_task.completed_at else None
            ),
            "progress_percentage": db_task.progress_percentage,
            "error": db_task.error_message,
            "result": db_task.result_json,
        }

    async def list_active_tasks(self, limit: int = 50) -> List[Dict[str, Any]]:
        """
        List active background tasks.

        Args:
            limit: Maximum number of tasks to return

        Returns:
            List of task information
        """
        # Get tasks from database
        db_tasks = self.repository_service.get_workflow_tasks(limit=limit)

        return [
            {
                "task_id": task.id,
                "status": task.status.value,
                "poem_id": task.poem_id,
                "source_lang": task.source_lang,
                "target_lang": task.target_lang,
                "workflow_mode": task.workflow_mode.value,
                "created_at": task.created_at.isoformat() if task.created_at else None,
                "started_at": task.started_at.isoformat() if task.started_at else None,
                "completed_at": (
                    task.completed_at.isoformat() if task.completed_at else None
                ),
                "error": task.error_message,
            }
            for task in db_tasks
        ]

    async def cancel_task(self, task_id: str) -> bool:
        """
        Cancel a background task (mark as failed).

        Args:
            task_id: Task identifier

        Returns:
            True if task was cancelled, False if not found
        """
        # Get task from database
        db_task = self.repository_service.get_workflow_task(task_id)
        if not db_task:
            return False

        if db_task.status in [TaskStatus.PENDING, TaskStatus.RUNNING]:
            # Update task status in database
            self.repository_service.update_workflow_task_status(
                task_id,
                TaskStatus.FAILED,
                progress_percentage=0,
                error_message="Task cancelled by user",
            )

            logger.info(f"Task {task_id} cancelled by user")
            return True

        return False

    async def get_workflow_modes(self) -> List[Dict[str, str]]:
        """
        Get available workflow modes.

        Returns:
            List of available workflow modes with descriptions
        """
        return [
            {
                "value": "hybrid",
                "label": "Hybrid",
                "description": "Balanced approach with reasoning for review steps",
            },
            {
                "value": "reasoning",
                "label": "Reasoning",
                "description": "Highest quality with detailed reasoning for all steps",
            },
            {
                "value": "non_reasoning",
                "label": "Non-Reasoning",
                "description": "Fast and cost-effective for standard translations",
            },
        ]

    async def validate_workflow_input(
        self, poem_id: str, source_lang: str, target_lang: str, workflow_mode: str
    ) -> Dict[str, Any]:
        """
        Validate workflow input without executing.

        Args:
            poem_id: ID of the poem to translate
            source_lang: Source language code
            target_lang: Target language code
            workflow_mode: Workflow mode to validate

        Returns:
            Validation result
        """
        try:
            # Check if poem exists
            poem = await self.poem_service.get_poem(poem_id)
            if not poem:
                return {"valid": False, "errors": [f"Poem not found: {poem_id}"]}

            # Validate languages
            if source_lang == target_lang:
                return {
                    "valid": False,
                    "errors": ["Source and target languages must be different"],
                }

            # Validate workflow mode
            try:
                WorkflowMode(workflow_mode)
            except ValueError:
                return {
                    "valid": False,
                    "errors": [f"Invalid workflow mode: {workflow_mode}"],
                }

            # Validate poem content
            if not poem.get("content") or len(poem["content"].strip()) < 10:
                return {
                    "valid": False,
                    "errors": ["Poem content is too short or empty"],
                }

            # Try to load configuration
            await self._load_configuration()

            return {
                "valid": True,
                "message": "Validation passed",
                "poem_preview": (
                    poem["content"][:100] + "..."
                    if len(poem["content"]) > 100
                    else poem["content"]
                ),
                "workflow_info": {
                    "workflow": self._workflow_config.name,
                    "version": self._workflow_config.version,
                    "providers": list(self._providers_config.providers.keys()),
                },
            }

        except Exception as e:
            return {"valid": False, "errors": [f"Validation error: {e}"]}


@asynccontextmanager
async def get_vpsweb_adapter(
    poem_service: PoemService,
    translation_service: TranslationService,
    repository_service: RepositoryWebService,
    config_path: Optional[str] = None,
):
    """
    Context manager for VPSWeb adapter instance.

    Args:
        poem_service: Repository poem service
        translation_service: Repository translation service
        repository_service: Repository service for database operations
        config_path: Optional config directory path

    Yields:
        VPSWebWorkflowAdapter instance
    """
    adapter = VPSWebWorkflowAdapter(
        poem_service=poem_service,
        translation_service=translation_service,
        repository_service=repository_service,
        config_path=config_path,
    )

    try:
        yield adapter
    finally:
        # Cleanup if needed
        pass
