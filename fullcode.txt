src/vpsweb/repository/migrations/versions/001_initial_schema.py:
<code>
"""Initial 4-table schema for v0.3.1 prototype

Revision ID: 001_initial_schema
Revises:
Create Date: 2025-01-19 12:00:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import sqlite

# revision identifiers, used by Alembic.
revision = '001_initial_schema'
down_revision = None
branch_labels = None
depends_on = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###

    # Create poems table
    op.create_table('poems',
    sa.Column('id', sa.String(length=26), nullable=False),
    sa.Column('poet_name', sa.String(length=200), nullable=False),
    sa.Column('poem_title', sa.String(length=300), nullable=False),
    sa.Column('source_language', sa.String(length=10), nullable=False),
    sa.Column('original_text', sa.Text(), nullable=False),
    sa.Column('metadata_json', sa.Text(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), nullable=False),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_poems_id'), 'poems', ['id'], unique=False)
    op.create_index(op.f('ix_poems_poet_name'), 'poems', ['poet_name'], unique=False)
    op.create_index(op.f('ix_poems_poem_title'), 'poems', ['poem_title'], unique=False)
    op.create_index(op.f('ix_poems_source_language'), 'poems', ['source_language'], unique=False)
    op.create_index('idx_poems_created_at', 'poems', ['created_at'], unique=False)

    # Create translations table
    op.create_table('translations',
    sa.Column('id', sa.String(length=26), nullable=False),
    sa.Column('poem_id', sa.String(length=26), nullable=False),
    sa.Column('translator_type', sa.String(length=10), nullable=False),
    sa.Column('translator_info', sa.String(length=200), nullable=True),
    sa.Column('target_language', sa.String(length=10), nullable=False),
    sa.Column('translated_text', sa.Text(), nullable=False),
    sa.Column('quality_rating', sa.Integer(), nullable=True),
    sa.Column('raw_path', sa.String(length=500), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['poem_id'], ['poems.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_translations_id'), 'translations', ['id'], unique=False)
    op.create_index(op.f('ix_translations_poem_id'), 'translations', ['poem_id'], unique=False)
    op.create_index(op.f('ix_translations_translator_type'), 'translations', ['translator_type'], unique=False)
    op.create_index(op.f('ix_translations_target_language'), 'translations', ['target_language'], unique=False)
    op.create_index('idx_translations_created_at', 'translations', ['created_at'], unique=False)

    # Create ai_logs table
    op.create_table('ai_logs',
    sa.Column('id', sa.String(length=26), nullable=False),
    sa.Column('translation_id', sa.String(length=26), nullable=False),
    sa.Column('model_name', sa.String(length=100), nullable=False),
    sa.Column('workflow_mode', sa.String(length=20), nullable=False),
    sa.Column('token_usage_json', sa.Text(), nullable=True),
    sa.Column('cost_info_json', sa.Text(), nullable=True),
    sa.Column('runtime_seconds', sa.Integer(), nullable=True),
    sa.Column('notes', sa.Text(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['translation_id'], ['translations.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_ai_logs_id'), 'ai_logs', ['id'], unique=False)
    op.create_index(op.f('ix_ai_logs_translation_id'), 'ai_logs', ['translation_id'], unique=False)
    op.create_index(op.f('ix_ai_logs_model_name'), 'ai_logs', ['model_name'], unique=False)
    op.create_index(op.f('ix_ai_logs_workflow_mode'), 'ai_logs', ['workflow_mode'], unique=False)
    op.create_index('idx_ai_logs_created_at', 'ai_logs', ['created_at'], unique=False)

    # Create human_notes table
    op.create_table('human_notes',
    sa.Column('id', sa.String(length=26), nullable=False),
    sa.Column('translation_id', sa.String(length=26), nullable=False),
    sa.Column('note_text', sa.Text(), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['translation_id'], ['translations.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_human_notes_id'), 'human_notes', ['id'], unique=False)
    op.create_index(op.f('ix_human_notes_translation_id'), 'human_notes', ['translation_id'], unique=False)
    op.create_index('idx_human_notes_created_at', 'human_notes', ['created_at'], unique=False)

    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index('idx_human_notes_created_at', table_name='human_notes')
    op.drop_index(op.f('ix_human_notes_translation_id'), table_name='human_notes')
    op.drop_index(op.f('ix_human_notes_id'), table_name='human_notes')
    op.drop_table('human_notes')
    op.drop_index('idx_ai_logs_created_at', table_name='ai_logs')
    op.drop_index(op.f('ix_ai_logs_workflow_mode'), table_name='ai_logs')
    op.drop_index(op.f('ix_ai_logs_model_name'), table_name='ai_logs')
    op.drop_index(op.f('ix_ai_logs_translation_id'), table_name='ai_logs')
    op.drop_index(op.f('ix_ai_logs_id'), table_name='ai_logs')
    op.drop_table('ai_logs')
    op.drop_index('idx_translations_created_at', table_name='translations')
    op.drop_index(op.f('ix_translations_target_language'), table_name='translations')
    op.drop_index(op.f('ix_translations_translator_type'), table_name='translations')
    op.drop_index(op.f('ix_translations_poem_id'), table_name='translations')
    op.drop_index(op.f('ix_translations_id'), table_name='translations')
    op.drop_table('translations')
    op.drop_index('idx_poems_created_at', table_name='poems')
    op.drop_index(op.f('ix_poems_source_language'), table_name='poems')
    op.drop_index(op.f('ix_poems_poem_title'), table_name='poems')
    op.drop_index(op.f('ix_poems_poet_name'), table_name='poems')
    op.drop_index(op.f('ix_poems_id'), table_name='poems')
    op.drop_table('poems')
    # ### end Alembic commands ###

</code>

src/vpsweb/repository/migrations/script.py.mako:
<code>
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, Sequence[str], None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    """Upgrade schema."""
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    """Downgrade schema."""
    ${downgrades if downgrades else "pass"}

</code>

src/vpsweb/repository/migrations/env.py:
<code>
from logging.config import fileConfig
import sys
import os

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

# Add repository root to path to import our modules
repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
if repo_root not in sys.path:
    sys.path.insert(0, repo_root)

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Import database configuration and models directly to avoid circular imports
sys.path.insert(0, os.path.join(repo_root, 'src', 'vpsweb', 'repository'))

# Set up minimal database configuration for Alembic
from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base
from sqlalchemy.pool import StaticPool

# Get database URL from alembic.ini
database_url = config.get_main_option("sqlalchemy.url")
engine = create_engine(
    database_url,
    connect_args={"check_same_thread": False},
    poolclass=StaticPool,
)

# Create declarative base for Alembic
Base = declarative_base()

# Now import models
from models import Poem, Translation, AILog, HumanNote

# add your model's MetaData object here
# for 'autogenerate' support
target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
        connect_args={"check_same_thread": False},  # Required for SQLite
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            render_as_batch=True,  # Enable batch mode for SQLite
            compare_type=True,
            compare_server_default=True,
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

</code>

src/vpsweb/repository/migrations/README:
<code>
Generic single-database configuration.
</code>

src/vpsweb/repository/integration/__init__.py:
<code>

</code>

src/vpsweb/repository/tests/test_simple.py:
<code>
"""
Simple unit tests for VPSWeb Repository v0.3.1

Basic tests to verify models and schemas work without circular imports
"""

import pytest
import sys
from pathlib import Path

# Add repository root to path and isolate imports
repo_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(repo_root))
sys.path.insert(0, str(Path(__file__).parent.parent))

# Import directly from current module to avoid circular imports
from models import Poem, Translation, AILog, HumanNote, Base
from schemas import (
    PoemCreate, TranslationCreate, AILogCreate, HumanNoteCreate,
    TranslatorType, WorkflowMode
)
from pydantic import ValidationError


def test_pydantic_validation():
    """Test that Pydantic validation works"""

    # Test valid poem creation
    poem_data = {
        "poet_name": "陶渊明",
        "poem_title": "歸園田居",
        "source_language": "zh",
        "original_text": "採菊東籬下，悠然見南山。山氣日夕佳，飛鳥相與還。",
        "metadata_json": '{"dynasty": "東晉"}'
    }

    poem = PoemCreate(**poem_data)
    assert poem.poet_name == "陶渊明"
    assert poem.poem_title == "歸園田居"
    assert poem.source_language == "zh"
    assert len(poem.original_text) >= 10

    # Test invalid poem (too short text)
    invalid_poem_data = poem_data.copy()
    invalid_poem_data["original_text"] = "Short"

    with pytest.raises(ValidationError):
        PoemCreate(**invalid_poem_data)

    # Test valid translation
    trans_data = {
        "poem_id": "test_poem_001",
        "translator_type": TranslatorType.AI,
        "translator_info": "gpt-4",
        "target_language": "en",
        "translated_text": "Picking chrysanthemums by the eastern fence, I calmly see the Southern Mountain.",
        "quality_rating": 5
    }

    translation = TranslationCreate(**trans_data)
    assert translation.translator_type == TranslatorType.AI
    assert translation.target_language == "en"
    assert translation.quality_rating == 5

    # Test invalid quality rating
    invalid_trans_data = trans_data.copy()
    invalid_trans_data["quality_rating"] = 6

    with pytest.raises(ValidationError):
        TranslationCreate(**invalid_trans_data)


def test_model_creation():
    """Test SQLAlchemy model creation"""

    # Create poem model
    poem = Poem(
        id="test_poem_001",
        poet_name="李白",
        poem_title="靜夜思",
        source_language="zh",
        original_text="床前明月光，疑是地上霜。舉頭望明月，低頭思故鄉。"
    )

    assert poem.id == "test_poem_001"
    assert poem.poet_name == "李白"
    assert poem.poem_title == "靜夜思"

    # Test translation model
    translation = Translation(
        id="test_trans_001",
        poem_id="test_poem_001",
        translator_type=TranslatorType.AI,
        target_language="en",
        translated_text="Before my bed, the bright moonlight shines..."
    )

    assert translation.id == "test_trans_001"
    assert translation.poem_id == "test_poem_001"
    assert translation.translator_type == TranslatorType.AI

    # Test AI log model
    ai_log = AILog(
        id="test_ai_001",
        translation_id="test_trans_001",
        model_name="gpt-4",
        workflow_mode=WorkflowMode.REASONING,
        runtime_seconds=12.5
    )

    assert ai_log.id == "test_ai_001"
    assert ai_log.model_name == "gpt-4"
    assert ai_log.workflow_mode == WorkflowMode.REASONING

    # Test human note model
    note = HumanNote(
        id="test_note_001",
        translation_id="test_trans_001",
        note_text="This is a good translation that captures the essence."
    )

    assert note.id == "test_note_001"
    assert "good translation" in note.note_text


def test_enums():
    """Test enum values"""

    # Test TranslatorType
    assert TranslatorType.AI == "ai"
    assert TranslatorType.HUMAN == "human"
    assert list(TranslatorType) == [TranslatorType.AI, TranslatorType.HUMAN]

    # Test WorkflowMode
    assert WorkflowMode.REASONING == "reasoning"
    assert WorkflowMode.NON_REASONING == "non_reasoning"
    assert WorkflowMode.HYBRID == "hybrid"
    assert list(WorkflowMode) == [WorkflowMode.REASONING, WorkflowMode.NON_REASONING, WorkflowMode.HYBRID]


def test_model_relationships():
    """Test model relationships work correctly"""

    # Create models
    poem = Poem(
        id="test_poem_rel",
        poet_name="杜甫",
        poem_title="春望",
        source_language="zh",
        original_text="國破山河在，城春草木深。"
    )

    translation = Translation(
        id="test_trans_rel",
        poem_id="test_poem_rel",
        translator_type=TranslatorType.AI,
        target_language="en",
        translated_text="The state is destroyed, but the mountains and rivers remain."
    )

    ai_log = AILog(
        id="test_ai_rel",
        translation_id="test_trans_rel",
        model_name="claude-3",
        workflow_mode=WorkflowMode.HYBRID
    )

    # Test relationships (without database)
    # These would work when objects are properly associated through a session
    assert translation.poem_id == poem.id
    assert ai_log.translation_id == translation.id


def test_model_properties():
    """Test model helper properties"""

    # Create poem
    poem = Poem(
        id="test_poem_prop",
        poet_name="王維",
        poem_title="相思",
        source_language="zh",
        original_text="紅豆生南國，春來發幾枝。"
    )

    # Test translation count properties (would be 0 without actual relationships)
    # These properties would work properly when relationships are established
    assert hasattr(poem, 'translation_count')
    assert hasattr(poem, 'ai_translation_count')
    assert hasattr(poem, 'human_translation_count')

    # Create translation
    translation = Translation(
        id="test_trans_prop",
        poem_id="test_poem_prop",
        translator_type=TranslatorType.AI,
        target_language="en",
        translated_text="Red beans grow in the southern country..."
    )

    # Test translation properties
    assert hasattr(translation, 'has_ai_logs')
    assert hasattr(translation, 'has_human_notes')

    # Create AI log
    ai_log = AILog(
        id="test_ai_prop",
        translation_id="test_trans_prop",
        model_name="gpt-4",
        workflow_mode=WorkflowMode.REASONING,
        token_usage_json='{"prompt_tokens": 100, "completion_tokens": 50, "total_tokens": 150}'
    )

    # Test AI log properties
    assert hasattr(ai_log, 'token_usage')
    assert hasattr(ai_log, 'cost_info')

    # Test token_usage parsing
    if ai_log.token_usage:
        assert ai_log.token_usage["total_tokens"] == 150


if __name__ == "__main__":
    # Run tests directly
    test_pydantic_validation()
    print("✓ Pydantic validation tests passed")

    test_model_creation()
    print("✓ Model creation tests passed")

    test_enums()
    print("✓ Enum tests passed")

    test_model_relationships()
    print("✓ Model relationship tests passed")

    test_model_properties()
    print("✓ Model property tests passed")

    print("\nAll simple tests passed! ✅")
</code>

src/vpsweb/repository/tests/__init__.py:
<code>

</code>

src/vpsweb/repository/tests/test_models.py:
<code>
"""
Unit tests for VPSWeb Repository ORM Models v0.3.1

Tests for the SQLAlchemy ORM models: Poem, Translation, AILog, HumanNote
"""

import pytest
from datetime import datetime
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.pool import StaticPool

from ..models import Poem, Translation, AILog, HumanNote, Base
from ..schemas import TranslatorType, WorkflowMode


# Test database setup
@pytest.fixture
def db_session():
    """Create a test database session"""
    engine = create_engine(
        "sqlite:///:memory:",
        connect_args={"check_same_thread": False},
        poolclass=StaticPool,
    )

    # Create all tables
    Base.metadata.create_all(bind=engine)

    # Create session
    TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    session = TestingSessionLocal()

    yield session

    session.close()


@pytest.fixture
def sample_poem():
    """Create a sample poem for testing"""
    return Poem(
        id="test_poem_001",
        poet_name="陶渊明",
        poem_title="歸園田居",
        source_language="zh",
        original_text="採菊東籬下，悠然見南山。",
        metadata_json='{"dynasty": "東晉", "theme": "田園"}'
    )


@pytest.fixture
def sample_translation(sample_poem):
    """Create a sample translation for testing"""
    return Translation(
        id="test_trans_001",
        poem_id=sample_poem.id,
        translator_type=TranslatorType.AI,
        translator_info="gpt-4",
        target_language="en",
        translated_text="Picking chrysanthemums by the eastern fence, I calmly see the Southern Mountain.",
        quality_rating=5
    )


@pytest.fixture
def sample_ai_log(sample_translation):
    """Create a sample AI log for testing"""
    return AILog(
        id="test_ai_log_001",
        translation_id=sample_translation.id,
        model_name="gpt-4",
        workflow_mode=WorkflowMode.REASONING,
        token_usage_json='{"prompt_tokens": 100, "completion_tokens": 50, "total_tokens": 150}',
        cost_info_json='{"total_cost": 0.003, "currency": "USD"}',
        runtime_seconds=12.5,
        notes="Translation completed successfully"
    )


@pytest.fixture
def sample_human_note(sample_translation):
    """Create a sample human note for testing"""
    return HumanNote(
        id="test_note_001",
        translation_id=sample_translation.id,
        note_text="This translation captures the poetic essence well. The imagery is preserved effectively."
    )


class TestPoemModel:
    """Test cases for Poem model"""

    def test_create_poem(self, db_session, sample_poem):
        """Test creating a poem"""
        db_session.add(sample_poem)
        db_session.commit()
        db_session.refresh(sample_poem)

        assert sample_poem.id == "test_poem_001"
        assert sample_poem.poet_name == "陶渊明"
        assert sample_poem.poem_title == "歸園田居"
        assert sample_poem.source_language == "zh"
        assert sample_poem.original_text == "採菊東籬下，悠然見南山。"
        assert sample_poem.metadata_json is not None
        assert sample_poem.created_at is not None
        assert sample_poem.updated_at is not None

    def test_poem_string_representation(self, sample_poem):
        """Test poem __repr__ method"""
        repr_str = repr(sample_poem)
        assert "Poem" in repr_str
        assert sample_poem.id in repr_str
        assert sample_poem.poem_title in repr_str
        assert sample_poem.poet_name in repr_str

    def test_poem_translation_count(self, db_session, sample_poem, sample_translation):
        """Test poem translation count property"""
        # Add poem and translation to database
        db_session.add(sample_poem)
        db_session.add(sample_translation)
        db_session.commit()

        # Test the relationship
        assert sample_poem.translation_count == 1
        assert sample_poem.ai_translation_count == 1
        assert sample_poem.human_translation_count == 0

    def test_poem_with_multiple_translations(self, db_session, sample_poem):
        """Test poem with multiple translations"""
        # Add poem
        db_session.add(sample_poem)
        db_session.commit()

        # Add AI translation
        ai_translation = Translation(
            id="test_ai_trans",
            poem_id=sample_poem.id,
            translator_type=TranslatorType.AI,
            translator_info="claude-3",
            target_language="en",
            translated_text="AI translation of the poem."
        )
        db_session.add(ai_translation)

        # Add human translation
        human_translation = Translation(
            id="test_human_trans",
            poem_id=sample_poem.id,
            translator_type=TranslatorType.HUMAN,
            translator_info="John Translator",
            target_language="en",
            translated_text="Human translation of the poem."
        )
        db_session.add(human_translation)
        db_session.commit()

        # Test counts
        assert sample_poem.translation_count == 2
        assert sample_poem.ai_translation_count == 1
        assert sample_poem.human_translation_count == 1


class TestTranslationModel:
    """Test cases for Translation model"""

    def test_create_translation(self, db_session, sample_poem, sample_translation):
        """Test creating a translation"""
        # Add poem first
        db_session.add(sample_poem)
        db_session.commit()

        # Add translation
        db_session.add(sample_translation)
        db_session.commit()
        db_session.refresh(sample_translation)

        assert sample_translation.id == "test_trans_001"
        assert sample_translation.poem_id == sample_poem.id
        assert sample_translation.translator_type == TranslatorType.AI
        assert sample_translation.translator_info == "gpt-4"
        assert sample_translation.target_language == "en"
        assert sample_translation.quality_rating == 5
        assert sample_translation.created_at is not None

    def test_translation_relationships(self, db_session, sample_poem, sample_translation, sample_ai_log, sample_human_note):
        """Test translation relationships with AI logs and human notes"""
        # Add all to database
        db_session.add(sample_poem)
        db_session.add(sample_translation)
        db_session.add(sample_ai_log)
        db_session.add(sample_human_note)
        db_session.commit()

        # Test relationships
        assert sample_translation.poem == sample_poem
        assert len(sample_translation.ai_logs) == 1
        assert len(sample_translation.human_notes) == 1
        assert sample_translation.ai_logs[0] == sample_ai_log
        assert sample_translation.human_notes[0] == sample_human_note

    def test_translation_properties(self, sample_translation, sample_ai_log, sample_human_note):
        """Test translation helper properties"""
        # Set up relationships manually for testing
        sample_translation.ai_logs = [sample_ai_log]
        sample_translation.human_notes = [sample_human_note]

        assert sample_translation.has_ai_logs is True
        assert sample_translation.has_human_notes is True

    def test_translation_string_representation(self, sample_translation):
        """Test translation __repr__ method"""
        repr_str = repr(sample_translation)
        assert "Translation" in repr_str
        assert sample_translation.id in repr_str
        assert str(sample_translation.translator_type) in repr_str
        assert sample_translation.target_language in repr_str


class TestAILogModel:
    """Test cases for AILog model"""

    def test_create_ai_log(self, db_session, sample_poem, sample_translation, sample_ai_log):
        """Test creating an AI log"""
        # Add poem and translation first
        db_session.add(sample_poem)
        db_session.add(sample_translation)
        db_session.commit()

        # Add AI log
        db_session.add(sample_ai_log)
        db_session.commit()
        db_session.refresh(sample_ai_log)

        assert sample_ai_log.id == "test_ai_log_001"
        assert sample_ai_log.translation_id == sample_translation.id
        assert sample_ai_log.model_name == "gpt-4"
        assert sample_ai_log.workflow_mode == WorkflowMode.REASONING
        assert sample_ai_log.runtime_seconds == 12.5
        assert sample_ai_log.created_at is not None

    def test_ai_log_relationship(self, db_session, sample_poem, sample_translation, sample_ai_log):
        """Test AI log relationship with translation"""
        # Add all to database
        db_session.add(sample_poem)
        db_session.add(sample_translation)
        db_session.add(sample_ai_log)
        db_session.commit()

        # Test relationship
        assert sample_ai_log.translation == sample_translation

    def test_ai_log_properties(self, sample_ai_log):
        """Test AI log helper properties"""
        # Test token usage parsing
        assert sample_ai_log.token_usage is not None
        assert sample_ai_log.token_usage["total_tokens"] == 150

        # Test cost info parsing
        assert sample_ai_log.cost_info is not None
        assert sample_ai_log.cost_info["total_cost"] == 0.003

    def test_ai_log_empty_properties(self):
        """Test AI log with empty JSON properties"""
        ai_log = AILog(
            id="test_empty",
            translation_id="test_trans",
            model_name="test-model",
            workflow_mode=WorkflowMode.NON_REASONING
        )

        assert ai_log.token_usage is None
        assert ai_log.cost_info is None

    def test_ai_log_string_representation(self, sample_ai_log):
        """Test AI log __repr__ method"""
        repr_str = repr(sample_ai_log)
        assert "AILog" in repr_str
        assert sample_ai_log.id in repr_str
        assert sample_ai_log.model_name in repr_str
        assert str(sample_ai_log.workflow_mode) in repr_str


class TestHumanNoteModel:
    """Test cases for HumanNote model"""

    def test_create_human_note(self, db_session, sample_poem, sample_translation, sample_human_note):
        """Test creating a human note"""
        # Add poem and translation first
        db_session.add(sample_poem)
        db_session.add(sample_translation)
        db_session.commit()

        # Add human note
        db_session.add(sample_human_note)
        db_session.commit()
        db_session.refresh(sample_human_note)

        assert sample_human_note.id == "test_note_001"
        assert sample_human_note.translation_id == sample_translation.id
        assert sample_human_note.note_text == "This translation captures the poetic essence well. The imagery is preserved effectively."
        assert sample_human_note.created_at is not None

    def test_human_note_relationship(self, db_session, sample_poem, sample_translation, sample_human_note):
        """Test human note relationship with translation"""
        # Add all to database
        db_session.add(sample_poem)
        db_session.add(sample_translation)
        db_session.add(sample_human_note)
        db_session.commit()

        # Test relationship
        assert sample_human_note.translation == sample_translation

    def test_human_note_string_representation(self, sample_human_note):
        """Test human note __repr__ method"""
        repr_str = repr(sample_human_note)
        assert "HumanNote" in repr_str
        assert sample_human_note.id in repr_str
        assert sample_human_note.translation_id in repr_str


class TestModelConstraints:
    """Test model constraints and validations"""

    def test_poem_required_fields(self):
        """Test poem required field constraints"""
        # Missing required fields should raise errors
        with pytest.raises(Exception):  # SQLAlchemy will raise IntegrityError
            poem = Poem()  # Missing all required fields
            # This would be caught at database level

    def test_translation_foreign_key_constraint(self, db_session):
        """Test translation foreign key constraint"""
        # Try to create translation with non-existent poem ID
        translation = Translation(
            id="test_invalid_fk",
            poem_id="non_existent_poem",
            translator_type=TranslatorType.AI,
            target_language="en",
            translated_text="Test translation"
        )

        with pytest.raises(Exception):  # SQLAlchemy will raise IntegrityError
            db_session.add(translation)
            db_session.commit()

    def test_quality_rating_range(self):
        """Test quality rating field constraints"""
        # This would be enforced at application level via Pydantic
        # Database level, we're using CheckConstraint in the model
        pass  # Tested in validation tests

    def test_workflow_mode_enum(self):
        """Test workflow mode enum constraints"""
        # Test valid workflow modes
        for mode in [WorkflowMode.REASONING, WorkflowMode.NON_REASONING, WorkflowMode.HYBRID]:
            ai_log = AILog(
                id=f"test_{mode.value}",
                translation_id="test_trans",
                model_name="test-model",
                workflow_mode=mode
            )
            assert ai_log.workflow_mode == mode

    def test_translator_type_enum(self):
        """Test translator type enum constraints"""
        # Test valid translator types
        for trans_type in [TranslatorType.AI, TranslatorType.HUMAN]:
            translation = Translation(
                id=f"test_{trans_type.value}",
                poem_id="test_poem",
                translator_type=trans_type,
                target_language="en",
                translated_text="Test translation"
            )
            assert translation.translator_type == trans_type
</code>

src/vpsweb/repository/tests/test_crud.py:
<code>
"""
Unit tests for VPSWeb Repository CRUD Operations v0.3.1

Tests for the CRUD operations: CRUDPoem, CRUDTranslation, CRUDAILog, CRUDHumanNote
"""

import pytest
from datetime import datetime
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.pool import StaticPool
from sqlalchemy.exc import IntegrityError

from ..models import Poem, Translation, AILog, HumanNote, Base
from ..crud import RepositoryService, CRUDPoem, CRUDTranslation, CRUDAILog, CRUDHumanNote
from ..schemas import (
    PoemCreate, PoemUpdate, TranslationCreate, TranslationUpdate,
    AILogCreate, HumanNoteCreate, TranslatorType, WorkflowMode
)


# Test database setup
@pytest.fixture
def db_session():
    """Create a test database session"""
    engine = create_engine(
        "sqlite:///:memory:",
        connect_args={"check_same_thread": False},
        poolclass=StaticPool,
    )

    # Create all tables
    Base.metadata.create_all(bind=engine)

    # Create session
    TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    session = TestingSessionLocal()

    yield session

    session.close()


@pytest.fixture
def repository_service(db_session):
    """Create repository service instance"""
    return RepositoryService(db_session)


@pytest.fixture
def poem_create_data():
    """Sample poem creation data"""
    return PoemCreate(
        poet_name="李白",
        poem_title="靜夜思",
        source_language="zh",
        original_text="床前明月光，疑是地上霜。舉頭望明月，低頭思故鄉。",
        metadata_json='{"dynasty": "唐", "theme": "思鄉"}'
    )


@pytest.fixture
def translation_create_data():
    """Sample translation creation data"""
    return TranslationCreate(
        poem_id="test_poem_id",
        translator_type=TranslatorType.AI,
        translator_info="gpt-4",
        target_language="en",
        translated_text="Before my bed, the bright moonlight shines. I wonder if it's frost on the ground. I raise my head to gaze at the bright moon, then lower it thinking of my hometown.",
        quality_rating=4
    )


@pytest.fixture
def ai_log_create_data():
    """Sample AI log creation data"""
    return AILogCreate(
        translation_id="test_translation_id",
        model_name="gpt-4",
        workflow_mode=WorkflowMode.REASONING,
        runtime_seconds=8.7,
        token_usage_json='{"prompt_tokens": 120, "completion_tokens": 80, "total_tokens": 200}',
        cost_info_json='{"total_cost": 0.004, "currency": "USD"}',
        notes="Translation completed with good quality"
    )


class TestCRUDPoem:
    """Test cases for CRUDPoem operations"""

    def test_create_poem(self, repository_service, poem_create_data):
        """Test creating a poem"""
        poem = repository_service.poems.create(poem_create_data)

        assert poem.id is not None
        assert poem.poet_name == poem_create_data.poet_name
        assert poem.poem_title == poem_create_data.poem_title
        assert poem.source_language == poem_create_data.source_language
        assert poem.original_text == poem_create_data.original_text
        assert poem.metadata_json == poem_create_data.metadata_json
        assert poem.created_at is not None
        assert poem.updated_at is not None

    def test_get_poem_by_id(self, repository_service, poem_create_data):
        """Test getting poem by ID"""
        # Create poem
        created_poem = repository_service.poems.create(poem_create_data)

        # Get poem by ID
        retrieved_poem = repository_service.poems.get_by_id(created_poem.id)

        assert retrieved_poem is not None
        assert retrieved_poem.id == created_poem.id
        assert retrieved_poem.poet_name == created_poem.poet_name
        assert retrieved_poem.poem_title == created_poem.poem_title

    def test_get_nonexistent_poem(self, repository_service):
        """Test getting non-existent poem"""
        poem = repository_service.poems.get_by_id("non_existent_id")
        assert poem is None

    def test_get_multiple_poems(self, repository_service, poem_create_data):
        """Test getting multiple poems"""
        # Create multiple poems
        poem1_data = poem_create_data.model_copy()
        poem1_data.poet_name = "李白"
        poem1_data.poem_title = "靜夜思"

        poem2_data = poem_create_data.model_copy()
        poem2_data.poet_name = "杜甫"
        poem2_data.poem_title = "春望"

        poem1 = repository_service.poems.create(poem1_data)
        poem2 = repository_service.poems.create(poem2_data)

        # Get all poems
        poems = repository_service.poems.get_multi()

        assert len(poems) == 2
        poem_titles = [p.poem_title for p in poems]
        assert "靜夜思" in poem_titles
        assert "春望" in poem_titles

    def test_filter_poems_by_poet(self, repository_service, poem_create_data):
        """Test filtering poems by poet name"""
        # Create poems by different poets
        poem1_data = poem_create_data.model_copy()
        poem1_data.poet_name = "李白"
        poem1_data.poem_title = "靜夜思"

        poem2_data = poem_create_data.model_copy()
        poem2_data.poet_name = "杜甫"
        poem2_data.poem_title = "春望"

        repository_service.poems.create(poem1_data)
        repository_service.poems.create(poem2_data)

        # Filter by poet
        li_bai_poems = repository_service.poems.get_multi(poet_name="李白")
        du_fu_poems = repository_service.poems.get_multi(poet_name="杜甫")

        assert len(li_bai_poems) == 1
        assert len(du_fu_poems) == 1
        assert li_bai_poems[0].poet_name == "李白"
        assert du_fu_poems[0].poet_name == "杜甫"

    def test_update_poem(self, repository_service, poem_create_data):
        """Test updating a poem"""
        # Create poem
        poem = repository_service.poems.create(poem_create_data)

        # Update poem
        update_data = PoemUpdate(
            poet_name="李白 (修改)",
            poem_title="靜夜思 (修改版)",
            original_text=poem.original_text + " (Additional text)"
        )

        updated_poem = repository_service.poems.update(poem.id, update_data)

        assert updated_poem is not None
        assert updated_poem.poet_name == "李白 (修改)"
        assert updated_poem.poem_title == "靜夜思 (修改版)"
        assert "Additional text" in updated_poem.original_text

    def test_update_nonexistent_poem(self, repository_service):
        """Test updating non-existent poem"""
        update_data = PoemUpdate(poet_name="Updated")
        result = repository_service.poems.update("non_existent", update_data)
        assert result is None

    def test_delete_poem(self, repository_service, poem_create_data):
        """Test deleting a poem"""
        # Create poem
        poem = repository_service.poems.create(poem_create_data)

        # Delete poem
        deleted = repository_service.poems.delete(poem.id)

        assert deleted is True

        # Verify deletion
        retrieved_poem = repository_service.poems.get_by_id(poem.id)
        assert retrieved_poem is None

    def test_delete_nonexistent_poem(self, repository_service):
        """Test deleting non-existent poem"""
        deleted = repository_service.poems.delete("non_existent")
        assert deleted is False

    def test_count_poems(self, repository_service, poem_create_data):
        """Test counting poems"""
        # Initially should be 0
        assert repository_service.poems.count() == 0

        # Create poems
        repository_service.poems.create(poem_create_data)
        assert repository_service.poems.count() == 1

        # Create another poem
        poem2_data = poem_create_data.model_copy()
        poem2_data.poem_title = "Another Poem"
        repository_service.poems.create(poem2_data)
        assert repository_service.poems.count() == 2


class TestCRUDTranslation:
    """Test cases for CRUDTranslation operations"""

    def test_create_translation(self, repository_service, poem_create_data, translation_create_data):
        """Test creating a translation"""
        # Create poem first
        poem = repository_service.poems.create(poem_create_data)

        # Update translation data with correct poem_id
        translation_create_data.poem_id = poem.id

        # Create translation
        translation = repository_service.translations.create(translation_create_data)

        assert translation.id is not None
        assert translation.poem_id == poem.id
        assert translation.translator_type == translation_create_data.translator_type
        assert translation.target_language == translation_create_data.target_language
        assert translation.translated_text == translation_create_data.translated_text
        assert translation.created_at is not None

    def test_get_translations_by_poem(self, repository_service, poem_create_data, translation_create_data):
        """Test getting translations by poem"""
        # Create poem
        poem = repository_service.poems.create(poem_create_data)

        # Create multiple translations for the poem
        translation_create_data.poem_id = poem.id
        translation_create_data.translator_type = TranslatorType.AI
        translation_create_data.translator_info = "gpt-4"
        ai_translation = repository_service.translations.create(translation_create_data)

        translation_create_data.translator_type = TranslatorType.HUMAN
        translation_create_data.translator_info = "John Translator"
        human_translation = repository_service.translations.create(translation_create_data)

        # Get translations by poem
        translations = repository_service.translations.get_by_poem(poem.id)

        assert len(translations) == 2
        assert ai_translation in translations
        assert human_translation in translations

    def test_filter_translations_by_type(self, repository_service, poem_create_data, translation_create_data):
        """Test filtering translations by type"""
        # Create poem
        poem = repository_service.poems.create(poem_create_data)

        # Create AI translation
        translation_create_data.poem_id = poem.id
        translation_create_data.translator_type = TranslatorType.AI
        repository_service.translations.create(translation_create_data)

        # Filter by translator type
        ai_translations = repository_service.translations.get_multi(translator_type=TranslatorType.AI)
        human_translations = repository_service.translations.get_multi(translator_type=TranslatorType.HUMAN)

        assert len(ai_translations) == 1
        assert len(human_translations) == 0
        assert ai_translations[0].translator_type == TranslatorType.AI

    def test_update_translation(self, repository_service, poem_create_data, translation_create_data):
        """Test updating a translation"""
        # Create poem and translation
        poem = repository_service.poems.create(poem_create_data)
        translation_create_data.poem_id = poem.id
        translation = repository_service.translations.create(translation_create_data)

        # Update translation
        update_data = TranslationUpdate(
            quality_rating=5,
            translated_text=translation.translated_text + " (Improved version)"
        )

        updated_translation = repository_service.translations.update(translation.id, update_data)

        assert updated_translation is not None
        assert updated_translation.quality_rating == 5
        assert "Improved version" in updated_translation.translated_text


class TestCRUDAILog:
    """Test cases for CRUDAILog operations"""

    def test_create_ai_log(self, repository_service, poem_create_data, translation_create_data, ai_log_create_data):
        """Test creating an AI log"""
        # Create poem and translation first
        poem = repository_service.poems.create(poem_create_data)
        translation_create_data.poem_id = poem.id
        translation = repository_service.translations.create(translation_create_data)

        # Update AI log data with correct translation_id
        ai_log_create_data.translation_id = translation.id

        # Create AI log
        ai_log = repository_service.ai_logs.create(ai_log_create_data)

        assert ai_log.id is not None
        assert ai_log.translation_id == translation.id
        assert ai_log.model_name == ai_log_create_data.model_name
        assert ai_log.workflow_mode == ai_log_create_data.workflow_mode
        assert ai_log.runtime_seconds == ai_log_create_data.runtime_seconds

    def test_get_ai_logs_by_translation(self, repository_service, poem_create_data, translation_create_data, ai_log_create_data):
        """Test getting AI logs by translation"""
        # Create poem and translation
        poem = repository_service.poems.create(poem_create_data)
        translation_create_data.poem_id = poem.id
        translation = repository_service.translations.create(translation_create_data)

        # Create multiple AI logs
        ai_log_create_data.translation_id = translation.id
        ai_log_create_data.model_name = "gpt-4"
        log1 = repository_service.ai_logs.create(ai_log_create_data)

        ai_log_create_data.model_name = "claude-3"
        log2 = repository_service.ai_logs.create(ai_log_create_data)

        # Get AI logs by translation
        logs = repository_service.ai_logs.get_by_translation(translation.id)

        assert len(logs) == 2
        assert log1 in logs
        assert log2 in logs

    def test_filter_ai_logs_by_model(self, repository_service, poem_create_data, translation_create_data, ai_log_create_data):
        """Test filtering AI logs by model"""
        # Create poem and translation
        poem = repository_service.poems.create(poem_create_data)
        translation_create_data.poem_id = poem.id
        translation = repository_service.translations.create(translation_create_data)

        # Create AI logs for different models
        ai_log_create_data.translation_id = translation.id
        ai_log_create_data.model_name = "gpt-4"
        repository_service.ai_logs.create(ai_log_create_data)

        ai_log_create_data.model_name = "claude-3"
        repository_service.ai_logs.create(ai_log_create_data)

        # Filter by model
        gpt_logs = repository_service.ai_logs.get_by_model("gpt-4")
        claude_logs = repository_service.ai_logs.get_by_model("claude-3")

        assert len(gpt_logs) == 1
        assert len(claude_logs) == 1
        assert gpt_logs[0].model_name == "gpt-4"
        assert claude_logs[0].model_name == "claude-3"


class TestCRUDHumanNote:
    """Test cases for CRUDHumanNote operations"""

    def test_create_human_note(self, repository_service, poem_create_data, translation_create_data):
        """Test creating a human note"""
        # Create poem and translation first
        poem = repository_service.poems.create(poem_create_data)
        translation_create_data.poem_id = poem.id
        translation = repository_service.translations.create(translation_create_data)

        # Create human note
        note_data = HumanNoteCreate(
            translation_id=translation.id,
            note_text="This is an excellent translation that captures the poetic essence."
        )

        note = repository_service.human_notes.create(note_data)

        assert note.id is not None
        assert note.translation_id == translation.id
        assert note.note_text == note_data.note_text
        assert note.created_at is not None

    def test_get_human_notes_by_translation(self, repository_service, poem_create_data, translation_create_data):
        """Test getting human notes by translation"""
        # Create poem and translation
        poem = repository_service.poems.create(poem_create_data)
        translation_create_data.poem_id = poem.id
        translation = repository_service.translations.create(translation_create_data)

        # Create multiple human notes
        note1_data = HumanNoteCreate(
            translation_id=translation.id,
            note_text="First note: Good translation."
        )
        note2_data = HumanNoteCreate(
            translation_id=translation.id,
            note_text="Second note: Could improve word choice."
        )

        note1 = repository_service.human_notes.create(note1_data)
        note2 = repository_service.human_notes.create(note2_data)

        # Get notes by translation
        notes = repository_service.human_notes.get_by_translation(translation.id)

        assert len(notes) == 2
        assert note1 in notes
        assert note2 in notes

    def test_delete_human_note(self, repository_service, poem_create_data, translation_create_data):
        """Test deleting a human note"""
        # Create poem and translation
        poem = repository_service.poems.create(poem_create_data)
        translation_create_data.poem_id = poem.id
        translation = repository_service.translations.create(translation_create_data)

        # Create note
        note_data = HumanNoteCreate(
            translation_id=translation.id,
            note_text="Test note for deletion."
        )
        note = repository_service.human_notes.create(note_data)

        # Delete note
        deleted = repository_service.human_notes.delete(note.id)

        assert deleted is True

        # Verify deletion
        retrieved_note = repository_service.human_notes.get_by_id(note.id)
        assert retrieved_note is None


class TestRepositoryService:
    """Test cases for RepositoryService"""

    def test_get_repository_stats(self, repository_service, poem_create_data, translation_create_data):
        """Test getting repository statistics"""
        # Initially should be empty
        stats = repository_service.get_repository_stats()
        assert stats["total_poems"] == 0
        assert stats["total_translations"] == 0

        # Create poem
        poem = repository_service.poems.create(poem_create_data)

        # Create AI translation
        translation_create_data.poem_id = poem.id
        translation_create_data.translator_type = TranslatorType.AI
        repository_service.translations.create(translation_create_data)

        # Create human translation
        translation_create_data.translator_type = TranslatorType.HUMAN
        translation_create_data.translator_info = "Human Translator"
        repository_service.translations.create(translation_create_data)

        # Get updated stats
        stats = repository_service.get_repository_stats()

        assert stats["total_poems"] == 1
        assert stats["total_translations"] == 2
        assert stats["ai_translations"] == 1
        assert stats["human_translations"] == 1
        assert "zh" in stats["languages"]
        assert stats["latest_translation"] is not None

    def test_search_poems(self, repository_service, poem_create_data):
        """Test searching poems"""
        # Create poems with different content
        poem1 = repository_service.poems.create(poem_create_data)

        poem2_data = poem_create_data.model_copy()
        poem2_data.poet_name = "杜甫"
        poem2_data.poem_title = "春望"
        poem2_data.original_text = "國破山河在，城春草木深。"
        poem2 = repository_service.poems.create(poem2_data)

        # Search by poet name
        results = repository_service.search_poems("李白")
        assert len(results) == 1
        assert results[0].id == poem1.id

        # Search by title
        results = repository_service.search_poems("春望")
        assert len(results) == 1
        assert results[0].id == poem2.id

        # Search by content
        results = repository_service.search_poems("山河")
        assert len(results) == 1
        assert results[0].id == poem2.id

    def test_get_poem_with_translations(self, repository_service, poem_create_data, translation_create_data, ai_log_create_data):
        """Test getting poem with all related data"""
        # Create poem
        poem = repository_service.poems.create(poem_create_data)

        # Create translation
        translation_create_data.poem_id = poem.id
        translation = repository_service.translations.create(translation_create_data)

        # Create AI log
        ai_log_create_data.translation_id = translation.id
        ai_log = repository_service.ai_logs.create(ai_log_create_data)

        # Create human note
        note_data = HumanNoteCreate(
            translation_id=translation.id,
            note_text="Excellent translation quality."
        )
        note = repository_service.human_notes.create(note_data)

        # Get poem with all relations
        result = repository_service.get_poem_with_translations(poem.id)

        assert result is not None
        assert result["poem"].id == poem.id
        assert len(result["translations"]) == 1

        translation_data = result["translations"][0]
        assert translation_data["translation"].id == translation.id
        assert len(translation_data["ai_logs"]) == 1
        assert len(translation_data["human_notes"]) == 1
        assert translation_data["ai_logs"][0].id == ai_log.id
        assert translation_data["human_notes"][0].id == note.id

    def test_get_poem_with_translations_not_found(self, repository_service):
        """Test getting non-existent poem with translations"""
        result = repository_service.get_poem_with_translations("non_existent")
        assert result is None
</code>

src/vpsweb/repository/generate_migration.py:
<code>
#!/usr/bin/env python3
"""
Simple script to generate initial migration for VPSWeb Repository v0.3.1
"""

import os
import sys
from pathlib import Path

# Add repository root to path
repo_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(repo_root))

# Set up minimal imports
from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base
from sqlalchemy.pool import StaticPool

# Create database engine
database_url = "sqlite:///./repository_root/repo.db"
engine = create_engine(
    database_url,
    connect_args={"check_same_thread": False},
    poolclass=StaticPool,
)

# Create declarative base
Base = declarative_base()

# Import and register models
sys.path.insert(0, os.path.join(repo_root, 'src', 'vpsweb', 'repository'))

# Import models with their columns defined inline
from sqlalchemy import Column, String, Text, DateTime, ForeignKey, JSON, Integer
from datetime import datetime
from sqlalchemy.sql import func

class Poem(Base):
    __tablename__ = "poems"

    id = Column(String(26), primary_key=True, index=True)
    poet_name = Column(String(200), nullable=False, index=True)
    poem_title = Column(String(300), nullable=False, index=True)
    source_language = Column(String(10), nullable=False, index=True)
    original_text = Column(Text, nullable=False)
    metadata_json = Column(Text, nullable=True)
    created_at = Column(DateTime, nullable=False, default=datetime.utcnow, server_default=func.now())
    updated_at = Column(DateTime, nullable=False, default=datetime.utcnow, server_default=func.now(), onupdate=datetime.utcnow)

class Translation(Base):
    __tablename__ = "translations"

    id = Column(String(26), primary_key=True, index=True)
    poem_id = Column(String(26), ForeignKey("poems.id", ondelete="CASCADE"), nullable=False, index=True)
    translator_type = Column(String(10), nullable=False, index=True)
    translator_info = Column(String(200), nullable=True)
    target_language = Column(String(10), nullable=False, index=True)
    translated_text = Column(Text, nullable=False)
    quality_rating = Column(Integer, nullable=True)
    raw_path = Column(String(500), nullable=True)
    created_at = Column(DateTime, nullable=False, default=datetime.utcnow, server_default=func.now())

class AILog(Base):
    __tablename__ = "ai_logs"

    id = Column(String(26), primary_key=True, index=True)
    translation_id = Column(String(26), ForeignKey("translations.id", ondelete="CASCADE"), nullable=False, index=True)
    model_name = Column(String(100), nullable=False, index=True)
    workflow_mode = Column(String(20), nullable=False, index=True)
    token_usage_json = Column(Text, nullable=True)
    cost_info_json = Column(Text, nullable=True)
    runtime_seconds = Column(Integer, nullable=True)
    notes = Column(Text, nullable=True)
    created_at = Column(DateTime, nullable=False, default=datetime.utcnow, server_default=func.now())

class HumanNote(Base):
    __tablename__ = "human_notes"

    id = Column(String(26), primary_key=True, index=True)
    translation_id = Column(String(26), ForeignKey("translations.id", ondelete="CASCADE"), nullable=False, index=True)
    note_text = Column(Text, nullable=False)
    created_at = Column(DateTime, nullable=False, default=datetime.utcnow, server_default=func.now())

if __name__ == "__main__":
    # Create the migration file manually
    migration_content = '''"""Initial 4-table schema for v0.3.1 prototype

Revision ID: 001_initial_schema
Revises:
Create Date: 2025-01-19 12:00:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import sqlite

# revision identifiers, used by Alembic.
revision = '001_initial_schema'
down_revision = None
branch_labels = None
depends_on = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###

    # Create poems table
    op.create_table('poems',
    sa.Column('id', sa.String(length=26), nullable=False),
    sa.Column('poet_name', sa.String(length=200), nullable=False),
    sa.Column('poem_title', sa.String(length=300), nullable=False),
    sa.Column('source_language', sa.String(length=10), nullable=False),
    sa.Column('original_text', sa.Text(), nullable=False),
    sa.Column('metadata_json', sa.Text(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), nullable=False),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_poems_id'), 'poems', ['id'], unique=False)
    op.create_index(op.f('ix_poems_poet_name'), 'poems', ['poet_name'], unique=False)
    op.create_index(op.f('ix_poems_poem_title'), 'poems', ['poem_title'], unique=False)
    op.create_index(op.f('ix_poems_source_language'), 'poems', ['source_language'], unique=False)
    op.create_index('idx_poems_created_at', 'poems', ['created_at'], unique=False)

    # Create translations table
    op.create_table('translations',
    sa.Column('id', sa.String(length=26), nullable=False),
    sa.Column('poem_id', sa.String(length=26), nullable=False),
    sa.Column('translator_type', sa.String(length=10), nullable=False),
    sa.Column('translator_info', sa.String(length=200), nullable=True),
    sa.Column('target_language', sa.String(length=10), nullable=False),
    sa.Column('translated_text', sa.Text(), nullable=False),
    sa.Column('quality_rating', sa.Integer(), nullable=True),
    sa.Column('raw_path', sa.String(length=500), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['poem_id'], ['poems.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_translations_id'), 'translations', ['id'], unique=False)
    op.create_index(op.f('ix_translations_poem_id'), 'translations', ['poem_id'], unique=False)
    op.create_index(op.f('ix_translations_translator_type'), 'translations', ['translator_type'], unique=False)
    op.create_index(op.f('ix_translations_target_language'), 'translations', ['target_language'], unique=False)
    op.create_index('idx_translations_created_at', 'translations', ['created_at'], unique=False)

    # Create ai_logs table
    op.create_table('ai_logs',
    sa.Column('id', sa.String(length=26), nullable=False),
    sa.Column('translation_id', sa.String(length=26), nullable=False),
    sa.Column('model_name', sa.String(length=100), nullable=False),
    sa.Column('workflow_mode', sa.String(length=20), nullable=False),
    sa.Column('token_usage_json', sa.Text(), nullable=True),
    sa.Column('cost_info_json', sa.Text(), nullable=True),
    sa.Column('runtime_seconds', sa.Integer(), nullable=True),
    sa.Column('notes', sa.Text(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['translation_id'], ['translations.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_ai_logs_id'), 'ai_logs', ['id'], unique=False)
    op.create_index(op.f('ix_ai_logs_translation_id'), 'ai_logs', ['translation_id'], unique=False)
    op.create_index(op.f('ix_ai_logs_model_name'), 'ai_logs', ['model_name'], unique=False)
    op.create_index(op.f('ix_ai_logs_workflow_mode'), 'ai_logs', ['workflow_mode'], unique=False)
    op.create_index('idx_ai_logs_created_at', 'ai_logs', ['created_at'], unique=False)

    # Create human_notes table
    op.create_table('human_notes',
    sa.Column('id', sa.String(length=26), nullable=False),
    sa.Column('translation_id', sa.String(length=26), nullable=False),
    sa.Column('note_text', sa.Text(), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['translation_id'], ['translations.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_human_notes_id'), 'human_notes', ['id'], unique=False)
    op.create_index(op.f('ix_human_notes_translation_id'), 'human_notes', ['translation_id'], unique=False)
    op.create_index('idx_human_notes_created_at', 'human_notes', ['created_at'], unique=False)

    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index('idx_human_notes_created_at', table_name='human_notes')
    op.drop_index(op.f('ix_human_notes_translation_id'), table_name='human_notes')
    op.drop_index(op.f('ix_human_notes_id'), table_name='human_notes')
    op.drop_table('human_notes')
    op.drop_index('idx_ai_logs_created_at', table_name='ai_logs')
    op.drop_index(op.f('ix_ai_logs_workflow_mode'), table_name='ai_logs')
    op.drop_index(op.f('ix_ai_logs_model_name'), table_name='ai_logs')
    op.drop_index(op.f('ix_ai_logs_translation_id'), table_name='ai_logs')
    op.drop_index(op.f('ix_ai_logs_id'), table_name='ai_logs')
    op.drop_table('ai_logs')
    op.drop_index('idx_translations_created_at', table_name='translations')
    op.drop_index(op.f('ix_translations_target_language'), table_name='translations')
    op.drop_index(op.f('ix_translations_translator_type'), table_name='translations')
    op.drop_index(op.f('ix_translations_poem_id'), table_name='translations')
    op.drop_index(op.f('ix_translations_id'), table_name='translations')
    op.drop_table('translations')
    op.drop_index('idx_poems_created_at', table_name='poems')
    op.drop_index(op.f('ix_poems_source_language'), table_name='poems')
    op.drop_index(op.f('ix_poems_poem_title'), table_name='poems')
    op.drop_index(op.f('ix_poems_poet_name'), table_name='poems')
    op.drop_index(op.f('ix_poems_id'), table_name='poems')
    op.drop_table('poems')
    # ### end Alembic commands ###
'''

    # Write migration file
    migrations_dir = Path(__file__).parent / "migrations" / "versions"
    migrations_dir.mkdir(parents=True, exist_ok=True)

    migration_file = migrations_dir / "001_initial_schema.py"
    with open(migration_file, 'w') as f:
        f.write(migration_content)

    print(f"Created migration file: {migration_file}")
    print("Initial 4-table schema migration generated successfully!")
</code>

src/vpsweb/repository/service.py:
<code>
"""
VPSWeb Repository Service Layer v0.3.1

Service layer that provides a clean interface between the webui and repository layers.
Handles database operations, error handling, and business logic.
"""

from typing import List, Optional, Dict, Any
from datetime import datetime
from sqlalchemy.orm import Session

from .crud import RepositoryService
from .schemas import (
    PoemCreate, PoemUpdate, PoemResponse,
    TranslationCreate, TranslationUpdate, TranslationResponse,
    AILogCreate, AILogResponse,
    HumanNoteCreate, HumanNoteResponse,
    RepositoryStats
)
from .models import Poem, Translation, AILog, HumanNote


class RepositoryWebService:
    """
    Web service layer for repository operations.
    Provides high-level methods for the web UI to interact with the repository.
    """

    def __init__(self, db: Session):
        self.db = db
        self.repo = RepositoryService(db)

    # Dashboard and Statistics Methods
    def get_dashboard_data(self) -> Dict[str, Any]:
        """Get data for the main dashboard"""
        stats = self.repo.get_repository_stats()
        recent_poems = self.repo.poems.get_multi(limit=5)
        recent_translations = self.repo.translations.get_multi(limit=5)

        return {
            "stats": stats,
            "recent_poems": [self._poem_to_response(p) for p in recent_poems],
            "recent_translations": [self._translation_to_response(t) for t in recent_translations]
        }

    # Poem Methods
    def create_poem(self, poem_data: PoemCreate) -> PoemResponse:
        """Create a new poem"""
        try:
            poem = self.repo.poems.create(poem_data)
            return self._poem_to_response(poem)
        except Exception as e:
            raise self._handle_error("Failed to create poem", e)

    def get_poem(self, poem_id: str) -> Optional[PoemResponse]:
        """Get a poem by ID"""
        poem = self.repo.poems.get_by_id(poem_id)
        if poem:
            return self._poem_to_response(poem)
        return None

    def get_poems_paginated(
        self,
        page: int = 1,
        page_size: int = 10,
        search: Optional[str] = None,
        poet: Optional[str] = None,
        language: Optional[str] = None
    ) -> Dict[str, Any]:
        """Get paginated poems with optional filtering"""
        offset = (page - 1) * page_size

        if search:
            poems = self.repo.search_poems(search, limit=page_size)
            total = len(poems)  # Note: For large datasets, this should be optimized
        else:
            poems = self.repo.poems.get_multi(
                skip=offset,
                limit=page_size,
                poet_name=poet,
                language=language
            )
            total = self.repo.poems.count()

        return {
            "poems": [self._poem_to_response(p) for p in poems],
            "pagination": {
                "current_page": page,
                "page_size": page_size,
                "total_items": total,
                "total_pages": (total + page_size - 1) // page_size,
                "has_next": offset + page_size < total,
                "has_previous": page > 1
            }
        }

    def update_poem(self, poem_id: str, poem_data: PoemUpdate) -> Optional[PoemResponse]:
        """Update a poem"""
        try:
            poem = self.repo.poems.update(poem_id, poem_data)
            if poem:
                return self._poem_to_response(poem)
            return None
        except Exception as e:
            raise self._handle_error("Failed to update poem", e)

    def delete_poem(self, poem_id: str) -> bool:
        """Delete a poem and all related data"""
        try:
            return self.repo.poems.delete(poem_id)
        except Exception as e:
            raise self._handle_error("Failed to delete poem", e)

    # Translation Methods
    def create_translation(self, translation_data: TranslationCreate) -> TranslationResponse:
        """Create a new translation"""
        try:
            # Verify poem exists
            poem = self.repo.poems.get_by_id(translation_data.poem_id)
            if not poem:
                raise ValueError(f"Poem with ID {translation_data.poem_id} not found")

            translation = self.repo.translations.create(translation_data)
            return self._translation_to_response(translation)
        except Exception as e:
            raise self._handle_error("Failed to create translation", e)

    def get_translation(self, translation_id: str) -> Optional[TranslationResponse]:
        """Get a translation by ID"""
        translation = self.repo.translations.get_by_id(translation_id)
        if translation:
            return self._translation_to_response(translation)
        return None

    def get_poem_translations(self, poem_id: str) -> List[TranslationResponse]:
        """Get all translations for a poem"""
        translations = self.repo.translations.get_by_poem(poem_id)
        return [self._translation_to_response(t) for t in translations]

    def get_translation_with_details(self, translation_id: str) -> Optional[Dict[str, Any]]:
        """Get translation with all related details (AI logs, human notes)"""
        translation = self.repo.translations.get_by_id(translation_id)
        if not translation:
            return None

        ai_logs = self.repo.ai_logs.get_by_translation(translation_id)
        human_notes = self.repo.human_notes.get_by_translation(translation_id)

        return {
            "translation": self._translation_to_response(translation),
            "ai_logs": [self._ai_log_to_response(log) for log in ai_logs],
            "human_notes": [self._human_note_to_response(note) for note in human_notes]
        }

    def update_translation(self, translation_id: str, translation_data: TranslationUpdate) -> Optional[TranslationResponse]:
        """Update a translation"""
        try:
            translation = self.repo.translations.update(translation_id, translation_data)
            if translation:
                return self._translation_to_response(translation)
            return None
        except Exception as e:
            raise self._handle_error("Failed to update translation", e)

    def delete_translation(self, translation_id: str) -> bool:
        """Delete a translation and all related data"""
        try:
            return self.repo.translations.delete(translation_id)
        except Exception as e:
            raise self._handle_error("Failed to delete translation", e)

    # AI Log Methods
    def create_ai_log(self, ai_log_data: AILogCreate) -> AILogResponse:
        """Create a new AI log entry"""
        try:
            # Verify translation exists
            translation = self.repo.translations.get_by_id(ai_log_data.translation_id)
            if not translation:
                raise ValueError(f"Translation with ID {ai_log_data.translation_id} not found")

            ai_log = self.repo.ai_logs.create(ai_log_data)
            return self._ai_log_to_response(ai_log)
        except Exception as e:
            raise self._handle_error("Failed to create AI log", e)

    def get_ai_logs_by_translation(self, translation_id: str) -> List[AILogResponse]:
        """Get all AI logs for a translation"""
        ai_logs = self.repo.ai_logs.get_by_translation(translation_id)
        return [self._ai_log_to_response(log) for log in ai_logs]

    def get_ai_logs_by_model(self, model_name: str) -> List[AILogResponse]:
        """Get all AI logs for a specific model"""
        ai_logs = self.repo.ai_logs.get_by_model(model_name)
        return [self._ai_log_to_response(log) for log in ai_logs]

    # Human Note Methods
    def create_human_note(self, note_data: HumanNoteCreate) -> HumanNoteResponse:
        """Create a new human note"""
        try:
            # Verify translation exists
            translation = self.repo.translations.get_by_id(note_data.translation_id)
            if not translation:
                raise ValueError(f"Translation with ID {note_data.translation_id} not found")

            note = self.repo.human_notes.create(note_data)
            return self._human_note_to_response(note)
        except Exception as e:
            raise self._handle_error("Failed to create human note", e)

    def get_human_notes_by_translation(self, translation_id: str) -> List[HumanNoteResponse]:
        """Get all human notes for a translation"""
        notes = self.repo.human_notes.get_by_translation(translation_id)
        return [self._human_note_to_response(note) for note in notes]

    def delete_human_note(self, note_id: str) -> bool:
        """Delete a human note"""
        try:
            return self.repo.human_notes.delete(note_id)
        except Exception as e:
            raise self._handle_error("Failed to delete human note", e)

    # Search and Comparison Methods
    def search_poems(self, query: str, limit: int = 20) -> List[PoemResponse]:
        """Search poems by content"""
        poems = self.repo.search_poems(query, limit=limit)
        return [self._poem_to_response(p) for p in poems]

    def get_comparison_view(self, poem_id: str) -> Optional[Dict[str, Any]]:
        """Get comprehensive comparison view for a poem"""
        return self.repo.get_poem_with_translations(poem_id)

    def get_repository_stats(self) -> RepositoryStats:
        """Get comprehensive repository statistics"""
        stats = self.repo.get_repository_stats()
        return RepositoryStats(**stats)

    # Helper Methods
    def _poem_to_response(self, poem: Poem) -> PoemResponse:
        """Convert poem model to response schema"""
        return PoemResponse(
            id=poem.id,
            poet_name=poem.poet_name,
            poem_title=poem.poem_title,
            source_language=poem.source_language,
            original_text=poem.original_text,
            metadata_json=poem.metadata_json,
            created_at=poem.created_at,
            updated_at=poem.updated_at,
            translation_count=poem.translation_count
        )

    def _translation_to_response(self, translation: Translation) -> TranslationResponse:
        """Convert translation model to response schema"""
        return TranslationResponse(
            id=translation.id,
            poem_id=translation.poem_id,
            translator_type=translation.translator_type,
            translator_info=translation.translator_info,
            target_language=translation.target_language,
            translated_text=translation.translated_text,
            quality_rating=translation.quality_rating,
            raw_path=translation.raw_path,
            created_at=translation.created_at
        )

    def _ai_log_to_response(self, ai_log: AILog) -> AILogResponse:
        """Convert AI log model to response schema"""
        return AILogResponse(
            id=ai_log.id,
            translation_id=ai_log.translation_id,
            model_name=ai_log.model_name,
            workflow_mode=ai_log.workflow_mode,
            token_usage_json=ai_log.token_usage_json,
            cost_info_json=ai_log.cost_info_json,
            runtime_seconds=ai_log.runtime_seconds,
            notes=ai_log.notes,
            created_at=ai_log.created_at
        )

    def _human_note_to_response(self, note: HumanNote) -> HumanNoteResponse:
        """Convert human note model to response schema"""
        return HumanNoteResponse(
            id=note.id,
            translation_id=note.translation_id,
            note_text=note.note_text,
            created_at=note.created_at
        )

    def _handle_error(self, message: str, error: Exception) -> Exception:
        """Handle and format errors consistently"""
        # Log the error here in a real application
        # For now, just wrap it with a descriptive message
        error_msg = f"{message}: {str(error)}"

        # You could create custom exception types here
        return Exception(error_msg)


# Factory function for dependency injection
def create_repository_service(db: Session) -> RepositoryWebService:
    """Create repository web service instance"""
    return RepositoryWebService(db)
</code>

src/vpsweb/repository/test_validation.py:
<code>
#!/usr/bin/env python3
"""
Test script for enhanced Pydantic validation in VPSWeb Repository v0.3.1
"""

import sys
from pathlib import Path

# Add repository root to path
repo_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(repo_root))

from pydantic import ValidationError
from schemas import (
    PoemCreate, TranslationCreate, AILogCreate, HumanNoteCreate,
    TranslatorType, WorkflowMode
)

def test_poem_validation():
    """Test poem schema validation"""
    print("=== Testing Poem Validation ===")

    # Valid poem
    try:
        poem = PoemCreate(
            poet_name="陶渊明",
            poem_title="歸園田居",
            source_language="zh",
            original_text="採菊東籬下，悠然見南山。山氣日夕佳，飛鳥相與還。"
        )
        print(f"✓ Valid poem created: {poem.poet_name} - {poem.poem_title}")
    except ValidationError as e:
        print(f"✗ Unexpected validation error: {e}")

    # Invalid poet name (empty)
    try:
        poem = PoemCreate(
            poet_name="",
            poem_title="Test Poem",
            source_language="en",
            original_text="This is a test poem with enough content."
        )
        print("✗ Should have failed with empty poet name")
    except ValidationError as e:
        print(f"✓ Correctly caught empty poet name: {e.errors()[0]['msg']}")

    # Invalid language code
    try:
        poem = PoemCreate(
            poet_name="Test Poet",
            poem_title="Test",
            source_language="x",  # Too short
            original_text="This is a test poem with enough content."
        )
        print("✗ Should have failed with invalid language code")
    except ValidationError as e:
        print(f"✓ Correctly caught invalid language code: {e.errors()[0]['msg']}")

    # Invalid original text (too short)
    try:
        poem = PoemCreate(
            poet_name="Test Poet",
            poem_title="Test",
            source_language="en",
            original_text="Short"  # Too short
        )
        print("✗ Should have failed with short original text")
    except ValidationError as e:
        print(f"✓ Correctly caught short original text: {e.errors()[0]['msg']}")

def test_translation_validation():
    """Test translation schema validation"""
    print("\n=== Testing Translation Validation ===")

    # Valid translation
    try:
        translation = TranslationCreate(
            poem_id="test_poem_id",
            translator_type=TranslatorType.AI,
            translator_info="gpt-4",
            target_language="en",
            translated_text="Picking chrysanthemums by the eastern fence, I calmly see the Southern Mountain."
        )
        print(f"✓ Valid translation created: {translation.translator_type} - {translation.target_language}")
    except ValidationError as e:
        print(f"✗ Unexpected validation error: {e}")

    # Invalid quality rating
    try:
        translation = TranslationCreate(
            poem_id="test_poem_id",
            translator_type=TranslatorType.HUMAN,
            translator_info="John Doe",
            target_language="en",
            translated_text="This is a valid translation with enough content to pass validation.",
            quality_rating=6  # Too high
        )
        print("✗ Should have failed with invalid quality rating")
    except ValidationError as e:
        print(f"✓ Correctly caught invalid quality rating: {e.errors()[0]['msg']}")

def test_ai_log_validation():
    """Test AI log schema validation"""
    print("\n=== Testing AI Log Validation ===")

    # Valid AI log
    try:
        ai_log = AILogCreate(
            translation_id="test_translation_id",
            model_name="gpt-4",
            workflow_mode=WorkflowMode.REASONING,
            runtime_seconds=45.5,
            token_usage_json='{"prompt_tokens": 100, "completion_tokens": 50, "total_tokens": 150}',
            notes="Translation completed successfully"
        )
        print(f"✓ Valid AI log created: {ai_log.model_name} - {ai_log.workflow_mode}")
    except ValidationError as e:
        print(f"✗ Unexpected validation error: {e}")

    # Invalid runtime (negative)
    try:
        ai_log = AILogCreate(
            translation_id="test_translation_id",
            model_name="gpt-4",
            workflow_mode=WorkflowMode.HYBRID,
            runtime_seconds=-10  # Negative
        )
        print("✗ Should have failed with negative runtime")
    except ValidationError as e:
        print(f"✓ Correctly caught negative runtime: {e.errors()[0]['msg']}")

    # Invalid JSON
    try:
        ai_log = AILogCreate(
            translation_id="test_translation_id",
            model_name="gpt-4",
            workflow_mode=WorkflowMode.NON_REASONING,
            token_usage_json='{"invalid": json}'  # Invalid JSON
        )
        print("✗ Should have failed with invalid JSON")
    except ValidationError as e:
        print(f"✓ Correctly caught invalid JSON: {e.errors()[0]['msg']}")

def test_human_note_validation():
    """Test human note schema validation"""
    print("\n=== Testing Human Note Validation ===")

    # Valid human note
    try:
        note = HumanNoteCreate(
            translation_id="test_translation_id",
            note_text="This translation captures the poetic essence well. Consider alternative wording for clarity."
        )
        print(f"✓ Valid human note created: {note.note_text[:50]}...")
    except ValidationError as e:
        print(f"✗ Unexpected validation error: {e}")

    # Invalid note text (too short)
    try:
        note = HumanNoteCreate(
            translation_id="test_translation_id",
            note_text="Hi"  # Too short
        )
        print("✗ Should have failed with short note text")
    except ValidationError as e:
        print(f"✓ Correctly caught short note text: {e.errors()[0]['msg']}")

def main():
    """Run all validation tests"""
    print("Testing Enhanced Pydantic Schemas for VPSWeb Repository v0.3.1")
    print("=" * 60)

    test_poem_validation()
    test_translation_validation()
    test_ai_log_validation()
    test_human_note_validation()

    print("\n" + "=" * 60)
    print("Validation testing completed!")

if __name__ == "__main__":
    main()
</code>

src/vpsweb/repository/models.py:
<code>
"""
VPSWeb Repository ORM Models v0.3.1

SQLAlchemy ORM models for the 4-table database schema.
Defines Poem, Translation, AILog, and HumanNote models with relationships.
"""

from datetime import datetime
from typing import List, Optional
from sqlalchemy import (
    Column, String, Text, DateTime, ForeignKey,
    JSON, Integer, CheckConstraint, Index
)
from sqlalchemy.orm import relationship, Mapped, mapped_column
from sqlalchemy.sql import func
from .database import Base


class Poem(Base):
    """Poem model representing original poetry content"""

    __tablename__ = "poems"

    # Primary key
    id: Mapped[str] = mapped_column(String(26), primary_key=True, index=True)

    # Core fields
    poet_name: Mapped[str] = mapped_column(String(200), nullable=False, index=True)
    poem_title: Mapped[str] = mapped_column(String(300), nullable=False, index=True)
    source_language: Mapped[str] = mapped_column(String(10), nullable=False, index=True)
    original_text: Mapped[str] = mapped_column(Text, nullable=False)

    # Optional metadata
    metadata_json: Mapped[Optional[str]] = mapped_column(Text, nullable=True)

    # Timestamps
    created_at: Mapped[datetime] = mapped_column(
        DateTime,
        nullable=False,
        default=datetime.utcnow,
        server_default=func.now()
    )
    updated_at: Mapped[datetime] = mapped_column(
        DateTime,
        nullable=False,
        default=datetime.utcnow,
        server_default=func.now(),
        onupdate=datetime.utcnow
    )

    # Relationships
    translations: Mapped[List["Translation"]] = relationship(
        "Translation",
        back_populates="poem",
        cascade="all, delete-orphan",
        lazy="dynamic"
    )

    # Indexes for better query performance
    __table_args__ = (
        Index('idx_poems_created_at', 'created_at'),
        Index('idx_poems_poet_name', 'poet_name'),
        Index('idx_poems_title', 'poem_title'),
        Index('idx_poems_language', 'source_language'),
    )

    def __repr__(self) -> str:
        return f"Poem(id={self.id}, title='{self.poem_title}', poet='{self.poet_name}')"

    @property
    def translation_count(self) -> int:
        """Get the number of translations for this poem"""
        return self.translations.count()

    @property
    def ai_translation_count(self) -> int:
        """Get the number of AI translations"""
        return self.translations.filter_by(translator_type='ai').count()

    @property
    def human_translation_count(self) -> int:
        """Get the number of human translations"""
        return self.translations.filter_by(translator_type='human').count()


class Translation(Base):
    """Translation model representing translated poetry content"""

    __tablename__ = "translations"

    # Primary key
    id: Mapped[str] = mapped_column(String(26), primary_key=True, index=True)

    # Foreign key to Poem
    poem_id: Mapped[str] = mapped_column(
        String(26),
        ForeignKey("poems.id", ondelete="CASCADE"),
        nullable=False,
        index=True
    )

    # Translation metadata
    translator_type: Mapped[str] = mapped_column(
        String(10),
        nullable=False,
        index=True
    )  # 'ai' or 'human'
    translator_info: Mapped[Optional[str]] = mapped_column(String(200), nullable=True)
    target_language: Mapped[str] = mapped_column(String(10), nullable=False, index=True)
    translated_text: Mapped[str] = mapped_column(Text, nullable=False)

    # Optional fields
    quality_rating: Mapped[Optional[int]] = mapped_column(
        Integer,
        CheckConstraint('quality_rating >= 1 AND quality_rating <= 5'),
        nullable=True
    )
    raw_path: Mapped[Optional[str]] = mapped_column(String(500), nullable=True)

    # Timestamp
    created_at: Mapped[datetime] = mapped_column(
        DateTime,
        nullable=False,
        default=datetime.utcnow,
        server_default=func.now()
    )

    # Relationships
    poem: Mapped["Poem"] = relationship("Poem", back_populates="translations")
    ai_logs: Mapped[List["AILog"]] = relationship(
        "AILog",
        back_populates="translation",
        cascade="all, delete-orphan"
    )
    human_notes: Mapped[List["HumanNote"]] = relationship(
        "HumanNote",
        back_populates="translation",
        cascade="all, delete-orphan"
    )

    # Indexes
    __table_args__ = (
        Index('idx_translations_poem_id', 'poem_id'),
        Index('idx_translations_type', 'translator_type'),
        Index('idx_translations_language', 'target_language'),
        Index('idx_translations_created_at', 'created_at'),
        CheckConstraint(
            "translator_type IN ('ai', 'human')",
            name='ck_translator_type'
        ),
    )

    def __repr__(self) -> str:
        return f"Translation(id={self.id}, type={self.translator_type}, language={self.target_language})"

    @property
    def has_ai_logs(self) -> bool:
        """Check if this translation has AI logs"""
        return len(self.ai_logs) > 0

    @property
    def has_human_notes(self) -> bool:
        """Check if this translation has human notes"""
        return len(self.human_notes) > 0


class AILog(Base):
    """AILog model tracking AI translation execution details"""

    __tablename__ = "ai_logs"

    # Primary key
    id: Mapped[str] = mapped_column(String(26), primary_key=True, index=True)

    # Foreign key to Translation
    translation_id: Mapped[str] = mapped_column(
        String(26),
        ForeignKey("translations.id", ondelete="CASCADE"),
        nullable=False,
        index=True
    )

    # AI execution details
    model_name: Mapped[str] = mapped_column(String(100), nullable=False, index=True)
    workflow_mode: Mapped[str] = mapped_column(
        String(20),
        nullable=False,
        index=True
    )  # 'reasoning', 'non_reasoning', 'hybrid'

    # Performance and usage data
    token_usage_json: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
    cost_info_json: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
    runtime_seconds: Mapped[Optional[float]] = mapped_column(
        nullable=True
    )

    # Additional information
    notes: Mapped[Optional[str]] = mapped_column(Text, nullable=True)

    # Timestamp
    created_at: Mapped[datetime] = mapped_column(
        DateTime,
        nullable=False,
        default=datetime.utcnow,
        server_default=func.now()
    )

    # Relationships
    translation: Mapped["Translation"] = relationship(
        "Translation",
        back_populates="ai_logs"
    )

    # Indexes
    __table_args__ = (
        Index('idx_ai_logs_translation_id', 'translation_id'),
        Index('idx_ai_logs_model_name', 'model_name'),
        Index('idx_ai_logs_workflow_mode', 'workflow_mode'),
        Index('idx_ai_logs_created_at', 'created_at'),
        CheckConstraint(
            "workflow_mode IN ('reasoning', 'non_reasoning', 'hybrid')",
            name='ck_workflow_mode'
        ),
    )

    def __repr__(self) -> str:
        return f"AILog(id={self.id}, model={self.model_name}, mode={self.workflow_mode})"

    @property
    def token_usage(self) -> Optional[dict]:
        """Parse token usage JSON if available"""
        if self.token_usage_json:
            import json
            return json.loads(self.token_usage_json)
        return None

    @property
    def cost_info(self) -> Optional[dict]:
        """Parse cost info JSON if available"""
        if self.cost_info_json:
            import json
            return json.loads(self.cost_info_json)
        return None


class HumanNote(Base):
    """HumanNote model for annotations on human translations"""

    __tablename__ = "human_notes"

    # Primary key
    id: Mapped[str] = mapped_column(String(26), primary_key=True, index=True)

    # Foreign key to Translation
    translation_id: Mapped[str] = mapped_column(
        String(26),
        ForeignKey("translations.id", ondelete="CASCADE"),
        nullable=False,
        index=True
    )

    # Note content
    note_text: Mapped[str] = mapped_column(Text, nullable=False)

    # Timestamp
    created_at: Mapped[datetime] = mapped_column(
        DateTime,
        nullable=False,
        default=datetime.utcnow,
        server_default=func.now()
    )

    # Relationships
    translation: Mapped["Translation"] = relationship(
        "Translation",
        back_populates="human_notes"
    )

    # Indexes
    __table_args__ = (
        Index('idx_human_notes_translation_id', 'translation_id'),
        Index('idx_human_notes_created_at', 'created_at'),
    )

    def __repr__(self) -> str:
        return f"HumanNote(id={self.id}, translation_id={self.translation_id})"
</code>

src/vpsweb/repository/run_tests.py:
<code>
#!/usr/bin/env python3
"""
Isolated test runner for VPSWeb Repository v0.3.1
"""

import sys
import os
from pathlib import Path

# Add repository root to path
repo_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(repo_root))

# Change to repository directory for proper package imports
os.chdir(Path(__file__).parent)

def test_imports():
    """Test that we can import all modules correctly"""
    try:
        # Test direct imports
        from models import Poem, Translation, AILog, HumanNote, Base
        from schemas import PoemCreate, TranslationCreate, AILogCreate, HumanNoteCreate
        from schemas import TranslatorType, WorkflowMode
        from pydantic import ValidationError
        print("✓ All imports successful")
        return True
    except ImportError as e:
        print(f"✗ Import error: {e}")
        return False

def test_basic_models():
    """Test basic model creation without database"""
    try:
        from models import Poem, Translation, AILog, HumanNote

        # Test poem model
        poem = Poem(
            id="test_poem",
            poet_name="李白",
            poem_title="靜夜思",
            source_language="zh",
            original_text="床前明月光，疑是地上霜。舉頭望明月，低頭思故鄉。"
        )
        assert poem.poet_name == "李白"
        assert poem.poem_title == "靜夜思"
        print("✓ Poem model creation works")

        # Test translation model
        translation = Translation(
            id="test_trans",
            poem_id="test_poem",
            translator_type="ai",
            target_language="en",
            translated_text="Before my bed, the bright moonlight shines..."
        )
        assert translation.translator_type == "ai"
        print("✓ Translation model creation works")

        # Test AI log model
        ai_log = AILog(
            id="test_ai",
            translation_id="test_trans",
            model_name="gpt-4",
            workflow_mode="reasoning"
        )
        assert ai_log.model_name == "gpt-4"
        print("✓ AI log model creation works")

        # Test human note model
        note = HumanNote(
            id="test_note",
            translation_id="test_trans",
            note_text="This is a good translation."
        )
        assert "good translation" in note.note_text
        print("✓ Human note model creation works")

        return True
    except Exception as e:
        print(f"✗ Model creation error: {e}")
        return False

def test_pydantic_schemas():
    """Test Pydantic schema validation"""
    try:
        from schemas import PoemCreate, TranslationCreate, AILogCreate, HumanNoteCreate
        from schemas import TranslatorType, WorkflowMode
        from pydantic import ValidationError

        # Test valid poem
        poem = PoemCreate(
            poet_name="陶渊明",
            poem_title="歸園田居",
            source_language="zh",
            original_text="採菊東籬下，悠然見南山。山氣日夕佳，飛鳥相與還。"
        )
        assert poem.poet_name == "陶渊明"
        print("✓ Valid poem schema works")

        # Test invalid poem (too short)
        try:
            invalid_poem = PoemCreate(
                poet_name="Test",
                poem_title="Test",
                source_language="en",
                original_text="Short"  # Too short
            )
            print("✗ Should have failed with short text")
            return False
        except ValidationError:
            print("✓ Invalid poem correctly rejected")

        # Test valid translation
        translation = TranslationCreate(
            poem_id="test_poem",
            translator_type=TranslatorType.AI,
            translator_info="gpt-4",
            target_language="en",
            translated_text="This is a valid translation with enough content to pass validation.",
            quality_rating=4
        )
        assert translation.translator_type == TranslatorType.AI
        print("✓ Valid translation schema works")

        # Test invalid quality rating
        try:
            invalid_trans = TranslationCreate(
                poem_id="test_poem",
                translator_type=TranslatorType.HUMAN,
                target_language="en",
                translated_text="This is a valid translation.",
                quality_rating=6  # Too high
            )
            print("✗ Should have failed with invalid quality rating")
            return False
        except ValidationError:
            print("✓ Invalid quality rating correctly rejected")

        return True
    except Exception as e:
        print(f"✗ Schema validation error: {e}")
        return False

def test_database_connection():
    """Test database connection and table creation"""
    try:
        from database import engine, create_session, check_db_connection
        from models import Base

        # Test database connection
        if check_db_connection():
            print("✓ Database connection successful")
        else:
            print("✗ Database connection failed")
            return False

        # Test session creation
        session = create_session()
        try:
            # Test simple query
            result = session.execute("SELECT 1").scalar()
            assert result == 1
            print("✓ Database session works")
        finally:
            session.close()

        return True
    except Exception as e:
        print(f"✗ Database connection error: {e}")
        return False

def main():
    """Run all tests"""
    print("Running VPSWeb Repository v0.3.1 Isolated Tests")
    print("=" * 50)

    tests = [
        ("Import Tests", test_imports),
        ("Model Tests", test_basic_models),
        ("Schema Tests", test_pydantic_schemas),
        ("Database Tests", test_database_connection)
    ]

    passed = 0
    total = len(tests)

    for test_name, test_func in tests:
        print(f"\n--- {test_name} ---")
        if test_func():
            passed += 1
        else:
            print(f"FAILED: {test_name}")

    print("\n" + "=" * 50)
    print(f"Test Results: {passed}/{total} passed")

    if passed == total:
        print("🎉 All tests passed!")
        return 0
    else:
        print("❌ Some tests failed!")
        return 1

if __name__ == "__main__":
    exit(main())
</code>

src/vpsweb/repository/demo_service.py:
<code>
#!/usr/bin/env python3
"""
Demo script for VPSWeb Repository Service Layer v0.3.1

Demonstrates the usage of the repository service layer with sample data.
"""

import sys
import os
from pathlib import Path

# Add repository root to path
repo_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(repo_root))

# Change to repository directory for proper package imports
os.chdir(Path(__file__).parent)

from database import create_session, init_db
from service import create_repository_service
from schemas import (
    PoemCreate, TranslationCreate, AILogCreate, HumanNoteCreate,
    TranslatorType, WorkflowMode
)


def demo_repository_service():
    """Demonstrate repository service functionality"""
    print("🎭 VPSWeb Repository Service Layer Demo v0.3.1")
    print("=" * 50)

    # Initialize database
    print("\n📊 Initializing database...")
    init_db()
    session = create_session()
    service = create_repository_service(session)
    print("✓ Database initialized")

    # Create sample poems
    print("\n📝 Creating sample poems...")
    poem1_data = PoemCreate(
        poet_name="陶渊明",
        poem_title="歸園田居·其一",
        source_language="zh",
        original_text="少無適俗韻，性本愛丘山。誤落塵網中，一去三十年。羈鳥戀舊林，池魚思故淵。開荒南野際，守拙歸園田。方宅十餘畝，草屋八九間。榆柳蔭後簷，桃李羅堂前。曖曖遠人村，依依墟里煙。狗吠深巷中，雞鳴桑樹顛。戶庭無塵雜，虛室有餘閒。久在樊籠裡，復得返自然。",
        metadata_json='{"dynasty": "東晉", "theme": "田園", "form": "五言詩"}'
    )

    poem2_data = PoemCreate(
        poet_name="李白",
        poem_title="靜夜思",
        source_language="zh",
        original_text="床前明月光，疑是地上霜。舉頭望明月，低頭思故鄉。",
        metadata_json='{"dynasty": "唐", "theme": "思鄉", "form": "五言絕句"}'
    )

    poem1 = service.create_poem(poem1_data)
    poem2 = service.create_poem(poem2_data)
    print(f"✓ Created poems: {poem1.poem_title}, {poem2.poem_title}")

    # Create translations
    print("\n🌍 Creating translations...")
    trans1_data = TranslationCreate(
        poem_id=poem1.id,
        translator_type=TranslatorType.AI,
        translator_info="gpt-4",
        target_language="en",
        translated_text="From youth I had no taste for common ways, my nature was to love hills and mountains. By mistake I fell into the worldly net, and for thirty years was gone. A caged bird longs for its old forest, a pond fish thinks of its former deep. I open wasteland at the southern wilds, keeping my simplicity and returning to garden and field. A homestead of ten-plus acres, thatched cottage of eight or nine rooms. Elms and willows shade the back eaves, peach and plum are arrayed before the hall. Faintly visible distant villages, lingering smoke from deserted courtyards. A dog barks in the deep lane, a rooster crows atop the mulberry tree. Courtyard and gate have no dust or clutter, the empty room has ample leisure. Long having been in a cage, I return again to nature.",
        quality_rating=4
    )

    trans2_data = TranslationCreate(
        poem_id=poem2.id,
        translator_type=TranslatorType.HUMAN,
        translator_info="David Hinton",
        target_language="en",
        translated_text="A splash of white on the floor before my bed—moonlight? Frost? I lift my head to gaze at the bright moon, then lower it, thinking of home.",
        quality_rating=5
    )

    trans1 = service.create_translation(trans1_data)
    trans2 = service.create_translation(trans2_data)
    print(f"✓ Created translations: AI ({trans1.translator_type}), Human ({trans2.translator_type})")

    # Create AI logs
    print("\n🤖 Creating AI logs...")
    ai_log1_data = AILogCreate(
        translation_id=trans1.id,
        model_name="gpt-4",
        workflow_mode=WorkflowMode.REASONING,
        runtime_seconds=45.2,
        token_usage_json='{"prompt_tokens": 850, "completion_tokens": 320, "total_tokens": 1170}',
        cost_info_json='{"total_cost": 0.041, "currency": "USD"}',
        notes="Translation completed with good poetic quality. Captured the pastoral essence effectively."
    )

    ai_log1 = service.create_ai_log(ai_log1_data)
    print(f"✓ Created AI log: {ai_log1.model_name} ({ai_log1.workflow_mode})")

    # Create human notes
    print("\n👤 Creating human notes...")
    note1_data = HumanNoteCreate(
        translation_id=trans2.id,
        note_text="Excellent translation that maintains the concise beauty of the original. The word choices 'splash' and 'frost' create effective imagery. The questioning form 'moonlight? Frost?' captures the contemplative mood perfectly."
    )

    note1 = service.create_human_note(note1_data)
    print(f"✓ Created human note for translation by {trans2.translator_info}")

    # Display dashboard
    print("\n📊 Dashboard Overview:")
    dashboard = service.get_dashboard_data()
    stats = dashboard["stats"]
    print(f"  • Total poems: {stats['total_poems']}")
    print(f"  • Total translations: {stats['total_translations']}")
    print(f"  • AI translations: {stats['ai_translations']}")
    print(f"  • Human translations: {stats['human_translations']}")
    print(f"  • Languages: {', '.join(stats['languages'])}")

    # Display poems with translations
    print("\n📚 Poems and Translations:")
    for poem in dashboard["recent_poems"]:
        print(f"\n📖 {poem.poem_title} by {poem.poet_name} ({poem.source_language})")
        translations = service.get_poem_translations(poem.id)
        for trans in translations:
            print(f"  🌐 {trans.translator_type.title()} translation to {trans.target_language}")
            if trans.translator_info:
                print(f"     By: {trans.translator_info}")
            if trans.quality_rating:
                print(f"     Quality: {'⭐' * trans.quality_rating}")
            # Show preview
            preview = trans.translated_text[:100] + "..." if len(trans.translated_text) > 100 else trans.translated_text
            print(f"     Preview: {preview}")

            # Show additional details
            details = service.get_translation_with_details(trans.id)
            if details["ai_logs"]:
                for ai_log in details["ai_logs"]:
                    print(f"     🤖 AI: {ai_log.model_name} ({ai_log.runtime_seconds}s)")
            if details["human_notes"]:
                for note in details["human_notes"]:
                    note_preview = note.note_text[:80] + "..." if len(note.note_text) > 80 else note.note_text
                    print(f"     👤 Note: {note_preview}")

    # Test search functionality
    print("\n🔍 Testing Search:")
    search_results = service.search_poems("自然")
    print(f"  Found {len(search_results)} poems matching '自然'")
    for result in search_results:
        print(f"    • {result.poem_title} - {result.poet_name}")

    # Test statistics
    print("\n📈 Detailed Statistics:")
    detailed_stats = service.get_repository_stats()
    print(f"  • Latest translation: {detailed_stats.latest_translation}")
    print(f"  • Languages represented: {len(detailed_stats.languages)}")

    # Test pagination
    print("\n📄 Testing Pagination:")
    paginated = service.get_poems_paginated(page=1, page_size=1)
    print(f"  Page {paginated['pagination']['current_page']} of {paginated['pagination']['total_pages']}")
    print(f"  Showing {len(paginated['poems'])} of {paginated['pagination']['total_items']} poems")

    # Cleanup
    session.close()
    print("\n✅ Demo completed successfully! All functionality working as expected.")
    print("\n🎯 Key Features Demonstrated:")
    print("  • Poem creation and management")
    print("  • Translation creation (AI and Human)")
    print("  • AI logging with performance metrics")
    print("  • Human annotation system")
    print("  • Dashboard with statistics")
    print("  • Search functionality")
    print("  • Pagination support")
    print("  • Comprehensive data relationships")


if __name__ == "__main__":
    demo_repository_service()
</code>

src/vpsweb/repository/alembic.ini:
<code>
# A generic, single database configuration.

[alembic]
# path to migration scripts.
# this is typically a path given in POSIX (e.g. forward slashes)
# format, relative to the token %(here)s which refers to the location of this
# ini file
script_location = %(here)s/migrations

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.  for multiple paths, the path separator
# is defined by "path_separator" below.
prepend_sys_path = .


# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the tzdata library which can be installed by adding
# `alembic[tz]` to the pip requirements.
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to <script_location>/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "path_separator"
# below.
# version_locations = %(here)s/bar:%(here)s/bat:%(here)s/alembic/versions

# path_separator; This indicates what character is used to split lists of file
# paths, including version_locations and prepend_sys_path within configparser
# files such as alembic.ini.
# The default rendered in new alembic.ini files is "os", which uses os.pathsep
# to provide os-dependent path splitting.
#
# Note that in order to support legacy alembic.ini files, this default does NOT
# take place if path_separator is not present in alembic.ini.  If this
# option is omitted entirely, fallback logic is as follows:
#
# 1. Parsing of the version_locations option falls back to using the legacy
#    "version_path_separator" key, which if absent then falls back to the legacy
#    behavior of splitting on spaces and/or commas.
# 2. Parsing of the prepend_sys_path option falls back to the legacy
#    behavior of splitting on spaces, commas, or colons.
#
# Valid values for path_separator are:
#
# path_separator = :
# path_separator = ;
# path_separator = space
# path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

# database URL.  This is consumed by the user-maintained env.py script only.
# other means of configuring database URLs may be customized within the env.py
# file.
sqlalchemy.url = sqlite:///./repository_root/repo.db


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the module runner, against the "ruff" module
# hooks = ruff
# ruff.type = module
# ruff.module = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Alternatively, use the exec runner to execute a binary found on your PATH
# hooks = ruff
# ruff.type = exec
# ruff.executable = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Logging configuration.  This is also consumed by the user-maintained
# env.py script only.
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S

</code>

src/vpsweb/repository/database.py:
<code>
"""
VPSWeb Repository Database Configuration v0.3.1

SQLite database setup with SQLAlchemy ORM configuration.
Provides database session management and initialization utilities.
"""

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base, Session
from sqlalchemy.pool import StaticPool
from .settings import settings

# Create SQLAlchemy engine with SQLite-specific settings
engine = create_engine(
    settings.database_url,
    connect_args={"check_same_thread": False},  # Required for SQLite
    poolclass=StaticPool,  # Use StaticPool for SQLite
    echo=settings.log_level.lower() == "debug"  # Log SQL in debug mode
)

# Create session factory
SessionLocal = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=engine
)

# Create declarative base for ORM models
Base = declarative_base()


def init_db() -> None:
    """
    Initialize database tables.
    Creates all tables defined in ORM models.
    """
    from . import models  # Import models to ensure they're registered
    Base.metadata.create_all(bind=engine)


def get_db() -> Session:
    """
    Dependency function to get database session.

    Yields:
        Session: SQLAlchemy database session
    """
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


def create_session() -> Session:
    """
    Create a new database session manually.

    Returns:
        Session: SQLAlchemy database session
    """
    return SessionLocal()


def check_db_connection() -> bool:
    """
    Test database connection.

    Returns:
        bool: True if connection is successful
    """
    try:
        with engine.connect() as conn:
            conn.execute("SELECT 1")
        return True
    except Exception:
        return False
</code>

src/vpsweb/repository/__init__.py:
<code>
"""
VPSWeb Repository Module v0.3.1

Data layer for poetry translation repository.
Provides models, database operations, and VPSWeb integration.
"""

__version__ = "0.3.1"
__author__ = "VPSWeb Development Team"
</code>

src/vpsweb/repository/apply_migration.py:
<code>
#!/usr/bin/env python3
"""
Apply initial migration to create database tables
"""

import os
import sys
from pathlib import Path

# Add repository root to path
repo_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(repo_root))

from sqlalchemy import create_engine, text
from sqlalchemy.pool import StaticPool

# Create database engine
database_url = "sqlite:///./repository_root/repo.db"
engine = create_engine(
    database_url,
    connect_args={"check_same_thread": False},
    poolclass=StaticPool,
)

def create_tables():
    """Create all tables using SQL"""

    # Ensure repository_root directory exists
    repo_dir = Path("repository_root")
    repo_dir.mkdir(exist_ok=True)

    # Create tables
    with engine.connect() as conn:
        # Create poems table
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS poems (
                id VARCHAR(26) PRIMARY KEY,
                poet_name VARCHAR(200) NOT NULL,
                poem_title VARCHAR(300) NOT NULL,
                source_language VARCHAR(10) NOT NULL,
                original_text TEXT NOT NULL,
                metadata_json TEXT,
                created_at DATETIME NOT NULL,
                updated_at DATETIME NOT NULL
            )
        """))

        # Create translations table
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS translations (
                id VARCHAR(26) PRIMARY KEY,
                poem_id VARCHAR(26) NOT NULL,
                translator_type VARCHAR(10) NOT NULL,
                translator_info VARCHAR(200),
                target_language VARCHAR(10) NOT NULL,
                translated_text TEXT NOT NULL,
                quality_rating INTEGER,
                raw_path VARCHAR(500),
                created_at DATETIME NOT NULL,
                FOREIGN KEY (poem_id) REFERENCES poems(id) ON DELETE CASCADE
            )
        """))

        # Create ai_logs table
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS ai_logs (
                id VARCHAR(26) PRIMARY KEY,
                translation_id VARCHAR(26) NOT NULL,
                model_name VARCHAR(100) NOT NULL,
                workflow_mode VARCHAR(20) NOT NULL,
                token_usage_json TEXT,
                cost_info_json TEXT,
                runtime_seconds INTEGER,
                notes TEXT,
                created_at DATETIME NOT NULL,
                FOREIGN KEY (translation_id) REFERENCES translations(id) ON DELETE CASCADE
            )
        """))

        # Create human_notes table
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS human_notes (
                id VARCHAR(26) PRIMARY KEY,
                translation_id VARCHAR(26) NOT NULL,
                note_text TEXT NOT NULL,
                created_at DATETIME NOT NULL,
                FOREIGN KEY (translation_id) REFERENCES translations(id) ON DELETE CASCADE
            )
        """))

        # Create indexes
        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_poems_created_at ON poems (created_at)"))
        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_poems_poet_name ON poems (poet_name)"))
        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_poems_title ON poems (poem_title)"))
        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_poems_language ON poems (source_language)"))

        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_translations_poem_id ON translations (poem_id)"))
        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_translations_type ON translations (translator_type)"))
        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_translations_language ON translations (target_language)"))
        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_translations_created_at ON translations (created_at)"))

        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_ai_logs_translation_id ON ai_logs (translation_id)"))
        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_ai_logs_model_name ON ai_logs (model_name)"))
        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_ai_logs_workflow_mode ON ai_logs (workflow_mode)"))
        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_ai_logs_created_at ON ai_logs (created_at)"))

        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_human_notes_translation_id ON human_notes (translation_id)"))
        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_human_notes_created_at ON human_notes (created_at)"))

        conn.commit()

    print("Database tables created successfully!")
    print("Tables: poems, translations, ai_logs, human_notes")
    print("Indexes created for optimal query performance")

if __name__ == "__main__":
    create_tables()
</code>

src/vpsweb/repository/schemas.py:
<code>
"""
VPSWeb Repository Pydantic Schemas v0.3.1

Pydantic models for data validation and serialization.
Defines schemas for poems, translations, AI logs, and human notes.
"""

from datetime import datetime
from enum import Enum
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field, field_validator, model_validator, ConfigDict
from pydantic_core import ValidationError
import re


class TranslatorType(str, Enum):
    """Enum for translator types"""
    AI = "ai"
    HUMAN = "human"


class WorkflowMode(str, Enum):
    """Enum for workflow modes"""
    REASONING = "reasoning"
    NON_REASONING = "non_reasoning"
    HYBRID = "hybrid"


# Base schemas
class BaseSchema(BaseModel):
    """Base schema with common configuration"""
    model_config = ConfigDict(
        from_attributes=True,
        validate_assignment=True,
        json_encoders={
            datetime: lambda v: v.isoformat()
        }
    )


# Poem schemas
class PoemBase(BaseSchema):
    """Base poem schema with enhanced validation"""
    poet_name: str = Field(
        ...,
        min_length=1,
        max_length=200,
        description="Name of the poet",
        examples=["陶渊明", "William Shakespeare", "李白"]
    )
    poem_title: str = Field(
        ...,
        min_length=1,
        max_length=300,
        description="Title of the poem",
        examples=["歸園田居", "Sonnet 18", "靜夜思"]
    )
    source_language: str = Field(
        ...,
        min_length=2,
        max_length=10,
        description="Source language code (BCP-47)",
        examples=["zh", "en", "fr", "es"]
    )
    original_text: str = Field(
        ...,
        min_length=10,
        description="Original poem text (minimum 10 characters)",
        examples=["採菊東籬下，悠然見南山。", "Shall I compare thee to a summer's day?"]
    )
    metadata_json: Optional[str] = Field(
        None,
        max_length=5000,
        description="Optional metadata as JSON string (max 5000 characters)"
    )

    @field_validator('poet_name')
    @classmethod
    def validate_poet_name(cls, v: str) -> str:
        """Validate poet name format"""
        if not v or not v.strip():
            raise ValueError('Poet name cannot be empty')

        v = v.strip()

        # Check for reasonable length and characters
        if len(v) < 1:
            raise ValueError('Poet name must be at least 1 character long')

        # Allow letters, spaces, hyphens, and common punctuation
        if not re.match(r'^[\w\s\-\.\,\'\'\u4e00-\u9fff]+$', v):
            raise ValueError('Poet name contains invalid characters')

        return v

    @field_validator('poem_title')
    @classmethod
    def validate_poem_title(cls, v: str) -> str:
        """Validate poem title format"""
        if not v or not v.strip():
            raise ValueError('Poem title cannot be empty')

        v = v.strip()

        if len(v) < 1:
            raise ValueError('Poem title must be at least 1 character long')

        # Allow letters, numbers, spaces, and common punctuation
        if not re.match(r'^[\w\s\-\.\,\!\?\:\;\'\"()\u4e00-\u9fff]+$', v):
            raise ValueError('Poem title contains invalid characters')

        return v

    @field_validator('source_language')
    @classmethod
    def validate_language_code(cls, v: str) -> str:
        """Enhanced language code validation"""
        if not v or len(v.strip()) < 2:
            raise ValueError('Language code must be at least 2 characters')

        v = v.strip().lower()

        # Validate BCP-47 format (simplified)
        if not re.match(r'^[a-z]{2}(-[A-Z]{2})?(-[a-z]{3})?$', v):
            # Allow common language codes even if not strictly BCP-47
            if not re.match(r'^[a-z]{2,3}(-[A-Z]{2})?$', v):
                raise ValueError('Language code must be in valid format (e.g., "en", "zh-CN")')

        return v

    @field_validator('original_text')
    @classmethod
    def validate_original_text(cls, v: str) -> str:
        """Validate original poem text"""
        if not v or not v.strip():
            raise ValueError('Original text cannot be empty')

        v = v.strip()

        if len(v) < 10:
            raise ValueError('Original text must be at least 10 characters long')

        # Check for reasonable content (not just whitespace or punctuation)
        if len(re.sub(r'[\s\-\.\,\!\?\:\;\'\"()（）。，！？：；""'']+', '', v)) < 5:
            raise ValueError('Original text must contain meaningful content')

        return v

    @field_validator('metadata_json')
    @classmethod
    def validate_metadata_json(cls, v: Optional[str]) -> Optional[str]:
        """Validate metadata JSON format"""
        if v is None:
            return None

        v = v.strip()
        if not v:
            return None

        # Basic JSON format validation
        if not (v.startswith('{') and v.endswith('}')):
            raise ValueError('Metadata must be valid JSON format')

        return v


class PoemCreate(PoemBase):
    """Schema for creating a new poem"""
    pass


class PoemUpdate(BaseSchema):
    """Schema for updating an existing poem"""
    poet_name: Optional[str] = Field(None, min_length=1, max_length=200)
    poem_title: Optional[str] = Field(None, min_length=1, max_length=300)
    source_language: Optional[str] = Field(None, min_length=2, max_length=10)
    original_text: Optional[str] = Field(None, min_length=1)
    metadata_json: Optional[str] = None


class PoemResponse(PoemBase):
    """Schema for poem response"""
    id: str = Field(..., description="Poem ID (ULID)")
    created_at: datetime = Field(..., description="Creation timestamp")
    updated_at: datetime = Field(..., description="Last update timestamp")
    translation_count: Optional[int] = Field(0, description="Number of translations")


class PoemList(BaseSchema):
    """Schema for poem list response"""
    poems: List[PoemResponse] = Field(default_factory=list, description="List of poems")
    total: int = Field(..., description="Total number of poems")
    page: int = Field(1, ge=1, description="Current page number")
    page_size: int = Field(10, ge=1, le=100, description="Number of items per page")


# Translation schemas
class TranslationBase(BaseSchema):
    """Base translation schema with enhanced validation"""
    translator_type: TranslatorType = Field(
        ...,
        description="Type of translator",
        examples=[TranslatorType.AI, TranslatorType.HUMAN]
    )
    translator_info: Optional[str] = Field(
        None,
        max_length=200,
        description="Translator information (model name, person name, etc.)",
        examples=["gpt-4", "Claude-3.5", "王晓明"]
    )
    target_language: str = Field(
        ...,
        min_length=2,
        max_length=10,
        description="Target language code (BCP-47)",
        examples=["en", "zh", "fr", "es"]
    )
    translated_text: str = Field(
        ...,
        min_length=10,
        description="Translated text (minimum 10 characters)",
        examples=["Picking chrysanthemums by the eastern fence, I calmly see the Southern Mountain."]
    )
    quality_rating: Optional[int] = Field(
        None,
        ge=1,
        le=5,
        description="Quality rating (1=Poor, 5=Excellent)"
    )
    raw_path: Optional[str] = Field(
        None,
        max_length=500,
        description="Path to raw output file"
    )

    @field_validator('translator_info')
    @classmethod
    def validate_translator_info(cls, v: Optional[str]) -> Optional[str]:
        """Validate translator information"""
        if v is None:
            return None

        v = v.strip()
        if not v:
            return None

        if len(v) < 1:
            raise ValueError('Translator info must be at least 1 character long if provided')

        # Allow letters, numbers, spaces, hyphens, dots, and common punctuation
        if not re.match(r'^[\w\s\-\.\,\(\)\[\]\u4e00-\u9fff]+$', v):
            raise ValueError('Translator info contains invalid characters')

        return v

    @field_validator('target_language')
    @classmethod
    def validate_target_language(cls, v: str) -> str:
        """Enhanced target language code validation"""
        if not v or len(v.strip()) < 2:
            raise ValueError('Target language code must be at least 2 characters')

        v = v.strip().lower()

        # Same validation as source_language
        if not re.match(r'^[a-z]{2}(-[A-Z]{2})?(-[a-z]{3})?$', v):
            if not re.match(r'^[a-z]{2,3}(-[A-Z]{2})?$', v):
                raise ValueError('Target language code must be in valid format (e.g., "en", "zh-CN")')

        return v

    @field_validator('translated_text')
    @classmethod
    def validate_translated_text(cls, v: str) -> str:
        """Validate translated text content"""
        if not v or not v.strip():
            raise ValueError('Translated text cannot be empty')

        v = v.strip()

        if len(v) < 10:
            raise ValueError('Translated text must be at least 10 characters long')

        # Check for reasonable content
        if len(re.sub(r'[\s\-\.\,\!\?\:\;\'\"()（）。，！？：；""'']+', '', v)) < 5:
            raise ValueError('Translated text must contain meaningful content')

        return v

    @field_validator('raw_path')
    @classmethod
    def validate_raw_path(cls, v: Optional[str]) -> Optional[str]:
        """Validate raw file path"""
        if v is None:
            return None

        v = v.strip()
        if not v:
            return None

        # Basic path validation - prevent directory traversal
        if '..' in v:
            raise ValueError('Path cannot contain directory traversal')

        # Check for valid file extensions
        valid_extensions = ['.txt', '.json', '.md', '.log']
        if not any(v.lower().endswith(ext) for ext in valid_extensions):
            raise ValueError(f'Path must have valid file extension: {", ".join(valid_extensions)}')

        return v


class TranslationCreate(TranslationBase):
    """Schema for creating a new translation"""
    poem_id: str = Field(
        ...,
        min_length=1,
        max_length=50,
        description="ID of the parent poem"
    )

    @model_validator(mode='after')
    def validate_translation_consistency(self) -> 'TranslationCreate':
        """Validate consistency between translator_type and translator_info"""
        # If translator_type is AI, translator_info should be a model name
        if self.translator_type == TranslatorType.AI and self.translator_info:
            # Check if it looks like a model name
            model_patterns = ['gpt', 'claude', 'gemini', 'deepseek', 'qwen', 'llama']
            if not any(pattern.lower() in self.translator_info.lower() for pattern in model_patterns):
                # This is just a warning, not an error, as AI models can vary
                pass

        # If translator_type is HUMAN, translator_info should be a person's name
        if self.translator_type == TranslatorType.HUMAN and self.translator_info:
            # Basic check for human name format
            if len(self.translator_info) < 2 or len(self.translator_info) > 100:
                raise ValueError('Human translator name must be between 2 and 100 characters')

        return self


class TranslationUpdate(BaseSchema):
    """Schema for updating an existing translation"""
    translator_type: Optional[TranslatorType] = None
    translator_info: Optional[str] = Field(None, max_length=200)
    target_language: Optional[str] = Field(None, min_length=2, max_length=10)
    translated_text: Optional[str] = Field(None, min_length=1)
    quality_rating: Optional[int] = Field(None, ge=1, le=5)
    raw_path: Optional[str] = None


class TranslationResponse(TranslationBase):
    """Schema for translation response"""
    id: str = Field(..., description="Translation ID (ULID)")
    poem_id: str = Field(..., description="ID of the parent poem")
    created_at: datetime = Field(..., description="Creation timestamp")


class TranslationList(BaseSchema):
    """Schema for translation list response"""
    translations: List[TranslationResponse] = Field(default_factory=list, description="List of translations")
    total: int = Field(..., description="Total number of translations")
    poem_id: str = Field(..., description="Parent poem ID")


# AI Log schemas
class AILogBase(BaseSchema):
    """Base AI log schema with enhanced validation"""
    model_name: str = Field(
        ...,
        min_length=1,
        max_length=100,
        description="AI model name",
        examples=["gpt-4", "claude-3-sonnet", "deepseek-reasoner"]
    )
    workflow_mode: WorkflowMode = Field(
        ...,
        description="Translation workflow mode used",
        examples=[WorkflowMode.REASONING, WorkflowMode.NON_REASONING, WorkflowMode.HYBRID]
    )
    token_usage_json: Optional[str] = Field(
        None,
        max_length=2000,
        description="Token usage data as JSON string"
    )
    cost_info_json: Optional[str] = Field(
        None,
        max_length=1000,
        description="Cost information as JSON string"
    )
    runtime_seconds: Optional[float] = Field(
        None,
        ge=0,
        le=3600,  # Max 1 hour runtime
        description="Translation runtime in seconds (0-3600)"
    )
    notes: Optional[str] = Field(
        None,
        max_length=1000,
        description="Additional notes about the translation process"
    )

    @field_validator('model_name')
    @classmethod
    def validate_model_name(cls, v: str) -> str:
        """Validate AI model name"""
        if not v or not v.strip():
            raise ValueError('Model name cannot be empty')

        v = v.strip()

        if len(v) < 1:
            raise ValueError('Model name must be at least 1 character long')

        # Allow letters, numbers, hyphens, and dots in model names
        if not re.match(r'^[a-zA-Z0-9\-\.\u4e00-\u9fff]+$', v):
            raise ValueError('Model name contains invalid characters')

        return v

    @field_validator('runtime_seconds')
    @classmethod
    def validate_runtime_seconds(cls, v: Optional[float]) -> Optional[float]:
        """Validate runtime seconds"""
        if v is None:
            return None

        if v < 0:
            raise ValueError('Runtime cannot be negative')

        if v > 3600:  # 1 hour max
            raise ValueError('Runtime cannot exceed 1 hour (3600 seconds)')

        # Round to 3 decimal places for consistency
        return round(v, 3)

    @field_validator('token_usage_json')
    @classmethod
    def validate_token_usage_json(cls, v: Optional[str]) -> Optional[str]:
        """Validate token usage JSON format"""
        if v is None:
            return None

        v = v.strip()
        if not v:
            return None

        # Basic JSON validation
        if not (v.startswith('{') and v.endswith('}')):
            raise ValueError('Token usage must be valid JSON format')

        # Could add more sophisticated JSON parsing here if needed
        try:
            import json
            json.loads(v)
        except json.JSONDecodeError:
            raise ValueError('Token usage JSON is not valid')

        return v

    @field_validator('cost_info_json')
    @classmethod
    def validate_cost_info_json(cls, v: Optional[str]) -> Optional[str]:
        """Validate cost info JSON format"""
        if v is None:
            return None

        v = v.strip()
        if not v:
            return None

        # Basic JSON validation
        if not (v.startswith('{') and v.endswith('}')):
            raise ValueError('Cost info must be valid JSON format')

        try:
            import json
            json.loads(v)
        except json.JSONDecodeError:
            raise ValueError('Cost info JSON is not valid')

        return v

    @field_validator('notes')
    @classmethod
    def validate_notes(cls, v: Optional[str]) -> Optional[str]:
        """Validate notes content"""
        if v is None:
            return None

        v = v.strip()
        if not v:
            return None

        if len(v) < 1:
            raise ValueError('Notes must contain content if provided')

        return v


class AILogCreate(AILogBase):
    """Schema for creating a new AI log"""
    translation_id: str = Field(
        ...,
        min_length=1,
        max_length=50,
        description="ID of the parent translation"
    )

    @model_validator(mode='after')
    def validate_ai_log_consistency(self) -> 'AILogCreate':
        """Validate AI log data consistency"""
        # If workflow_mode is reasoning, runtime should typically be longer
        if self.workflow_mode == WorkflowMode.REASONING and self.runtime_seconds:
            if self.runtime_seconds < 5:  # Reasoning usually takes at least 5 seconds
                pass  # Just a note, not an error

        # If token_usage is provided, it should be reasonable
        if self.token_usage_json:
            try:
                import json
                usage_data = json.loads(self.token_usage_json)
                # Basic sanity checks
                if isinstance(usage_data, dict):
                    if 'total_tokens' in usage_data and usage_data['total_tokens'] > 100000:
                        pass  # Very high token usage, but allowed
                    if 'prompt_tokens' in usage_data and usage_data['prompt_tokens'] < 1:
                        raise ValueError('Prompt tokens must be at least 1')
            except json.JSONDecodeError:
                # This is already validated in field validator, but keep as backup
                raise ValueError('Invalid JSON format in token_usage_json')

        return self


class AILogResponse(AILogBase):
    """Schema for AI log response"""
    id: str = Field(..., description="AI log ID (ULID)")
    translation_id: str = Field(..., description="ID of the parent translation")
    created_at: datetime = Field(..., description="Creation timestamp")


# Human Note schemas
class HumanNoteBase(BaseSchema):
    """Base human note schema with enhanced validation"""
    note_text: str = Field(
        ...,
        min_length=5,
        max_length=2000,
        description="Human note text (minimum 5 characters)",
        examples=["This translation captures the poetic essence well.", "Consider alternative wording for the second line."]
    )

    @field_validator('note_text')
    @classmethod
    def validate_note_text(cls, v: str) -> str:
        """Validate human note content"""
        if not v or not v.strip():
            raise ValueError('Note text cannot be empty')

        v = v.strip()

        if len(v) < 5:
            raise ValueError('Note text must be at least 5 characters long')

        # Check for meaningful content
        if len(re.sub(r'[\s\-\.\,\!\?\:\;\'\"()（）。，！？：；""'']+', '', v)) < 3:
            raise ValueError('Note text must contain meaningful content')

        return v


class HumanNoteCreate(HumanNoteBase):
    """Schema for creating a new human note"""
    translation_id: str = Field(..., description="ID of the parent translation")


class HumanNoteResponse(HumanNoteBase):
    """Schema for human note response"""
    id: str = Field(..., description="Human note ID (ULID)")
    translation_id: str = Field(..., description="ID of the parent translation")
    created_at: datetime = Field(..., description="Creation timestamp")


# Translation workflow schemas
class TranslationRequest(BaseSchema):
    """Schema for translation workflow request"""
    poem_id: str = Field(..., description="ID of the poem to translate")
    target_language: str = Field(..., min_length=2, max_length=10, description="Target language")
    workflow_mode: WorkflowMode = Field(WorkflowMode.HYBRID, description="Translation workflow mode")
    model_override: Optional[str] = Field(None, max_length=100, description="Override AI model")


class TranslationResponse(BaseSchema):
    """Schema for translation workflow response"""
    translation_id: str = Field(..., description="ID of the created translation")
    translated_text: str = Field(..., description="The translated text")
    model_name: str = Field(..., description="AI model used")
    runtime_seconds: Optional[float] = Field(None, description="Translation runtime")
    token_usage: Optional[Dict[str, Any]] = Field(None, description="Token usage details")


# Comparison schemas
class ComparisonView(BaseSchema):
    """Schema for comparison view data"""
    poem: PoemResponse = Field(..., description="Poem information")
    translations: List[TranslationResponse] = Field(..., description="All translations for the poem")
    ai_logs: List[AILogResponse] = Field(default_factory=list, description="AI logs for translations")
    human_notes: List[HumanNoteResponse] = Field(default_factory=list, description="Human notes for translations")


# Statistics schemas
class RepositoryStats(BaseSchema):
    """Schema for repository statistics"""
    total_poems: int = Field(..., ge=0, description="Total number of poems")
    total_translations: int = Field(..., ge=0, description="Total number of translations")
    ai_translations: int = Field(..., ge=0, description="Number of AI translations")
    human_translations: int = Field(..., ge=0, description="Number of human translations")
    languages: List[str] = Field(default_factory=list, description="Languages in repository")
    latest_translation: Optional[datetime] = Field(None, description="Latest translation timestamp")


# Response wrappers
class APIResponse(BaseSchema):
    """Generic API response wrapper"""
    success: bool = Field(True, description="Request success status")
    message: str = Field("Operation completed successfully", description="Response message")
    data: Optional[Any] = Field(None, description="Response data")


class ErrorResponse(BaseSchema):
    """Schema for error responses"""
    success: bool = Field(False, description="Request success status")
    message: str = Field(..., description="Error message")
    error_code: Optional[str] = Field(None, description="Error code")
    details: Optional[Dict[str, Any]] = Field(None, description="Additional error details")
</code>

src/vpsweb/repository/settings.py:
<code>
"""
VPSWeb Repository Settings Configuration v0.3.1

Configuration settings for the repository layer.
"""

from pydantic_settings import BaseSettings


class RepositorySettings(BaseSettings):
    """Repository layer settings"""

    # Database settings
    database_url: str = "sqlite:///./repository_root/repo.db"

    # Repository storage settings
    repo_root: str = "./repository_root"
    storage_path: str = "./repository_root/data"

    # VPSWeb integration settings
    vpsweb_config_path: str = "./config"
    default_workflow_mode: str = "hybrid"

    # Logging settings
    log_level: str = "INFO"

    class Config:
        env_file = ".env"
        env_prefix = "REPO_"


# Global settings instance
settings = RepositorySettings()
</code>

src/vpsweb/repository/crud.py:
<code>
"""
VPSWeb Repository CRUD Operations v0.3.1

Comprehensive CRUD operations for the 4-table database schema.
Provides create, read, update, and delete operations for poems, translations,
AI logs, and human notes with proper error handling and type safety.
"""

from typing import List, Optional, Dict, Any
from datetime import datetime
from uuid import uuid4

from sqlalchemy import select, update, delete, func, and_, or_
from sqlalchemy.orm import Session, selectinload
from sqlalchemy.exc import IntegrityError, SQLAlchemyError

from .models import Poem, Translation, AILog, HumanNote
from .schemas import (
    PoemCreate, PoemUpdate, PoemResponse,
    TranslationCreate, TranslationUpdate, TranslationResponse,
    AILogCreate, AILogResponse,
    HumanNoteCreate, HumanNoteResponse,
    TranslatorType, WorkflowMode
)


class CRUDPoem:
    """CRUD operations for Poem model"""

    def __init__(self, db: Session):
        self.db = db

    def create(self, poem_data: PoemCreate) -> Poem:
        """
        Create a new poem

        Args:
            poem_data: Poem creation data

        Returns:
            Created poem object

        Raises:
            IntegrityError: If poem with same ID already exists
        """
        # Generate ULID-like ID (simplified for prototype)
        poem_id = str(uuid4()).replace('-', '')[:26]

        db_poem = Poem(
            id=poem_id,
            poet_name=poem_data.poet_name,
            poem_title=poem_data.poem_title,
            source_language=poem_data.source_language,
            original_text=poem_data.original_text,
            metadata_json=poem_data.metadata_json
        )

        try:
            self.db.add(db_poem)
            self.db.commit()
            self.db.refresh(db_poem)
            return db_poem
        except IntegrityError:
            self.db.rollback()
            raise
        except SQLAlchemyError as e:
            self.db.rollback()
            raise e

    def get_by_id(self, poem_id: str) -> Optional[Poem]:
        """
        Get poem by ID

        Args:
            poem_id: Poem ID

        Returns:
            Poem object if found, None otherwise
        """
        stmt = select(Poem).where(Poem.id == poem_id)
        result = self.db.execute(stmt).scalar_one_or_none()
        return result

    def get_multi(
        self,
        skip: int = 0,
        limit: int = 100,
        poet_name: Optional[str] = None,
        language: Optional[str] = None,
        title_search: Optional[str] = None
    ) -> List[Poem]:
        """
        Get multiple poems with optional filtering

        Args:
            skip: Number of records to skip
            limit: Maximum number of records to return
            poet_name: Filter by poet name
            language: Filter by source language
            title_search: Search in poem title

        Returns:
            List of poem objects
        """
        stmt = select(Poem)

        # Apply filters
        if poet_name:
            stmt = stmt.where(Poem.poet_name.ilike(f"%{poet_name}%"))
        if language:
            stmt = stmt.where(Poem.source_language == language)
        if title_search:
            stmt = stmt.where(Poem.poem_title.ilike(f"%{title_search}%"))

        # Apply ordering and pagination
        stmt = stmt.order_by(Poem.created_at.desc()).offset(skip).limit(limit)

        result = self.db.execute(stmt).scalars().all()
        return result

    def update(self, poem_id: str, poem_data: PoemUpdate) -> Optional[Poem]:
        """
        Update existing poem

        Args:
            poem_id: Poem ID
            poem_data: Poem update data

        Returns:
            Updated poem object if found, None otherwise
        """
        stmt = (
            update(Poem)
            .where(Poem.id == poem_id)
            .values(
                poet_name=poem_data.poet_name,
                poem_title=poem_data.poem_title,
                source_language=poem_data.source_language,
                original_text=poem_data.original_text,
                metadata_json=poem_data.metadata_json,
                updated_at=datetime.utcnow()
            )
        )

        result = self.db.execute(stmt)
        if result.rowcount == 0:
            return None

        self.db.commit()
        return self.get_by_id(poem_id)

    def delete(self, poem_id: str) -> bool:
        """
        Delete poem by ID

        Args:
            poem_id: Poem ID

        Returns:
            True if deleted, False if not found
        """
        stmt = delete(Poem).where(Poem.id == poem_id)
        result = self.db.execute(stmt)
        self.db.commit()
        return result.rowcount > 0

    def count(self) -> int:
        """Get total number of poems"""
        stmt = select(func.count(Poem.id))
        result = self.db.execute(stmt).scalar()
        return result

    def get_by_poet(self, poet_name: str) -> List[Poem]:
        """Get all poems by a specific poet"""
        stmt = select(Poem).where(Poem.poet_name == poet_name).order_by(Poem.created_at.desc())
        result = self.db.execute(stmt).scalars().all()
        return result


class CRUDTranslation:
    """CRUD operations for Translation model"""

    def __init__(self, db: Session):
        self.db = db

    def create(self, translation_data: TranslationCreate) -> Translation:
        """
        Create a new translation

        Args:
            translation_data: Translation creation data

        Returns:
            Created translation object
        """
        # Generate ULID-like ID
        translation_id = str(uuid4()).replace('-', '')[:26]

        db_translation = Translation(
            id=translation_id,
            poem_id=translation_data.poem_id,
            translator_type=translation_data.translator_type,
            translator_info=translation_data.translator_info,
            target_language=translation_data.target_language,
            translated_text=translation_data.translated_text,
            quality_rating=translation_data.quality_rating,
            raw_path=translation_data.raw_path
        )

        try:
            self.db.add(db_translation)
            self.db.commit()
            self.db.refresh(db_translation)
            return db_translation
        except IntegrityError:
            self.db.rollback()
            raise
        except SQLAlchemyError as e:
            self.db.rollback()
            raise e

    def get_by_id(self, translation_id: str) -> Optional[Translation]:
        """Get translation by ID"""
        stmt = select(Translation).where(Translation.id == translation_id)
        result = self.db.execute(stmt).scalar_one_or_none()
        return result

    def get_by_poem(self, poem_id: str) -> List[Translation]:
        """Get all translations for a poem"""
        stmt = (
            select(Translation)
            .where(Translation.poem_id == poem_id)
            .order_by(Translation.created_at.desc())
        )
        result = self.db.execute(stmt).scalars().all()
        return result

    def get_multi(
        self,
        skip: int = 0,
        limit: int = 100,
        translator_type: Optional[TranslatorType] = None,
        target_language: Optional[str] = None
    ) -> List[Translation]:
        """Get multiple translations with optional filtering"""
        stmt = select(Translation)

        if translator_type:
            stmt = stmt.where(Translation.translator_type == translator_type)
        if target_language:
            stmt = stmt.where(Translation.target_language == target_language)

        stmt = stmt.order_by(Translation.created_at.desc()).offset(skip).limit(limit)
        result = self.db.execute(stmt).scalars().all()
        return result

    def update(self, translation_id: str, translation_data: TranslationUpdate) -> Optional[Translation]:
        """Update existing translation"""
        stmt = (
            update(Translation)
            .where(Translation.id == translation_id)
            .values(
                translator_type=translation_data.translator_type,
                translator_info=translation_data.translator_info,
                target_language=translation_data.target_language,
                translated_text=translation_data.translated_text,
                quality_rating=translation_data.quality_rating,
                raw_path=translation_data.raw_path
            )
        )

        result = self.db.execute(stmt)
        if result.rowcount == 0:
            return None

        self.db.commit()
        return self.get_by_id(translation_id)

    def delete(self, translation_id: str) -> bool:
        """Delete translation by ID"""
        stmt = delete(Translation).where(Translation.id == translation_id)
        result = self.db.execute(stmt)
        self.db.commit()
        return result.rowcount > 0

    def count(self) -> int:
        """Get total number of translations"""
        stmt = select(func.count(Translation.id))
        result = self.db.execute(stmt).scalar()
        return result

    def get_by_language_pair(self, source_lang: str, target_lang: str) -> List[Translation]:
        """Get translations by language pair"""
        stmt = (
            select(Translation)
            .join(Poem)
            .where(
                and_(
                    Poem.source_language == source_lang,
                    Translation.target_language == target_lang
                )
            )
            .order_by(Translation.created_at.desc())
        )
        result = self.db.execute(stmt).scalars().all()
        return result


class CRUDAILog:
    """CRUD operations for AILog model"""

    def __init__(self, db: Session):
        self.db = db

    def create(self, ai_log_data: AILogCreate) -> AILog:
        """Create a new AI log entry"""
        # Generate ULID-like ID
        ai_log_id = str(uuid4()).replace('-', '')[:26]

        db_ai_log = AILog(
            id=ai_log_id,
            translation_id=ai_log_data.translation_id,
            model_name=ai_log_data.model_name,
            workflow_mode=ai_log_data.workflow_mode,
            token_usage_json=ai_log_data.token_usage_json,
            cost_info_json=ai_log_data.cost_info_json,
            runtime_seconds=ai_log_data.runtime_seconds,
            notes=ai_log_data.notes
        )

        try:
            self.db.add(db_ai_log)
            self.db.commit()
            self.db.refresh(db_ai_log)
            return db_ai_log
        except IntegrityError:
            self.db.rollback()
            raise
        except SQLAlchemyError as e:
            self.db.rollback()
            raise e

    def get_by_id(self, ai_log_id: str) -> Optional[AILog]:
        """Get AI log by ID"""
        stmt = select(AILog).where(AILog.id == ai_log_id)
        result = self.db.execute(stmt).scalar_one_or_none()
        return result

    def get_by_translation(self, translation_id: str) -> List[AILog]:
        """Get all AI logs for a translation"""
        stmt = (
            select(AILog)
            .where(AILog.translation_id == translation_id)
            .order_by(AILog.created_at.desc())
        )
        result = self.db.execute(stmt).scalars().all()
        return result

    def get_by_model(self, model_name: str) -> List[AILog]:
        """Get AI logs by model name"""
        stmt = (
            select(AILog)
            .where(AILog.model_name == model_name)
            .order_by(AILog.created_at.desc())
        )
        result = self.db.execute(stmt).scalars().all()
        return result

    def get_by_workflow_mode(self, workflow_mode: WorkflowMode) -> List[AILog]:
        """Get AI logs by workflow mode"""
        stmt = (
            select(AILog)
            .where(AILog.workflow_mode == workflow_mode)
            .order_by(AILog.created_at.desc())
        )
        result = self.db.execute(stmt).scalars().all()
        return result


class CRUDHumanNote:
    """CRUD operations for HumanNote model"""

    def __init__(self, db: Session):
        self.db = db

    def create(self, note_data: HumanNoteCreate) -> HumanNote:
        """Create a new human note"""
        # Generate ULID-like ID
        note_id = str(uuid4()).replace('-', '')[:26]

        db_note = HumanNote(
            id=note_id,
            translation_id=note_data.translation_id,
            note_text=note_data.note_text
        )

        try:
            self.db.add(db_note)
            self.db.commit()
            self.db.refresh(db_note)
            return db_note
        except IntegrityError:
            self.db.rollback()
            raise
        except SQLAlchemyError as e:
            self.db.rollback()
            raise e

    def get_by_id(self, note_id: str) -> Optional[HumanNote]:
        """Get human note by ID"""
        stmt = select(HumanNote).where(HumanNote.id == note_id)
        result = self.db.execute(stmt).scalar_one_or_none()
        return result

    def get_by_translation(self, translation_id: str) -> List[HumanNote]:
        """Get all human notes for a translation"""
        stmt = (
            select(HumanNote)
            .where(HumanNote.translation_id == translation_id)
            .order_by(HumanNote.created_at.desc())
        )
        result = self.db.execute(stmt).scalars().all()
        return result

    def delete(self, note_id: str) -> bool:
        """Delete human note by ID"""
        stmt = delete(HumanNote).where(HumanNote.id == note_id)
        result = self.db.execute(stmt)
        self.db.commit()
        return result.rowcount > 0


# Repository service that combines all CRUD operations
class RepositoryService:
    """Main repository service combining all CRUD operations"""

    def __init__(self, db: Session):
        self.db = db
        self.poems = CRUDPoem(db)
        self.translations = CRUDTranslation(db)
        self.ai_logs = CRUDAILog(db)
        self.human_notes = CRUDHumanNote(db)

    def get_repository_stats(self) -> Dict[str, Any]:
        """Get comprehensive repository statistics"""
        return {
            "total_poems": self.poems.count(),
            "total_translations": self.translations.count(),
            "ai_translations": self.db.execute(
                select(func.count(Translation.id))
                .where(Translation.translator_type == TranslatorType.AI)
            ).scalar(),
            "human_translations": self.db.execute(
                select(func.count(Translation.id))
                .where(Translation.translator_type == TranslatorType.HUMAN)
            ).scalar(),
            "languages": list(self.db.execute(
                select(Poem.source_language).distinct()
            ).scalars()),
            "latest_translation": self.db.execute(
                select(func.max(Translation.created_at))
            ).scalar()
        }

    def search_poems(self, query: str, limit: int = 50) -> List[Poem]:
        """Search poems by text content"""
        stmt = (
            select(Poem)
            .where(
                or_(
                    Poem.poem_title.ilike(f"%{query}%"),
                    Poem.original_text.ilike(f"%{query}%"),
                    Poem.poet_name.ilike(f"%{query}%")
                )
            )
            .limit(limit)
            .order_by(Poem.created_at.desc())
        )
        return self.db.execute(stmt).scalars().all()

    def get_poem_with_translations(self, poem_id: str) -> Optional[Dict[str, Any]]:
        """Get poem with all its translations and related data"""
        poem = self.poems.get_by_id(poem_id)
        if not poem:
            return None

        translations = self.translations.get_by_poem(poem_id)

        result = {
            "poem": poem,
            "translations": []
        }

        for translation in translations:
            ai_logs = self.ai_logs.get_by_translation(translation.id)
            human_notes = self.human_notes.get_by_translation(translation.id)

            result["translations"].append({
                "translation": translation,
                "ai_logs": ai_logs,
                "human_notes": human_notes
            })

        return result


# Dependency function for FastAPI
def get_repository_service(db: Session) -> RepositoryService:
    """Get repository service instance"""
    return RepositoryService(db)
</code>

src/vpsweb/utils/config_loader.py:
<code>
"""
Configuration loader for Vox Poetica Studio Web.

This module provides utilities for loading and validating YAML configuration files,
including environment variable substitution and comprehensive error handling.
"""

import os
import re
import yaml
from pathlib import Path
from typing import Any, Dict, Optional, Union
import logging
from pydantic import ValidationError

from ..models.config import (
    MainConfig,
    ProvidersConfig,
    CompleteConfig,
    WorkflowMode,
)
from ..models.wechat import (
    WeChatConfig,
    ArticleGenerationConfig,
)

logger = logging.getLogger(__name__)


class ConfigLoadError(Exception):
    """Custom exception for configuration loading errors."""

    pass


def substitute_env_vars(value: str) -> str:
    """
    Substitute environment variables in a string.

    Supports ${VAR_NAME} and ${VAR_NAME:-default_value} syntax.

    Args:
        value: String potentially containing environment variable references

    Returns:
        String with environment variables substituted

    Raises:
        ConfigLoadError: If required environment variable is not set
    """
    if not isinstance(value, str):
        return value

    # Pattern to match ${VAR_NAME} or ${VAR_NAME:-default_value}
    pattern = r"\$\{([^}]+)\}"

    def replace_var(match):
        var_expr = match.group(1)

        # Check for default value syntax
        if ":-" in var_expr:
            var_name, default_value = var_expr.split(":-", 1)
            var_name = var_name.strip()
            default_value = default_value.strip()

            env_value = os.getenv(var_name)
            if env_value is None:
                logger.info(
                    f"Environment variable '{var_name}' not set, using default value"
                )
                return default_value
            return env_value
        else:
            # No default value, variable is required
            var_name = var_expr.strip()
            env_value = os.getenv(var_name)

            if env_value is None:
                raise ConfigLoadError(
                    f"Required environment variable '{var_name}' is not set. "
                    f"Please set it in your .env file or environment."
                )
            return env_value

    try:
        result = re.sub(pattern, replace_var, value)
        return result
    except Exception as e:
        raise ConfigLoadError(
            f"Error substituting environment variables in '{value}': {e}"
        )


def substitute_env_vars_in_data(data: Any) -> Any:
    """
    Recursively substitute environment variables in YAML data.

    Args:
        data: YAML data (dict, list, string, etc.)

    Returns:
        Data with environment variables substituted
    """
    if isinstance(data, dict):
        return {key: substitute_env_vars_in_data(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [substitute_env_vars_in_data(item) for item in data]
    elif isinstance(data, str):
        return substitute_env_vars(data)
    else:
        return data


def load_yaml_file(file_path: Union[str, Path]) -> Dict[str, Any]:
    """
    Load a YAML file and substitute environment variables.

    Args:
        file_path: Path to the YAML file

    Returns:
        Dictionary containing the YAML data

    Raises:
        ConfigLoadError: If file cannot be read or parsed
    """
    file_path = Path(file_path)

    if not file_path.exists():
        raise ConfigLoadError(f"Configuration file not found: {file_path}")

    if not file_path.is_file():
        raise ConfigLoadError(f"Configuration path is not a file: {file_path}")

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            yaml_data = yaml.safe_load(f)

        if yaml_data is None:
            raise ConfigLoadError(f"Configuration file is empty: {file_path}")

        # Substitute environment variables
        processed_data = substitute_env_vars_in_data(yaml_data)

        logger.info(f"Successfully loaded configuration file: {file_path}")
        return processed_data

    except yaml.YAMLError as e:
        raise ConfigLoadError(f"Invalid YAML in configuration file {file_path}: {e}")
    except Exception as e:
        raise ConfigLoadError(f"Error reading configuration file {file_path}: {e}")


def load_main_config(config_path: Union[str, Path]) -> MainConfig:
    """
    Load and validate the main configuration file.

    Args:
        config_path: Path to the main configuration file

    Returns:
        Validated MainConfig instance

    Raises:
        ConfigLoadError: If configuration is invalid
    """
    logger.info(f"Loading main configuration from: {config_path}")

    try:
        config_data = load_yaml_file(config_path)

        # Validate with Pydantic
        main_config = MainConfig(**config_data)

        logger.info("Main configuration loaded and validated successfully")
        return main_config

    except ValidationError as e:
        error_details = []
        for error in e.errors():
            loc = " -> ".join(str(x) for x in error["loc"])
            error_details.append(f"  {loc}: {error['msg']}")

        raise ConfigLoadError(
            f"Invalid main configuration in {config_path}:\n" + "\n".join(error_details)
        )
    except Exception as e:
        raise ConfigLoadError(f"Error loading main configuration: {e}")


def load_providers_config(config_path: Union[str, Path]) -> ProvidersConfig:
    """
    Load and validate the providers configuration file.

    Args:
        config_path: Path to the providers configuration file

    Returns:
        Validated ProvidersConfig instance

    Raises:
        ConfigLoadError: If configuration is invalid
    """
    logger.info(f"Loading providers configuration from: {config_path}")

    try:
        config_data = load_yaml_file(config_path)

        # Validate with Pydantic
        providers_config = ProvidersConfig(**config_data)

        logger.info("Providers configuration loaded and validated successfully")
        return providers_config

    except ValidationError as e:
        error_details = []
        for error in e.errors():
            loc = " -> ".join(str(x) for x in error["loc"])
            error_details.append(f"  {loc}: {error['msg']}")

        raise ConfigLoadError(
            f"Invalid providers configuration in {config_path}:\n"
            + "\n".join(error_details)
        )
    except Exception as e:
        raise ConfigLoadError(f"Error loading providers configuration: {e}")


def load_article_generation_config(
    config_data: Dict[str, Any]
) -> ArticleGenerationConfig:
    """
    Load and validate article generation configuration.

    Args:
        config_data: Dictionary containing article generation settings

    Returns:
        Validated ArticleGenerationConfig instance

    Raises:
        ConfigLoadError: If configuration is invalid
    """
    try:
        # Use config_data directly since it's already the article_generation section
        article_gen_data = config_data

        # Validate with Pydantic
        article_gen_config = ArticleGenerationConfig(**article_gen_data)

        logger.info(
            "Article generation configuration loaded and validated successfully"
        )
        return article_gen_config

    except ValidationError as e:
        error_details = []
        for error in e.errors():
            loc = " -> ".join(str(x) for x in error["loc"])
            error_details.append(f"  {loc}: {error['msg']}")

        raise ConfigLoadError(
            f"Invalid article generation configuration:\n" + "\n".join(error_details)
        )
    except Exception as e:
        raise ConfigLoadError(f"Error loading article generation configuration: {e}")


def load_config(config_dir: Optional[Union[str, Path]] = None) -> CompleteConfig:
    """
    Load and validate the complete configuration from a directory.

    By default, looks for configuration files in the following order:
    1. Specified config directory
    2. ./config/ directory (relative to current working directory)
    3. ../config/ directory (relative to script location)

    Args:
        config_dir: Optional directory containing configuration files

    Returns:
        Complete validated configuration

    Raises:
        ConfigLoadError: If configuration cannot be loaded or validated
    """
    if config_dir is None:
        # Try to find config directory
        possible_paths = [
            Path("config"),
            Path(__file__).parent.parent.parent.parent / "config",
        ]

        for path in possible_paths:
            if path.exists() and path.is_dir():
                config_dir = path
                break

        if config_dir is None:
            raise ConfigLoadError(
                "Could not find configuration directory. "
                "Please specify config_dir or ensure config/ exists in the project root."
            )

    config_dir = Path(config_dir)

    if not config_dir.exists():
        raise ConfigLoadError(f"Configuration directory not found: {config_dir}")

    # Load main configuration
    main_config_path = config_dir / "default.yaml"
    if not main_config_path.exists():
        raise ConfigLoadError(f"Main configuration file not found: {main_config_path}")

    main_config = load_main_config(main_config_path)

    # Load providers configuration
    providers_config_path = config_dir / "models.yaml"
    if not providers_config_path.exists():
        raise ConfigLoadError(
            f"Providers configuration file not found: {providers_config_path}"
        )

    providers_config = load_providers_config(providers_config_path)

    # Combine into complete configuration
    complete_config = CompleteConfig(main=main_config, providers=providers_config)

    logger.info("Complete configuration loaded successfully")
    return complete_config


def validate_config_files(config_dir: Optional[Union[str, Path]] = None) -> bool:
    """
    Validate configuration files without loading the complete system.

    Args:
        config_dir: Optional directory containing configuration files

    Returns:
        True if configuration is valid, False otherwise

    Raises:
        ConfigLoadError: If configuration files are invalid
    """
    try:
        config = load_config(config_dir)

        # Additional validation - check all workflow modes
        workflow_modes = [
            WorkflowMode.REASONING,
            WorkflowMode.NON_REASONING,
            WorkflowMode.HYBRID,
        ]

        for mode in workflow_modes:
            try:
                steps = config.main.workflow.get_workflow_steps(mode)
                for step_name, step_config in steps.items():
                    provider_name = step_config.provider
                    if provider_name not in config.providers.providers:
                        raise ConfigLoadError(
                            f"Step '{step_name}' in {mode.value} mode references unknown provider '{provider_name}'"
                        )
            except ValueError as e:
                # Skip modes that aren't configured
                if f"Workflow mode '{mode.value}' is not configured" in str(e):
                    continue
                raise

        logger.info("Configuration validation passed")
        return True

    except Exception as e:
        logger.error(f"Configuration validation failed: {e}")
        raise


def load_wechat_complete_config(
    config_dir: Optional[Union[str, Path]] = None
) -> Dict[str, Any]:
    """
    Load complete WeChat-related configuration including API, article generation, and content settings.

    Args:
        config_dir: Directory containing configuration files

    Returns:
        Dictionary containing all WeChat-related configurations

    Raises:
        ConfigLoadError: If configuration cannot be loaded or validated
    """
    try:
        # Determine config path
        if config_dir:
            config_path = Path(config_dir) / "wechat.yaml"
        else:
            config_path = Path("config/wechat.yaml")

        # Load the full config
        full_config_data = load_yaml_file(config_path)

        # Extract WeChat API configuration
        wechat_config = {
            "appid": full_config_data.get("appid"),
            "secret": full_config_data.get("secret"),
            "base_url": full_config_data.get("base_url", "https://api.weixin.qq.com"),
            "token_cache_path": full_config_data.get(
                "token_cache_path", "outputs/.cache/wechat_token.json"
            ),
            "timeouts": full_config_data.get(
                "timeouts", {"connect": 5.0, "read": 20.0}
            ),
            "retry_config": full_config_data.get(
                "retry", {"attempts": 3, "backoff": "exponential"}
            ),
        }

        # Load article generation configuration
        article_gen_config = load_article_generation_config(
            full_config_data.get("article_generation", {})
        )

        # Combine all configurations
        complete_config = {
            "wechat": wechat_config,
            "article_generation": article_gen_config,
            "llm": full_config_data.get("llm", {}),
            "output": full_config_data.get("output", {}),
            "content": full_config_data.get("content", {}),
            "publishing": full_config_data.get("publishing", {}),
            "development": full_config_data.get("development", {}),
        }

        logger.info("Complete WeChat configuration loaded successfully")
        return complete_config

    except Exception as e:
        raise ConfigLoadError(f"Error loading complete WeChat configuration: {e}")


def validate_wechat_setup(config_dir: Optional[Union[str, Path]] = None) -> bool:
    """
    Validate WeChat configuration and setup without loading the complete system.

    Args:
        config_dir: Directory containing configuration files

    Returns:
        True if configuration is valid, False otherwise

    Raises:
        ConfigLoadError: If validation fails
    """
    try:
        logger.info("Validating WeChat configuration...")

        # Determine config path
        if config_dir:
            config_path = Path(config_dir) / "wechat.yaml"
        else:
            config_path = Path("config/wechat.yaml")

        # Load the full config
        full_config_data = load_yaml_file(config_path)

        # Extract WeChat API configuration
        wechat_config_dict = {
            "appid": full_config_data.get("appid"),
            "secret": full_config_data.get("secret"),
            "base_url": full_config_data.get("base_url", "https://api.weixin.qq.com"),
            "token_cache_path": full_config_data.get(
                "token_cache_path", "outputs/.cache/wechat_token.json"
            ),
            "timeouts": full_config_data.get(
                "timeouts", {"connect": 5.0, "read": 20.0}
            ),
            "retry_config": full_config_data.get(
                "retry", {"attempts": 3, "backoff": "exponential"}
            ),
        }

        # Convert to WeChatConfig object
        wechat_config = WeChatConfig(**wechat_config_dict)

        # Validate essential fields
        if not wechat_config.appid or wechat_config.appid == "YOUR_WECHAT_APPID":
            raise ConfigLoadError("WeChat AppID is not configured")

        if not wechat_config.secret or wechat_config.secret == "YOUR_WECHAT_SECRET":
            raise ConfigLoadError("WeChat Secret is not configured")

        # Validate article generation configuration
        article_gen_config = load_article_generation_config(
            full_config_data.get("article_generation", {})
        )

        # Check if essential directories can be created
        output_dir = Path(wechat_config.token_cache_path).parent
        if not output_dir.exists():
            try:
                output_dir.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created cache directory: {output_dir}")
            except Exception as e:
                raise ConfigLoadError(
                    f"Cannot create cache directory {output_dir}: {e}"
                )

        logger.info("WeChat configuration validation passed")
        return True

    except Exception as e:
        logger.error(f"WeChat configuration validation failed: {e}")
        raise

</code>

src/vpsweb/utils/filename_utils.py:
<code>
"""
Filename utilities for VPSWeb output management.

This module provides utilities for generating clean, descriptive filenames
for translation outputs, including sanitization and metadata extraction.
"""

import re
from typing import Optional, Dict, Any


def sanitize_filename_component(component: str, max_length: int = 30) -> str:
    """
    Sanitize a component for use in filenames.

    Args:
        component: The string component to sanitize
        max_length: Maximum length for the component

    Returns:
        Sanitized string safe for filename use
    """
    if not component:
        return "unknown"

    # Remove or replace problematic characters
    sanitized = re.sub(r'[<>:"/\\|?*]', "", component)  # Remove invalid filename chars
    sanitized = re.sub(r"\s+", "_", sanitized)  # Replace whitespace with underscores
    sanitized = re.sub(
        r"[^\w\-_]", "", sanitized
    )  # Remove any remaining non-alphanumeric chars except hyphen and underscore

    # Remove leading/trailing underscores and hyphens
    sanitized = sanitized.strip("_-")

    # Ensure it's not empty after sanitization
    if not sanitized:
        return "unknown"

    # Truncate to max_length
    if len(sanitized) > max_length:
        sanitized = sanitized[:max_length].rstrip("_-")

    return sanitized.lower()


def extract_poet_and_title(
    poem_text: str, metadata: Optional[Dict[str, Any]] = None
) -> tuple[str, str]:
    """
    Extract poet and title from poem text or metadata.

    Args:
        poem_text: The original poem text
        metadata: Optional metadata dictionary

    Returns:
        Tuple of (poet_name, title_name)
    """
    # First try to extract from metadata
    if metadata:
        poet = (
            metadata.get("author") or metadata.get("poet") or metadata.get("poet_name")
        )
        title = (
            metadata.get("title")
            or metadata.get("poem_title")
            or metadata.get("poem_name")
        )

        if poet and title:
            return sanitize_filename_component(poet), sanitize_filename_component(title)
        elif poet:
            return sanitize_filename_component(poet), "untitled"
        elif title:
            return "unknown", sanitize_filename_component(title)

    # Try to extract from poem text patterns
    lines = poem_text.strip().split("\n")

    # Look for common patterns: "Title\nAuthor: Name" or "Author\n\nTitle"
    if len(lines) >= 2:
        # Pattern 1: Author: Name on second line
        if (
            "作者" in lines[1]
            or "author" in lines[1].lower()
            or "by" in lines[1].lower()
        ):
            title = lines[0] if lines[0].strip() else "untitled"
            author_line = lines[1]
            # Extract name after common patterns
            author_match = re.search(
                r"(?:作者|author|by)[:：]\s*(.+)", author_line, re.IGNORECASE
            )
            if author_match:
                poet = author_match.group(1).strip()
            else:
                poet = author_line.strip()
            return sanitize_filename_component(poet), sanitize_filename_component(title)

        # Pattern 2: First line might be title, second might be author
        elif any(keyword in lines[1] for keyword in ["作者", "author", "by"]):
            title = lines[0] if lines[0].strip() else "untitled"
            poet = lines[1].strip()
            return sanitize_filename_component(poet), sanitize_filename_component(title)

    # Fallback: use first line as title, unknown author
    title = lines[0] if lines and lines[0].strip() else "untitled"
    return "unknown", sanitize_filename_component(title)


def generate_translation_filename(
    poet: str,
    title: str,
    source_lang: str,
    target_lang: str,
    timestamp: str,
    workflow_id: str,
    workflow_mode: Optional[str] = None,
    file_format: str = "json",
    is_log: bool = False,
) -> str:
    """
    Generate a descriptive filename for translation outputs.

    Args:
        poet: Sanitized poet name
        title: Sanitized poem title
        source_lang: Source language
        target_lang: Target language
        timestamp: Timestamp string (YYYYMMDD_HHMMSS)
        workflow_id: Workflow ID (will be truncated to last 8 chars)
        workflow_mode: Optional workflow mode
        file_format: File format (json, md)
        is_log: Whether this is a log file (for markdown)

    Returns:
        Generated filename
    """
    # Get hash suffix from workflow_id
    hash_suffix = workflow_id[-8:] if len(workflow_id) > 8 else workflow_id

    # Build components
    components = []

    # Add poet and title
    if poet != "unknown":
        components.append(poet)
    components.append(title)

    # Add languages
    components.append(f"{source_lang.lower()}_{target_lang.lower()}")

    # Add workflow mode for JSON files
    if file_format == "json" and workflow_mode:
        components.append(workflow_mode)

    # Add timestamp
    components.append(timestamp)

    # Add hash
    components.append(hash_suffix)

    # Add log suffix for log files (no "translation_" prefix)
    if is_log:
        components.append("log")

    # Join components (no "translation_" prefix)
    filename = f"{'_'.join(components)}.{file_format}"

    return filename


def generate_legacy_filename(
    source_lang: str,
    target_lang: str,
    timestamp: str,
    workflow_id: str,
    workflow_mode: Optional[str] = None,
    file_format: str = "json",
    is_log: bool = False,
) -> str:
    """
    Generate filename using legacy scheme (fallback).

    Args:
        source_lang: Source language
        target_lang: Target language
        timestamp: Timestamp string
        workflow_id: Workflow ID
        workflow_mode: Optional workflow mode
        file_format: File format
        is_log: Whether this is a log file

    Returns:
        Generated filename using legacy scheme
    """
    hash_suffix = workflow_id[-8:] if len(workflow_id) > 8 else workflow_id

    if is_log:
        return f"translation_log_{source_lang.lower()}_{target_lang.lower()}_{timestamp}_{hash_suffix}.{file_format}"
    else:
        if workflow_mode:
            return (
                f"translation_{workflow_mode}_{timestamp}_{hash_suffix}.{file_format}"
            )
        else:
            return f"translation_{timestamp}_{hash_suffix}.{file_format}"

</code>

src/vpsweb/utils/ulid_utils.py:
<code>
"""
ULID Generation and Validation Utility for VPSWeb Repository System

This module provides ULID (Universally Unique Lexicographically Sortable Identifier)
generation and validation capabilities for the repository system.

Features:
- ULID generation with time-based sorting
- ULID validation and parsing
- Time extraction from ULIDs
- Monotonic ULID generation
- Custom encoding/decoding support
- Batch ULID generation
"""

import time
import random
import re
from datetime import datetime, timezone
from typing import Optional, Dict, Any, List
from dataclasses import dataclass
import struct


# ULID alphabet: Crockford's Base32
CROCKFORD_BASE32 = "0123456789ABCDEFGHJKMNPQRSTVWXYZ"

# Encoding/decoding maps
ENCODE_MAP = {char: i for i, char in enumerate(CROCKFORD_BASE32)}
DECODE_MAP = {i: char for i, char in enumerate(CROCKFORD_BASE32)}


class ULIDError(Exception):
    """Base exception for ULID operations."""
    pass


class InvalidULIDError(ULIDError):
    """Invalid ULID format or encoding."""
    pass


class TimeOverflowError(ULIDError):
    """ULID time component overflow."""
    pass


@dataclass
class ULIDComponents:
    """Components of a parsed ULID."""
    timestamp: int  # Milliseconds since Unix epoch
    randomness: int  # Random component
    datetime: datetime  # Parsed datetime
    encoded: str  # Original encoded ULID


class ULIDGenerator:
    """
    Generates and validates ULIDs.

    ULIDs are 128-bit identifiers with:
    - 48-bit timestamp (milliseconds since Unix epoch)
    - 80-bit randomness for collision resistance
    - Lexicographically sortable
    - URL-safe encoding (Crockford's Base32)
    """

    def __init__(self, seed: Optional[int] = None):
        """
        Initialize ULID generator.

        Args:
            seed: Random seed for deterministic generation (testing only)
        """
        if seed is not None:
            random.seed(seed)
        self._last_time = 0
        self._last_random = 0

    def generate(self, timestamp_ms: Optional[int] = None) -> str:
        """
        Generate a new ULID.

        Args:
            timestamp_ms: Timestamp in milliseconds (uses current time if None)

        Returns:
            26-character ULID string
        """
        if timestamp_ms is None:
            timestamp_ms = int(time.time() * 1000)

        # Handle time overflow (ULID supports up to 10889 AD)
        if timestamp_ms > 0xFFFFFFFFFFFF:
            raise TimeOverflowError(f"Timestamp too large: {timestamp_ms}")

        # Ensure monotonicity
        if timestamp_ms == self._last_time:
            randomness = self._last_random + 1
        else:
            randomness = self._generate_randomness()

        # Update tracking
        self._last_time = timestamp_ms
        self._last_random = randomness

        return self._encode_ulid(timestamp_ms, randomness)

    def generate_monotonic(self, timestamp_ms: Optional[int] = None) -> str:
        """
        Generate a monotonic ULID (guaranteed increasing).

        Args:
            timestamp_ms: Timestamp in milliseconds

        Returns:
            Monotonic ULID string
        """
        return self.generate(timestamp_ms)

    def generate_batch(self, count: int) -> List[str]:
        """
        Generate multiple ULIDs.

        Args:
            count: Number of ULIDs to generate

        Returns:
            List of ULID strings
        """
        if count <= 0:
            return []

        ulids = []
        for _ in range(count):
            ulids.append(self.generate())

        return ulids

    def parse(self, ulid: str) -> ULIDComponents:
        """
        Parse a ULID into its components.

        Args:
            ulid: ULID string to parse

        Returns:
            ULIDComponents object

        Raises:
            InvalidULIDError: If ULID is invalid
        """
        if not self.is_valid(ulid):
            raise InvalidULIDError(f"Invalid ULID format: {ulid}")

        try:
            # Decode the ULID
            timestamp, randomness = self._decode_ulid(ulid)

            # Convert timestamp to datetime
            dt = datetime.fromtimestamp(timestamp / 1000, tz=timezone.utc)

            return ULIDComponents(
                timestamp=timestamp,
                randomness=randomness,
                datetime=dt,
                encoded=ulid
            )

        except Exception as e:
            raise InvalidULIDError(f"Failed to parse ULID {ulid}: {str(e)}")

    def is_valid(self, ulid: str) -> bool:
        """
        Check if a ULID string is valid.

        Args:
            ulid: ULID string to validate

        Returns:
            True if valid
        """
        if not isinstance(ulid, str):
            return False

        # Check length
        if len(ulid) != 26:
            return False

        # Check characters
        if not re.match(r'^[0123456789ABCDEFGHJKMNPQRSTVWXYZ]{26}$', ulid):
            return False

        return True

    def get_timestamp(self, ulid: str) -> int:
        """
        Extract timestamp from ULID.

        Args:
            ulid: ULID string

        Returns:
            Timestamp in milliseconds
        """
        if not self.is_valid(ulid):
            raise InvalidULIDError(f"Invalid ULID: {ulid}")

        # Decode only the timestamp part (first 48 bits)
        timestamp_part = ulid[:10]  # First 10 characters = 48 bits
        timestamp = 0

        for char in timestamp_part:
            timestamp = timestamp * 32 + ENCODE_MAP[char]

        return timestamp

    def get_datetime(self, ulid: str) -> datetime:
        """
        Extract datetime from ULID.

        Args:
            ulid: ULID string

        Returns:
            UTC datetime object
        """
        timestamp_ms = self.get_timestamp(ulid)
        return datetime.fromtimestamp(timestamp_ms / 1000, tz=timezone.utc)

    def compare(self, ulid1: str, ulid2: str) -> int:
        """
        Compare two ULIDs lexicographically.

        Args:
            ulid1: First ULID
            ulid2: Second ULID

        Returns:
            -1 if ulid1 < ulid2, 0 if equal, 1 if ulid1 > ulid2
        """
        if not self.is_valid(ulid1) or not self.is_valid(ulid2):
            raise InvalidULIDError("Both ULIDs must be valid for comparison")

        if ulid1 < ulid2:
            return -1
        elif ulid1 > ulid2:
            return 1
        else:
            return 0

    def _generate_randomness(self) -> int:
        """Generate 80-bit random component."""
        # Generate 10 bytes (80 bits) of randomness
        randomness = 0
        for _ in range(10):
            randomness = randomness * 256 + random.randint(0, 255)
        return randomness

    def _encode_ulid(self, timestamp: int, randomness: int) -> str:
        """Encode timestamp and randomness into ULID string."""
        # Ensure values fit in their bit ranges
        timestamp &= 0xFFFFFFFFFFFF  # 48 bits
        randomness &= 0xFFFFFFFFFFFFFFFFFFFFFFFF  # 80 bits

        # Encode timestamp (48 bits = 8 base32 characters + 4 bits of the 9th)
        encoded = []
        value = (timestamp << 80) | randomness

        # Encode 128 bits as 26 base32 characters
        for _ in range(26):
            encoded.append(DECODE_MAP[value & 0x1F])
            value >>= 5

        # Reverse to get correct order
        return ''.join(reversed(encoded))

    def _decode_ulid(self, ulid: str) -> tuple[int, int]:
        """Decode ULID string into timestamp and randomness."""
        value = 0

        for char in ulid:
            value = value * 32 + ENCODE_MAP[char]

        # Extract components
        randomness = value & 0xFFFFFFFFFFFFFFFFFFFFFFFF  # 80 bits
        timestamp = value >> 80  # 48 bits

        return timestamp, randomness

    def encode_binary(self, ulid: str) -> bytes:
        """
        Encode ULID to binary format.

        Args:
            ulid: ULID string

        Returns:
            16-byte binary representation
        """
        timestamp, randomness = self._decode_ulid(ulid)
        return struct.pack('>QQ', timestamp, randomness)

    def decode_binary(self, binary_data: bytes) -> str:
        """
        Decode binary data to ULID string.

        Args:
            binary_data: 16-byte binary data

        Returns:
            ULID string
        """
        if len(binary_data) != 16:
            raise InvalidULIDError("Binary data must be exactly 16 bytes")

        timestamp, randomness = struct.unpack('>QQ', binary_data)
        return self._encode_ulid(timestamp, randomness)


class ULIDPool:
    """
    Pool of pre-generated ULIDs for performance optimization.

    Useful when generating many ULIDs in quick succession.
    """

    def __init__(self, pool_size: int = 1000):
        """
        Initialize ULID pool.

        Args:
            pool_size: Number of ULIDs to pre-generate
        """
        self.pool_size = pool_size
        self._pool: List[str] = []
        self._generator = ULIDGenerator()
        self._refill()

    def _refill(self) -> None:
        """Refill the ULID pool."""
        self._pool = self._generator.generate_batch(self.pool_size)

    def get(self) -> str:
        """
        Get a ULID from the pool.

        Returns:
            ULID string
        """
        if not self._pool:
            self._refill()

        return self._pool.pop(0)

    def get_batch(self, count: int) -> List[str]:
        """
        Get multiple ULIDs from the pool.

        Args:
            count: Number of ULIDs needed

        Returns:
            List of ULID strings
        """
        if count <= 0:
            return []

        # If we don't have enough, generate more
        while len(self._pool) < count:
            self._pool.extend(self._generator.generate_batch(self.pool_size))

        result = self._pool[:count]
        self._pool = self._pool[count:]
        return result

    def size(self) -> int:
        """Get current pool size."""
        return len(self._pool)


# Global ULID generator instance
_ulid_generator: Optional[ULIDGenerator] = None
_ulid_pool: Optional[ULIDPool] = None


def get_ulid_generator() -> ULIDGenerator:
    """
    Get the global ULID generator instance.

    Returns:
        Global ULIDGenerator instance
    """
    global _ulid_generator
    if _ulid_generator is None:
        _ulid_generator = ULIDGenerator()
    return _ulid_generator


def get_ulid_pool() -> ULIDPool:
    """
    Get the global ULID pool instance.

    Returns:
        Global ULIDPool instance
    """
    global _ulid_pool
    if _ulid_pool is None:
        _ulid_pool = ULIDPool()
    return _ulid_pool


def generate_ulid(timestamp_ms: Optional[int] = None) -> str:
    """
    Generate a new ULID.

    Args:
        timestamp_ms: Timestamp in milliseconds (optional)

    Returns:
        26-character ULID string
    """
    generator = get_ulid_generator()
    return generator.generate(timestamp_ms)


def generate_ulid_batch(count: int) -> List[str]:
    """
    Generate multiple ULIDs.

    Args:
        count: Number of ULIDs to generate

    Returns:
        List of ULID strings
    """
    generator = get_ulid_generator()
    return generator.generate_batch(count)


def parse_ulid(ulid: str) -> ULIDComponents:
    """
    Parse a ULID into its components.

    Args:
        ulid: ULID string to parse

    Returns:
        ULIDComponents object
    """
    generator = get_ulid_generator()
    return generator.parse(ulid)


def is_valid_ulid(ulid: str) -> bool:
    """
    Check if a ULID string is valid.

    Args:
        ulid: ULID string to validate

    Returns:
        True if valid
    """
    generator = get_ulid_generator()
    return generator.is_valid(ulid)


def get_ulid_timestamp(ulid: str) -> int:
    """
    Extract timestamp from ULID.

    Args:
        ulid: ULID string

    Returns:
        Timestamp in milliseconds
    """
    generator = get_ulid_generator()
    return generator.get_timestamp(ulid)


def get_ulid_datetime(ulid: str) -> datetime:
    """
    Extract datetime from ULID.

    Args:
        ulid: ULID string

    Returns:
        UTC datetime object
    """
    generator = get_ulid_generator()
    return generator.get_datetime(ulid)


def compare_ulids(ulid1: str, ulid2: str) -> int:
    """
    Compare two ULIDs lexicographically.

    Args:
        ulid1: First ULID
        ulid2: Second ULID

    Returns:
        -1 if ulid1 < ulid2, 0 if equal, 1 if ulid1 > ulid2
    """
    generator = get_ulid_generator()
    return generator.compare(ulid1, ulid2)


def ulid_from_datetime(dt: datetime) -> str:
    """
    Generate ULID from datetime.

    Args:
        dt: DateTime object

    Returns:
        ULID string
    """
    timestamp_ms = int(dt.timestamp() * 1000)
    return generate_ulid(timestamp_ms)


def ulid_to_binary(ulid: str) -> bytes:
    """
    Convert ULID to binary format.

    Args:
        ulid: ULID string

    Returns:
        16-byte binary representation
    """
    generator = get_ulid_generator()
    return generator.encode_binary(ulid)


def binary_to_ulid(binary_data: bytes) -> str:
    """
    Convert binary data to ULID string.

    Args:
        binary_data: 16-byte binary data

    Returns:
        ULID string
    """
    generator = get_ulid_generator()
    return generator.decode_binary(binary_data)


def validate_ulid_list(ulids: List[str]) -> Dict[str, Any]:
    """
    Validate a list of ULIDs.

    Args:
        ulids: List of ULID strings to validate

    Returns:
        Dictionary with validation results
    """
    results = {
        'valid': [],
        'invalid': [],
        'duplicates': [],
        'count': len(ulids),
        'valid_count': 0,
        'invalid_count': 0,
        'duplicate_count': 0
    }

    seen = set()

    for ulid in ulids:
        if is_valid_ulid(ulid):
            results['valid'].append(ulid)
            results['valid_count'] += 1

            if ulid in seen:
                results['duplicates'].append(ulid)
                results['duplicate_count'] += 1
            else:
                seen.add(ulid)
        else:
            results['invalid'].append(ulid)
            results['invalid_count'] += 1

    return results


def get_ulid_stats(ulids: List[str]) -> Dict[str, Any]:
    """
    Get statistics for a list of ULIDs.

    Args:
        ulids: List of ULID strings

    Returns:
        Dictionary with ULID statistics
    """
    if not ulids:
        return {
            'count': 0,
            'time_range': None,
            'oldest': None,
            'newest': None,
            'span_hours': 0
        }

    timestamps = []
    valid_ulids = []

    for ulid in ulids:
        if is_valid_ulid(ulid):
            try:
                timestamp = get_ulid_timestamp(ulid)
                timestamps.append(timestamp)
                valid_ulids.append(ulid)
            except:
                continue

    if not timestamps:
        return {
            'count': len(ulids),
            'valid_count': 0,
            'time_range': None,
            'oldest': None,
            'newest': None,
            'span_hours': 0
        }

    oldest_ts = min(timestamps)
    newest_ts = max(timestamps)
    span_hours = (newest_ts - oldest_ts) / (1000 * 60 * 60)

    return {
        'count': len(ulids),
        'valid_count': len(valid_ulids),
        'time_range': {
            'oldest_timestamp': oldest_ts,
            'newest_timestamp': newest_ts,
            'oldest_datetime': datetime.fromtimestamp(oldest_ts / 1000, tz=timezone.utc).isoformat(),
            'newest_datetime': datetime.fromtimestamp(newest_ts / 1000, tz=timezone.utc).isoformat()
        },
        'oldest': valid_ulids[timestamps.index(oldest_ts)],
        'newest': valid_ulids[timestamps.index(newest_ts)],
        'span_hours': span_hours
    }
</code>

src/vpsweb/utils/__init__.py:
<code>

</code>

src/vpsweb/utils/language_mapper.py:
<code>
"""
Language Mapper Utility for VPSWeb Repository System

This module provides mapping between BCP-47 language codes and natural language names,
with support for common translation languages used in poetry.

Features:
- BCP-47 to natural language name mapping
- Natural language to BCP-47 code mapping
- Language validation and normalization
- Support for regional variants and scripts
- Poetry-specific language metadata
"""

from typing import Dict, Optional, List, Tuple
from enum import Enum
import re


class LanguageDirection(str, Enum):
    """Text direction for languages."""
    LTR = "ltr"  # Left-to-right
    RTL = "rtl"  # Right-to-left


class ScriptType(str, Enum):
    """Writing script types."""
    LATIN = "latin"
    CYRILLIC = "cyrillic"
    ARABIC = "arabic"
    CHINESE = "chinese"
    JAPANESE = "japanese"
    KOREAN = "korean"
    DEVANAGARI = "devanagari"
    GREEK = "greek"
    HEBREW = "hebrew"
    THAI = "thai"


class LanguageInfo:
    """
    Comprehensive language information for poetry translation.

    Contains metadata about languages commonly used in poetry,
    including script, direction, and poetic traditions.
    """

    def __init__(
        self,
        code: str,
        name: str,
        native_name: str,
        direction: LanguageDirection = LanguageDirection.LTR,
        script: ScriptType = ScriptType.LATIN,
        poetic_tradition: bool = True,
        common_in_translation: bool = True,
        regional_variants: Optional[List[str]] = None
    ):
        self.code = code
        self.name = name
        self.native_name = native_name
        self.direction = direction
        self.script = script
        self.poetic_tradition = poetic_tradition
        self.common_in_translation = common_in_translation
        self.regional_variants = regional_variants or []

    def __repr__(self) -> str:
        return f"LanguageInfo(code='{self.code}', name='{self.name}')"


class LanguageMapper:
    """
    Maps BCP-47 language codes to natural language names and metadata.

    Provides bidirectional mapping between language codes and names,
    with validation and normalization capabilities.
    """

    def __init__(self):
        """Initialize the language mapper with predefined languages."""
        self._code_to_info: Dict[str, LanguageInfo] = {}
        self._name_to_code: Dict[str, str] = {}
        self._native_name_to_code: Dict[str, str] = {}

        self._initialize_languages()

    def _initialize_languages(self) -> None:
        """Initialize the language database with common poetry languages."""

        # Major languages with strong poetic traditions
        languages = [
            # English
            LanguageInfo(
                code="en",
                name="English",
                native_name="English",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["en-US", "en-GB", "en-AU", "en-CA", "en-IE"]
            ),

            # Chinese
            LanguageInfo(
                code="zh",
                name="Chinese",
                native_name="中文",
                direction=LanguageDirection.LTR,
                script=ScriptType.CHINESE,
                regional_variants=["zh-CN", "zh-TW", "zh-HK", "zh-SG"]
            ),

            # Classical Chinese
            LanguageInfo(
                code="zh-Hant",
                name="Classical Chinese",
                native_name="文言文",
                direction=LanguageDirection.LTR,
                script=ScriptType.CHINESE,
                poetic_tradition=True
            ),

            # Japanese
            LanguageInfo(
                code="ja",
                name="Japanese",
                native_name="日本語",
                direction=LanguageDirection.LTR,
                script=ScriptType.JAPANESE,
                regional_variants=["ja-JP"]
            ),

            # Korean
            LanguageInfo(
                code="ko",
                name="Korean",
                native_name="한국어",
                direction=LanguageDirection.LTR,
                script=ScriptType.KOREAN,
                regional_variants=["ko-KR"]
            ),

            # French
            LanguageInfo(
                code="fr",
                name="French",
                native_name="Français",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["fr-FR", "fr-CA", "fr-BE", "fr-CH"]
            ),

            # German
            LanguageInfo(
                code="de",
                name="German",
                native_name="Deutsch",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["de-DE", "de-AT", "de-CH"]
            ),

            # Spanish
            LanguageInfo(
                code="es",
                name="Spanish",
                native_name="Español",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["es-ES", "es-MX", "es-AR", "es-CO"]
            ),

            # Italian
            LanguageInfo(
                code="it",
                name="Italian",
                native_name="Italiano",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["it-IT", "it-CH"]
            ),

            # Portuguese
            LanguageInfo(
                code="pt",
                name="Portuguese",
                native_name="Português",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["pt-PT", "pt-BR"]
            ),

            # Russian
            LanguageInfo(
                code="ru",
                name="Russian",
                native_name="Русский",
                direction=LanguageDirection.LTR,
                script=ScriptType.CYRILLIC,
                regional_variants=["ru-RU"]
            ),

            # Arabic
            LanguageInfo(
                code="ar",
                name="Arabic",
                native_name="العربية",
                direction=LanguageDirection.RTL,
                script=ScriptType.ARABIC,
                regional_variants=["ar-SA", "ar-EG", "ar-MA", "ar-IQ"]
            ),

            # Hindi
            LanguageInfo(
                code="hi",
                name="Hindi",
                native_name="हिन्दी",
                direction=LanguageDirection.LTR,
                script=ScriptType.DEVANAGARI,
                regional_variants=["hi-IN"]
            ),

            # Sanskrit (Classical)
            LanguageInfo(
                code="sa",
                name="Sanskrit",
                native_name="संस्कृतम्",
                direction=LanguageDirection.LTR,
                script=ScriptType.DEVANAGARI,
                poetic_tradition=True
            ),

            # Persian
            LanguageInfo(
                code="fa",
                name="Persian",
                native_name="فارسی",
                direction=LanguageDirection.RTL,
                script=ScriptType.ARABIC,
                regional_variants=["fa-IR", "fa-AF"]
            ),

            # Urdu
            LanguageInfo(
                code="ur",
                name="Urdu",
                native_name="اردو",
                direction=LanguageDirection.RTL,
                script=ScriptType.ARABIC,
                regional_variants=["ur-PK", "ur-IN"]
            ),

            # Turkish
            LanguageInfo(
                code="tr",
                name="Turkish",
                native_name="Türkçe",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["tr-TR"]
            ),

            # Greek
            LanguageInfo(
                code="el",
                name="Greek",
                native_name="Ελληνικά",
                direction=LanguageDirection.LTR,
                script=ScriptType.GREEK,
                regional_variants=["el-GR"]
            ),

            # Hebrew
            LanguageInfo(
                code="he",
                name="Hebrew",
                native_name="עברית",
                direction=LanguageDirection.RTL,
                script=ScriptType.HEBREW,
                regional_variants=["he-IL"]
            ),

            # Dutch
            LanguageInfo(
                code="nl",
                name="Dutch",
                native_name="Nederlands",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["nl-NL", "nl-BE"]
            ),

            # Swedish
            LanguageInfo(
                code="sv",
                name="Swedish",
                native_name="Svenska",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["sv-SE"]
            ),

            # Norwegian
            LanguageInfo(
                code="no",
                name="Norwegian",
                native_name="Norsk",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["nb-NO", "nn-NO"]
            ),

            # Danish
            LanguageInfo(
                code="da",
                name="Danish",
                native_name="Dansk",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["da-DK"]
            ),

            # Finnish
            LanguageInfo(
                code="fi",
                name="Finnish",
                native_name="Suomi",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["fi-FI"]
            ),

            # Polish
            LanguageInfo(
                code="pl",
                name="Polish",
                native_name="Polski",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["pl-PL"]
            ),

            # Czech
            LanguageInfo(
                code="cs",
                name="Czech",
                native_name="Čeština",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["cs-CZ"]
            ),

            # Hungarian
            LanguageInfo(
                code="hu",
                name="Hungarian",
                native_name="Magyar",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["hu-HU"]
            ),

            # Romanian
            LanguageInfo(
                code="ro",
                name="Romanian",
                native_name="Română",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["ro-RO"]
            ),

            # Ukrainian
            LanguageInfo(
                code="uk",
                name="Ukrainian",
                native_name="Українська",
                direction=LanguageDirection.LTR,
                script=ScriptType.CYRILLIC,
                regional_variants=["uk-UA"]
            ),

            # Bulgarian
            LanguageInfo(
                code="bg",
                name="Bulgarian",
                native_name="Български",
                direction=LanguageDirection.LTR,
                script=ScriptType.CYRILLIC,
                regional_variants=["bg-BG"]
            ),

            # Thai
            LanguageInfo(
                code="th",
                name="Thai",
                native_name="ไทย",
                direction=LanguageDirection.LTR,
                script=ScriptType.THAI,
                regional_variants=["th-TH"]
            ),

            # Vietnamese
            LanguageInfo(
                code="vi",
                name="Vietnamese",
                native_name="Tiếng Việt",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["vi-VN"]
            ),

            # Indonesian
            LanguageInfo(
                code="id",
                name="Indonesian",
                native_name="Bahasa Indonesia",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["id-ID"]
            ),

            # Malay
            LanguageInfo(
                code="ms",
                name="Malay",
                native_name="Bahasa Melayu",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                regional_variants=["ms-MY", "ms-SG"]
            ),

            # Latin (Classical)
            LanguageInfo(
                code="la",
                name="Latin",
                native_name="Latina",
                direction=LanguageDirection.LTR,
                script=ScriptType.LATIN,
                poetic_tradition=True
            ),

            # Ancient Greek
            LanguageInfo(
                code="grc",
                name="Ancient Greek",
                native_name="Ἀρχαία ἑλληνικὴ",
                direction=LanguageDirection.LTR,
                script=ScriptType.GREEK,
                poetic_tradition=True
            ),
        ]

        # Build lookup dictionaries
        for lang in languages:
            self._code_to_info[lang.code] = lang
            self._name_to_code[lang.name.lower()] = lang.code
            self._native_name_to_code[lang.native_name.lower()] = lang.code

            # Also add regional variants
            for variant in lang.regional_variants:
                self._code_to_info[variant] = lang

    def get_language_info(self, code: str) -> Optional[LanguageInfo]:
        """
        Get language information by BCP-47 code.

        Args:
            code: BCP-47 language code

        Returns:
            LanguageInfo object or None if not found
        """
        return self._code_to_info.get(self.normalize_code(code))

    def get_language_name(self, code: str) -> Optional[str]:
        """
        Get English language name by BCP-47 code.

        Args:
            code: BCP-47 language code

        Returns:
            Language name or None if not found
        """
        info = self.get_language_info(code)
        return info.name if info else None

    def get_native_language_name(self, code: str) -> Optional[str]:
        """
        Get native language name by BCP-47 code.

        Args:
            code: BCP-47 language code

        Returns:
            Native language name or None if not found
        """
        info = self.get_language_info(code)
        return info.native_name if info else None

    def get_language_code(self, name: str) -> Optional[str]:
        """
        Get BCP-47 language code by language name.

        Args:
            name: Language name (English or native)

        Returns:
            BCP-47 language code or None if not found
        """
        normalized_name = name.strip().lower()
        return self._name_to_code.get(normalized_name) or self._native_name_to_code.get(normalized_name)

    def normalize_code(self, code: str) -> str:
        """
        Normalize BCP-47 language code.

        Args:
            code: Raw language code

        Returns:
            Normalized language code
        """
        code = code.strip().lower()

        # Handle common variations
        if code in ['zh-cn', 'zh hans']:
            return 'zh-CN'
        elif code in ['zh-tw', 'zh hant']:
            return 'zh-TW'
        elif code in ['en-us']:
            return 'en-US'
        elif code in ['en-gb']:
            return 'en-GB'

        return code

    def is_valid_language_code(self, code: str) -> bool:
        """
        Check if a BCP-47 language code is valid and supported.

        Args:
            code: BCP-47 language code

        Returns:
            True if valid and supported
        """
        return self.get_language_info(code) is not None

    def is_valid_language_name(self, name: str) -> bool:
        """
        Check if a language name is valid and supported.

        Args:
            name: Language name (English or native)

        Returns:
            True if valid and supported
        """
        return self.get_language_code(name) is not None

    def get_poetry_languages(self) -> List[LanguageInfo]:
        """
        Get all languages with strong poetic traditions.

        Returns:
            List of LanguageInfo objects for poetry languages
        """
        return [info for info in self._code_to_info.values() if info.poetic_tradition]

    def get_common_translation_languages(self) -> List[LanguageInfo]:
        """
        Get languages commonly used in translation.

        Returns:
            List of LanguageInfo objects for common translation languages
        """
        return [info for info in self._code_to_info.values() if info.common_in_translation]

    def get_rtl_languages(self) -> List[LanguageInfo]:
        """
        Get all right-to-left languages.

        Returns:
            List of LanguageInfo objects for RTL languages
        """
        return [info for info in self._code_to_info.values() if info.direction == LanguageDirection.RTL]

    def get_languages_by_script(self, script: ScriptType) -> List[LanguageInfo]:
        """
        Get languages by writing script.

        Args:
            script: Script type to filter by

        Returns:
            List of LanguageInfo objects for the specified script
        """
        return [info for info in self._code_to_info.values() if info.script == script]

    def search_languages(self, query: str) -> List[LanguageInfo]:
        """
        Search languages by name or code.

        Args:
            query: Search query

        Returns:
            List of matching LanguageInfo objects
        """
        query = query.strip().lower()
        results = []

        for code, info in self._code_to_info.items():
            if (query in code.lower() or
                query in info.name.lower() or
                query in info.native_name.lower()):
                results.append(info)

        return results

    def get_all_languages(self) -> Dict[str, LanguageInfo]:
        """
        Get all supported languages.

        Returns:
            Dictionary mapping language codes to LanguageInfo objects
        """
        return dict(self._code_to_info)

    def add_custom_language(self, info: LanguageInfo) -> None:
        """
        Add a custom language to the mapper.

        Args:
            info: LanguageInfo object for the custom language
        """
        self._code_to_info[info.code] = info
        self._name_to_code[info.name.lower()] = info.code
        self._native_name_to_code[info.native_name.lower()] = info.code


# Global language mapper instance
_language_mapper: Optional[LanguageMapper] = None


def get_language_mapper() -> LanguageMapper:
    """
    Get the global language mapper instance.

    Returns:
        Global LanguageMapper instance
    """
    global _language_mapper
    if _language_mapper is None:
        _language_mapper = LanguageMapper()
    return _language_mapper


def validate_language_code(code: str) -> Tuple[bool, Optional[str]]:
    """
    Validate a BCP-47 language code.

    Args:
        code: Language code to validate

    Returns:
        Tuple of (is_valid, error_message)
    """
    if not code or not code.strip():
        return False, "Language code cannot be empty"

    code = code.strip()

    # Basic BCP-47 format validation
    if not re.match(r'^[a-z]{2}(-[A-Z][a-z]{3})?(-[A-Z]{2})?(-[A-Z0-9]{5,8})?$', code):
        return False, f"Invalid BCP-47 language code format: {code}"

    mapper = get_language_mapper()
    if not mapper.is_valid_language_code(code):
        return False, f"Language code not supported: {code}"

    return True, None


def get_display_name(code: str, use_native: bool = False) -> str:
    """
    Get display name for a language code.

    Args:
        code: BCP-47 language code
        use_native: Whether to use native name instead of English

    Returns:
        Display name for the language
    """
    mapper = get_language_mapper()

    if use_native:
        return mapper.get_native_language_name(code) or code
    else:
        return mapper.get_language_name(code) or code


# Common language pairs for poetry translation
COMMON_TRANSLATION_PAIRS = [
    ("en", "zh"),    # English ↔ Chinese
    ("zh", "en"),    # Chinese ↔ English
    ("en", "ja"),    # English ↔ Japanese
    ("ja", "en"),    # Japanese ↔ English
    ("en", "fr"),    # English ↔ French
    ("fr", "en"),    # French ↔ English
    ("en", "de"),    # English ↔ German
    ("de", "en"),    # German ↔ English
    ("en", "es"),    # English ↔ Spanish
    ("es", "en"),    # Spanish ↔ English
    ("en", "ru"),    # English ↔ Russian
    ("ru", "en"),    # Russian ↔ English
    ("en", "ar"),    # English ↔ Arabic
    ("ar", "en"),    # Arabic ↔ English
    ("zh", "ja"),    # Chinese ↔ Japanese
    ("ja", "zh"),    # Japanese ↔ Chinese
    ("fr", "de"),    # French ↔ German
    ("de", "fr"),    # German ↔ French
    ("es", "fr"),    # Spanish ↔ French
    ("fr", "es"),    # French ↔ Spanish
]


def get_common_translation_pairs() -> List[Tuple[str, str]]:
    """
    Get common language pairs for poetry translation.

    Returns:
        List of (source_code, target_code) tuples
    """
    return COMMON_TRANSLATION_PAIRS.copy()
</code>

src/vpsweb/utils/file_storage.py:
<code>
"""
File Storage Utility for VPSWeb Repository System

This module provides file management operations for the repository system,
including organized storage of poems, translations, and metadata files.

Features:
- Organized file storage with directory structure
- File validation and security checks
- Backup and restore operations
- File compression and archiving
- Metadata file management
- Import/export functionality
"""

import os
import json
import shutil
import zipfile
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, BinaryIO
from datetime import datetime, timezone
import aiofiles
import asyncio

# Simple configuration for v0.3.1 repository system
import os
from pathlib import Path

def get_default_repo_root() -> Path:
    """Get default repository root directory"""
    # Use environment variable or default to repository_root in project
    repo_root = os.environ.get("VPSWEB_REPO_ROOT")
    if repo_root:
        return Path(repo_root)

    # Default to repository_root directory in project
    current_file = Path(__file__)
    project_root = current_file.parent.parent.parent.parent
    return project_root / "repository_root"


class FileStorageError(Exception):
    """Base exception for file storage operations."""
    pass


class FileNotFoundError(FileStorageError):
    """File not found in storage."""
    pass


class InvalidFileTypeError(FileStorageError):
    """Invalid file type for operation."""
    pass


class SecurityValidationError(FileStorageError):
    """Security validation failed."""
    pass


class FileStorageManager:
    """
    Manages file storage operations for the repository system.

    Provides organized storage for poems, translations, and related files
    with proper directory structure and security validation.
    """

    def __init__(self, repo_root: Optional[Path] = None):
        """
        Initialize the file storage manager.

        Args:
            repo_root: Repository root directory
        """
        self.repo_root = repo_root or get_default_repo_root()
        self.ensure_directory_structure()

    def ensure_directory_structure(self) -> None:
        """Ensure all required directories exist."""
        directories = [
            "poems",
            "translations",
            "ai_logs",
            "human_notes",
            "exports",
            "imports",
            "backups",
            "temp",
            "logs"
        ]

        for directory in directories:
            dir_path = self.repo_root / directory
            dir_path.mkdir(parents=True, exist_ok=True)

    def get_poem_directory(self, poem_id: str) -> Path:
        """
        Get the directory path for a poem.

        Args:
            poem_id: Unique identifier for the poem

        Returns:
            Path to poem directory
        """
        return self.repo_root / "poems" / poem_id

    def get_translation_directory(self, translation_id: str) -> Path:
        """
        Get the directory path for a translation.

        Args:
            translation_id: Unique identifier for the translation

        Returns:
            Path to translation directory
        """
        return self.repo_root / "translations" / translation_id

    def get_ai_log_directory(self, log_id: str) -> Path:
        """
        Get the directory path for an AI log.

        Args:
            log_id: Unique identifier for the AI log

        Returns:
            Path to AI log directory
        """
        return self.repo_root / "ai_logs" / log_id

    def get_human_note_directory(self, note_id: str) -> Path:
        """
        Get the directory path for a human note.

        Args:
            note_id: Unique identifier for the human note

        Returns:
            Path to human note directory
        """
        return self.repo_root / "human_notes" / note_id

    def validate_file_path(self, file_path: Path, allowed_extensions: Optional[List[str]] = None) -> bool:
        """
        Validate file path for security.

        Args:
            file_path: File path to validate
            allowed_extensions: List of allowed file extensions

        Returns:
            True if valid

        Raises:
            SecurityValidationError: If path validation fails
        """
        # Check if path is within repository root
        try:
            file_path.resolve().relative_to(self.repo_root.resolve())
        except ValueError:
            raise SecurityValidationError(f"Path outside repository: {file_path}")

        # Check file extension
        if allowed_extensions:
            if file_path.suffix.lower() not in [ext.lower() for ext in allowed_extensions]:
                raise SecurityValidationError(
                    f"File extension not allowed: {file_path.suffix}. "
                    f"Allowed: {allowed_extensions}"
                )

        # Check for dangerous file patterns
        dangerous_patterns = [
            "..", "~", "$", "<", ">", "|", ";", "&", "`",
            "script", "executable", "batch", "cmd"
        ]

        file_str = str(file_path).lower()
        for pattern in dangerous_patterns:
            if pattern in file_str:
                raise SecurityValidationError(f"Dangerous pattern in path: {pattern}")

        return True

    async def save_file(
        self,
        file_path: Path,
        content: Union[str, bytes],
        allowed_extensions: Optional[List[str]] = None,
        create_directories: bool = True
    ) -> Dict[str, Any]:
        """
        Save file content to storage.

        Args:
            file_path: Path where to save the file
            content: File content (string or bytes)
            allowed_extensions: List of allowed file extensions
            create_directories: Whether to create parent directories

        Returns:
            Dictionary with file metadata

        Raises:
            FileStorageError: If save operation fails
        """
        try:
            # Validate path
            self.validate_file_path(file_path, allowed_extensions)

            # Create parent directories if needed
            if create_directories:
                file_path.parent.mkdir(parents=True, exist_ok=True)

            # Write file
            if isinstance(content, str):
                async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                    await f.write(content)
            else:
                async with aiofiles.open(file_path, 'wb') as f:
                    await f.write(content)

            # Get file metadata
            stat = file_path.stat()
            file_hash = await self.calculate_file_hash(file_path)

            metadata = {
                'path': str(file_path),
                'size': stat.st_size,
                'created': datetime.fromtimestamp(stat.st_ctime, tz=timezone.utc),
                'modified': datetime.fromtimestamp(stat.st_mtime, tz=timezone.utc),
                'hash': file_hash,
                'extension': file_path.suffix
            }

            return metadata

        except Exception as e:
            raise FileStorageError(f"Failed to save file {file_path}: {str(e)}")

    async def load_file(self, file_path: Path, mode: str = 'r') -> Union[str, bytes]:
        """
        Load file content from storage.

        Args:
            file_path: Path to file to load
            mode: File read mode ('r' for text, 'rb' for binary)

        Returns:
            File content

        Raises:
            FileNotFoundError: If file doesn't exist
            FileStorageError: If load operation fails
        """
        try:
            # Validate path
            self.validate_file_path(file_path)

            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")

            # Read file
            async with aiofiles.open(file_path, mode) as f:
                return await f.read()

        except FileNotFoundError:
            raise
        except Exception as e:
            raise FileStorageError(f"Failed to load file {file_path}: {str(e)}")

    async def delete_file(self, file_path: Path) -> bool:
        """
        Delete a file from storage.

        Args:
            file_path: Path to file to delete

        Returns:
            True if file was deleted

        Raises:
            FileStorageError: If delete operation fails
        """
        try:
            # Validate path
            self.validate_file_path(file_path)

            if file_path.exists():
                await asyncio.to_thread(file_path.unlink)
                return True
            return False

        except Exception as e:
            raise FileStorageError(f"Failed to delete file {file_path}: {str(e)}")

    async def calculate_file_hash(self, file_path: Path, algorithm: str = 'sha256') -> str:
        """
        Calculate hash of a file.

        Args:
            file_path: Path to file
            algorithm: Hash algorithm to use

        Returns:
            Hexadecimal hash string
        """
        hash_obj = hashlib.new(algorithm)

        async with aiofiles.open(file_path, 'rb') as f:
            while chunk := await f.read(8192):
                hash_obj.update(chunk)

        return hash_obj.hexdigest()

    async def backup_directory(
        self,
        source_dir: Path,
        backup_name: Optional[str] = None,
        compression: bool = True
    ) -> Path:
        """
        Create backup of a directory.

        Args:
            source_dir: Directory to backup
            backup_name: Name for backup file
            compression: Whether to compress the backup

        Returns:
            Path to backup file
        """
        try:
            if not source_dir.exists():
                raise FileNotFoundError(f"Source directory not found: {source_dir}")

            # Generate backup name
            if not backup_name:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_name = f"backup_{source_dir.name}_{timestamp}"

            backup_dir = self.repo_root / "backups"
            backup_dir.mkdir(exist_ok=True)

            if compression:
                backup_path = backup_dir / f"{backup_name}.zip"
                await self._create_zip_backup(source_dir, backup_path)
            else:
                backup_path = backup_dir / backup_name
                await asyncio.to_thread(shutil.copytree, source_dir, backup_path)

            return backup_path

        except Exception as e:
            raise FileStorageError(f"Failed to create backup: {str(e)}")

    async def _create_zip_backup(self, source_dir: Path, backup_path: Path) -> None:
        """Create a compressed zip backup."""
        def create_zip():
            with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for file_path in source_dir.rglob('*'):
                    if file_path.is_file():
                        arcname = file_path.relative_to(source_dir)
                        zipf.write(file_path, arcname)

        await asyncio.to_thread(create_zip)

    async def restore_backup(self, backup_path: Path, target_dir: Path) -> None:
        """
        Restore a backup to target directory.

        Args:
            backup_path: Path to backup file
            target_dir: Directory where to restore
        """
        try:
            # Validate paths
            self.validate_file_path(backup_path)
            self.validate_file_path(target_dir)

            if not backup_path.exists():
                raise FileNotFoundError(f"Backup file not found: {backup_path}")

            # Remove target directory if it exists
            if target_dir.exists():
                await asyncio.to_thread(shutil.rmtree, target_dir)

            target_dir.parent.mkdir(parents=True, exist_ok=True)

            if backup_path.suffix == '.zip':
                await self._extract_zip_backup(backup_path, target_dir)
            else:
                await asyncio.to_thread(shutil.copytree, backup_path, target_dir)

        except Exception as e:
            raise FileStorageError(f"Failed to restore backup: {str(e)}")

    async def _extract_zip_backup(self, backup_path: Path, target_dir: Path) -> None:
        """Extract a zip backup."""
        def extract_zip():
            with zipfile.ZipFile(backup_path, 'r') as zipf:
                zipf.extractall(target_dir)

        await asyncio.to_thread(extract_zip)

    async def save_poem_data(self, poem_id: str, poem_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Save poem data to structured file storage.

        Args:
            poem_id: Unique identifier for the poem
            poem_data: Poem data dictionary

        Returns:
            Dictionary with save metadata
        """
        poem_dir = self.get_poem_directory(poem_id)
        poem_file = poem_dir / "poem.json"

        return await self.save_file(
            poem_file,
            json.dumps(poem_data, indent=2, ensure_ascii=False),
            allowed_extensions=['.json']
        )

    async def load_poem_data(self, poem_id: str) -> Dict[str, Any]:
        """
        Load poem data from file storage.

        Args:
            poem_id: Unique identifier for the poem

        Returns:
            Poem data dictionary

        Raises:
            FileNotFoundError: If poem file doesn't exist
        """
        poem_file = self.get_poem_directory(poem_id) / "poem.json"
        content = await self.load_file(poem_file, 'r')
        return json.loads(content)

    async def save_translation_data(
        self,
        translation_id: str,
        translation_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Save translation data to structured file storage.

        Args:
            translation_id: Unique identifier for the translation
            translation_data: Translation data dictionary

        Returns:
            Dictionary with save metadata
        """
        translation_dir = self.get_translation_directory(translation_id)
        translation_file = translation_dir / "translation.json"

        return await self.save_file(
            translation_file,
            json.dumps(translation_data, indent=2, ensure_ascii=False),
            allowed_extensions=['.json']
        )

    async def load_translation_data(self, translation_id: str) -> Dict[str, Any]:
        """
        Load translation data from file storage.

        Args:
            translation_id: Unique identifier for the translation

        Returns:
            Translation data dictionary

        Raises:
            FileNotFoundError: If translation file doesn't exist
        """
        translation_file = self.get_translation_directory(translation_id) / "translation.json"
        content = await self.load_file(translation_file, 'r')
        return json.loads(content)

    async def list_files(
        self,
        directory: Path,
        pattern: str = "*",
        recursive: bool = False
    ) -> List[Dict[str, Any]]:
        """
        List files in a directory with metadata.

        Args:
            directory: Directory to list
            pattern: Glob pattern for matching files
            recursive: Whether to search recursively

        Returns:
            List of file metadata dictionaries
        """
        try:
            self.validate_file_path(directory)

            if not directory.exists():
                return []

            files = []
            glob_pattern = directory.rglob(pattern) if recursive else directory.glob(pattern)

            for file_path in glob_pattern:
                if file_path.is_file():
                    stat = file_path.stat()
                    file_hash = await self.calculate_file_hash(file_path)

                    files.append({
                        'path': str(file_path),
                        'name': file_path.name,
                        'size': stat.st_size,
                        'created': datetime.fromtimestamp(stat.st_ctime, tz=timezone.utc),
                        'modified': datetime.fromtimestamp(stat.st_mtime, tz=timezone.utc),
                        'hash': file_hash,
                        'extension': file_path.suffix,
                        'relative_path': str(file_path.relative_to(self.repo_root))
                    })

            return sorted(files, key=lambda x: x['name'])

        except Exception as e:
            raise FileStorageError(f"Failed to list files in {directory}: {str(e)}")

    async def cleanup_temp_files(self, max_age_hours: int = 24) -> int:
        """
        Clean up temporary files older than specified age.

        Args:
            max_age_hours: Maximum age in hours for temp files

        Returns:
            Number of files cleaned up
        """
        try:
            temp_dir = self.repo_root / "temp"
            if not temp_dir.exists():
                return 0

            cutoff_time = datetime.now().timestamp() - (max_age_hours * 3600)
            cleaned_count = 0

            for file_path in temp_dir.rglob('*'):
                if file_path.is_file() and file_path.stat().st_mtime < cutoff_time:
                    await self.delete_file(file_path)
                    cleaned_count += 1

            return cleaned_count

        except Exception as e:
            raise FileStorageError(f"Failed to cleanup temp files: {str(e)}")

    async def get_storage_stats(self) -> Dict[str, Any]:
        """
        Get storage statistics.

        Returns:
            Dictionary with storage statistics
        """
        try:
            stats = {
                'total_size': 0,
                'file_count': 0,
                'directory_sizes': {}
            }

            for directory in self.repo_root.iterdir():
                if directory.is_dir():
                    dir_size = 0
                    dir_files = 0

                    for file_path in directory.rglob('*'):
                        if file_path.is_file():
                            dir_size += file_path.stat().st_size
                            dir_files += 1

                    stats['directory_sizes'][directory.name] = {
                        'size': dir_size,
                        'file_count': dir_files
                    }
                    stats['total_size'] += dir_size
                    stats['file_count'] += dir_files

            return stats

        except Exception as e:
            raise FileStorageError(f"Failed to get storage stats: {str(e)}")


# Global file storage manager instance
_file_storage_manager: Optional[FileStorageManager] = None


def get_file_storage_manager(repo_root: Optional[Path] = None) -> FileStorageManager:
    """
    Get the global file storage manager instance.

    Args:
        repo_root: Repository root directory

    Returns:
        Global FileStorageManager instance
    """
    global _file_storage_manager
    if _file_storage_manager is None:
        _file_storage_manager = FileStorageManager(repo_root)
    return _file_storage_manager
</code>

src/vpsweb/utils/storage.py:
<code>
"""
Storage Handler for Vox Poetica Studio Web.

This module provides file-based storage for translation outputs with proper
serialization/deserialization of Pydantic models and comprehensive error handling.
"""

import json
import logging
from pathlib import Path
from typing import List, Optional, Dict
from datetime import datetime

from ..models.translation import TranslationOutput
from .markdown_export import MarkdownExporter
from .filename_utils import (
    extract_poet_and_title,
    generate_translation_filename,
)

logger = logging.getLogger(__name__)


class StorageError(Exception):
    """Base exception for storage operations."""

    pass


class SaveError(StorageError):
    """Raised when saving a translation fails."""

    pass


class LoadError(StorageError):
    """Raised when loading a translation fails."""

    pass


class StorageHandler:
    """
    Handles storage operations for translation outputs.

    Provides methods for saving, loading, and listing translation outputs
    with proper file naming, JSON serialization, and error handling.
    """

    def __init__(self, output_dir: str = "outputs"):
        """
        Initialize the storage handler.

        Args:
            output_dir: Directory where translation files will be stored.
                       Will be created if it doesn't exist.

        Raises:
            StorageError: If the output directory cannot be created
        """
        self.output_dir = Path(output_dir)
        self.json_dir = self.output_dir / "json"

        try:
            # Create output directories if they don't exist
            self.output_dir.mkdir(parents=True, exist_ok=True)
            self.json_dir.mkdir(parents=True, exist_ok=True)
            logger.info(
                f"Storage handler initialized with output directory: {self.output_dir.absolute()}"
            )
            logger.info(f"JSON files will be stored in: {self.json_dir.absolute()}")

            # Initialize markdown exporter
            self.markdown_exporter = MarkdownExporter(output_dir)

        except Exception as e:
            logger.error(f"Failed to create output directory '{self.output_dir}': {e}")
            raise StorageError(
                f"Could not create output directory '{self.output_dir}': {e}"
            )

    def save_translation(
        self,
        output: TranslationOutput,
        workflow_mode: str = None,
        include_mode_tag: bool = False,
    ) -> Path:
        """
        Save a translation output to a timestamped JSON file.

        Args:
            output: TranslationOutput instance to save
            workflow_mode: Workflow mode used for the translation
            include_mode_tag: Whether to include workflow mode in filename

        Returns:
            Path to the saved file

        Raises:
            SaveError: If saving fails due to file I/O or serialization issues
        """
        try:
            # Generate timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # Extract poet and title information
            poet, title = extract_poet_and_title(
                output.input.original_poem, output.input.metadata
            )

            # Generate new descriptive filename
            filename = generate_translation_filename(
                poet=poet,
                title=title,
                source_lang=output.input.source_lang,
                target_lang=output.input.target_lang,
                timestamp=timestamp,
                workflow_id=output.workflow_id,
                workflow_mode=workflow_mode,
                file_format="json",
                is_log=False,
            )

            file_path = self.json_dir / filename

            # Convert to dictionary for JSON serialization
            output_dict = output.to_dict()

            # Save as formatted JSON
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(output_dict, f, ensure_ascii=False, indent=2)

            logger.info(f"Translation saved to: {file_path}")
            logger.debug(
                f"Workflow ID: {output.workflow_id}, Total tokens: {output.total_tokens}"
            )
            logger.info(f"Generated filename with poet '{poet}' and title '{title}'")

            return file_path

        except Exception as e:
            logger.error(
                f"JSON serialization failed for workflow {output.workflow_id}: {e}"
            )
            raise SaveError(f"Failed to serialize translation output: {e}")
        except IOError as e:
            logger.error(f"File I/O error while saving translation: {e}")
            raise SaveError(f"Failed to write translation file: {e}")
        except Exception as e:
            logger.error(f"Unexpected error while saving translation: {e}")
            raise SaveError(f"Failed to save translation: {e}")

    def load_translation(self, file_path: Path) -> TranslationOutput:
        """
        Load a translation output from a JSON file.

        Args:
            file_path: Path to the translation JSON file

        Returns:
            TranslationOutput instance

        Raises:
            LoadError: If loading fails due to file I/O, JSON parsing, or validation issues
        """
        try:
            # Validate file exists and is readable
            if not file_path.exists():
                raise LoadError(f"Translation file not found: {file_path}")

            if not file_path.is_file():
                raise LoadError(f"Path is not a file: {file_path}")

            # Load JSON data
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # Parse into TranslationOutput model
            translation_output = TranslationOutput.from_dict(data)

            logger.info(f"Translation loaded from: {file_path}")
            logger.debug(f"Workflow ID: {translation_output.workflow_id}")

            return translation_output

        except Exception as e:
            logger.error(f"JSON parsing failed for file {file_path}: {e}")
            raise LoadError(f"Invalid JSON format in translation file: {e}")
        except KeyError as e:
            logger.error(f"Missing required field in translation file {file_path}: {e}")
            raise LoadError(f"Translation file missing required field: {e}")
        except ValueError as e:
            logger.error(f"Data validation failed for file {file_path}: {e}")
            raise LoadError(f"Translation data validation failed: {e}")
        except IOError as e:
            logger.error(f"File I/O error while loading translation: {e}")
            raise LoadError(f"Failed to read translation file: {e}")
        except Exception as e:
            logger.error(f"Unexpected error while loading translation: {e}")
            raise LoadError(f"Failed to load translation: {e}")

    def list_translations(self) -> List[Path]:
        """
        List all translation files in the JSON subdirectory.

        Returns:
            List of Path objects for all translation JSON files

        Raises:
            StorageError: If directory listing fails
        """
        try:
            # Find all JSON files in json subdirectory
            # Support both new naming (poet-led) and legacy naming (translation_-prefixed)
            translation_files = list(self.json_dir.glob("*.json"))

            # Sort by modification time (newest first)
            translation_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

            logger.debug(
                f"Found {len(translation_files)} translation files in {self.json_dir}"
            )

            return translation_files

        except Exception as e:
            logger.error(f"Failed to list translation files in {self.json_dir}: {e}")
            raise StorageError(f"Failed to list translation files: {e}")

    def get_translation_by_id(self, workflow_id: str) -> Optional[TranslationOutput]:
        """
        Find and load a translation by workflow ID.

        Args:
            workflow_id: The workflow ID to search for

        Returns:
            TranslationOutput if found, None otherwise

        Raises:
            StorageError: If search operation fails
        """
        try:
            translation_files = self.list_translations()

            for file_path in translation_files:
                try:
                    translation = self.load_translation(file_path)
                    if translation.workflow_id == workflow_id:
                        logger.info(
                            f"Found translation with workflow ID: {workflow_id}"
                        )
                        return translation
                except LoadError:
                    # Skip files that can't be loaded
                    continue

            logger.debug(f"No translation found with workflow ID: {workflow_id}")
            return None

        except Exception as e:
            logger.error(f"Failed to search for translation with ID {workflow_id}: {e}")
            raise StorageError(f"Failed to search for translation: {e}")

    def delete_translation(self, file_path: Path) -> bool:
        """
        Delete a translation file.

        Args:
            file_path: Path to the translation file to delete

        Returns:
            True if deletion was successful, False otherwise

        Raises:
            StorageError: If deletion fails
        """
        try:
            # Ensure the file is within our output directory for safety
            if not file_path.is_relative_to(self.output_dir):
                logger.warning(
                    f"Attempted to delete file outside output directory: {file_path}"
                )
                return False

            if file_path.exists() and file_path.is_file():
                file_path.unlink()
                logger.info(f"Translation file deleted: {file_path}")
                return True
            else:
                logger.warning(f"Translation file not found for deletion: {file_path}")
                return False

        except Exception as e:
            logger.error(f"Failed to delete translation file {file_path}: {e}")
            raise StorageError(f"Failed to delete translation file: {e}")

    def save_translation_with_markdown(
        self,
        output: TranslationOutput,
        workflow_mode: str = None,
        include_mode_tag: bool = False,
    ) -> Dict[str, Path]:
        """
        Save a translation output to both JSON and markdown files.

        Args:
            output: TranslationOutput instance to save
            workflow_mode: Workflow mode used for the translation
            include_mode_tag: Whether to include workflow mode in filename

        Returns:
            Dictionary with paths to saved files:
            {
                'json': Path to JSON file,
                'markdown_final': Path to final translation markdown,
                'markdown_log': Path to full log markdown
            }

        Raises:
            SaveError: If saving fails due to file I/O or serialization issues
        """
        try:
            # Save JSON file (existing functionality)
            json_path = self.save_translation(output, workflow_mode, include_mode_tag)

            # Save markdown files (new functionality)
            markdown_paths = self.markdown_exporter.export_both(output)

            result = {
                "json": json_path,
                "markdown_final": Path(markdown_paths["final_translation"]),
                "markdown_log": Path(markdown_paths["full_log"]),
            }

            logger.info(f"Translation saved to multiple formats:")
            logger.info(f"  JSON: {result['json']}")
            logger.info(f"  Markdown (final): {result['markdown_final']}")
            logger.info(f"  Markdown (log): {result['markdown_log']}")

            return result

        except Exception as e:
            logger.error(f"Failed to save translation with markdown: {e}")
            raise SaveError(f"Failed to save translation with markdown: {e}")

    def get_storage_info(self) -> dict:
        """
        Get information about the storage directory.

        Returns:
            Dictionary with storage statistics

        Raises:
            StorageError: If statistics collection fails
        """
        try:
            translation_files = self.list_translations()
            total_size = sum(
                file_path.stat().st_size for file_path in translation_files
            )

            return {
                "output_directory": str(self.output_dir.absolute()),
                "total_files": len(translation_files),
                "total_size_bytes": total_size,
                "total_size_mb": round(total_size / (1024 * 1024), 2),
                "oldest_file": (
                    min(translation_files, key=lambda x: x.stat().st_mtime).name
                    if translation_files
                    else None
                ),
                "newest_file": (
                    max(translation_files, key=lambda x: x.stat().st_mtime).name
                    if translation_files
                    else None
                ),
            }

        except Exception as e:
            logger.error(f"Failed to get storage info: {e}")
            raise StorageError(f"Failed to get storage information: {e}")

    def __repr__(self) -> str:
        """String representation of the storage handler."""
        return f"StorageHandler(output_dir='{self.output_dir}')"

</code>

src/vpsweb/utils/datetime_utils.py:
<code>
"""
Date/Time Utilities for VPSWeb Repository System

This module provides comprehensive date/time utilities with timezone support
for the repository system.

Features:
- Timezone-aware datetime handling
- ISO 8601 formatting and parsing
- Relative time calculations
- Time zone conversion utilities
- Duration formatting and parsing
- Workday and business time calculations
- Poetry-specific date formatting
"""

import re
from datetime import datetime, timezone, timedelta, date
from typing import Optional, Dict, Any, Union, Tuple
from dateutil import parser, tz
from dateutil.relativedelta import relativedelta
import pytz
import calendar


class DateTimeError(Exception):
    """Base exception for datetime operations."""
    pass


class TimezoneError(DateTimeError):
    """Timezone-related errors."""
    pass


class ParsingError(DateTimeError):
    """Date/time parsing errors."""
    pass


class TimezoneManager:
    """
    Manages timezone operations and conversions.

    Provides utilities for working with different timezones
    and handling timezone-aware datetime objects.
    """

    # Common timezones for poetry translation
    COMMON_TIMEZONES = {
        'UTC': 'UTC',
        'US/Eastern': 'America/New_York',
        'US/Central': 'America/Chicago',
        'US/Mountain': 'America/Denver',
        'US/Pacific': 'America/Los_Angeles',
        'Europe/London': 'Europe/London',
        'Europe/Paris': 'Europe/Paris',
        'Europe/Berlin': 'Europe/Berlin',
        'Asia/Shanghai': 'Asia/Shanghai',
        'Asia/Tokyo': 'Asia/Tokyo',
        'Asia/Seoul': 'Asia/Seoul',
        'Asia/Dubai': 'Asia/Dubai',
        'Australia/Sydney': 'Australia/Sydney',
    }

    def __init__(self, default_timezone: str = 'UTC'):
        """
        Initialize timezone manager.

        Args:
            default_timezone: Default timezone to use
        """
        self.default_timezone = default_timezone
        self._timezone_cache: Dict[str, Any] = {}

    def get_timezone(self, tz_name: str) -> Any:
        """
        Get timezone object by name.

        Args:
            tz_name: Timezone name

        Returns:
            Timezone object

        Raises:
            TimezoneError: If timezone is invalid
        """
        if tz_name in self._timezone_cache:
            return self._timezone_cache[tz_name]

        try:
            # Try direct timezone lookup
            tz_obj = pytz.timezone(tz_name)
            self._timezone_cache[tz_name] = tz_obj
            return tz_obj
        except pytz.UnknownTimeZoneError:
            raise TimezoneError(f"Unknown timezone: {tz_name}")

    def convert_timezone(
        self,
        dt: datetime,
        from_tz: str,
        to_tz: str
    ) -> datetime:
        """
        Convert datetime from one timezone to another.

        Args:
            dt: Datetime object
            from_tz: Source timezone
            to_tz: Target timezone

        Returns:
            Datetime in target timezone
        """
        from_timezone = self.get_timezone(from_tz)
        to_timezone = self.get_timezone(to_tz)

        # Ensure datetime is timezone-aware
        if dt.tzinfo is None:
            dt = from_timezone.localize(dt)

        return dt.astimezone(to_timezone)

    def to_utc(self, dt: datetime, from_tz: Optional[str] = None) -> datetime:
        """
        Convert datetime to UTC.

        Args:
            dt: Datetime object
            from_tz: Source timezone (uses datetime's timezone if None)

        Returns:
            UTC datetime
        """
        if dt.tzinfo is None and from_tz:
            from_timezone = self.get_timezone(from_tz)
            dt = from_timezone.localize(dt)

        return dt.astimezone(timezone.utc)

    def from_utc(self, dt: datetime, to_tz: str) -> datetime:
        """
        Convert UTC datetime to target timezone.

        Args:
            dt: UTC datetime
            to_tz: Target timezone

        Returns:
            Datetime in target timezone
        """
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)

        to_timezone = self.get_timezone(to_tz)
        return dt.astimezone(to_timezone)

    def now(self, tz_name: Optional[str] = None) -> datetime:
        """
        Get current time in specified timezone.

        Args:
            tz_name: Timezone name (uses default if None)

        Returns:
            Current datetime in specified timezone
        """
        tz_name = tz_name or self.default_timezone
        tz_obj = self.get_timezone(tz_name)
        return datetime.now(tz_obj)


class DateTimeFormatter:
    """
    Provides various datetime formatting options.

    Supports ISO 8601, human-readable, and poetry-specific formats.
    """

    # Standard formats
    ISO_FORMAT = "%Y-%m-%dT%H:%M:%S%z"
    ISO_FORMAT_U = "%Y-%m-%dT%H:%M:%SZ"
    DATE_FORMAT = "%Y-%m-%d"
    TIME_FORMAT = "%H:%M:%S"
    DATETIME_FORMAT = "%Y-%m-%d %H:%M:%S"

    # Poetry-specific formats
    POETRY_DATE_FORMAT = "%B %d, %Y"  # "October 18, 2025"
    POETRY_DATETIME_FORMAT = "%B %d, %Y at %I:%M %p"  # "October 18, 2025 at 2:30 PM"
    CLASSICAL_FORMAT = "%d %B %Y"  # "18 October 2025"

    # Relative formats
    RELATIVE_FORMATS = {
        'seconds': '%S seconds',
        'minutes': '%M minutes',
        'hours': '%H hours',
        'days': '%d days',
        'weeks': '%W weeks',
        'months': '%M months',
        'years': '%Y years'
    }

    @staticmethod
    def to_iso_string(dt: datetime, microseconds: bool = False) -> str:
        """
        Convert datetime to ISO 8601 string.

        Args:
            dt: Datetime object
            microseconds: Whether to include microseconds

        Returns:
            ISO 8601 formatted string
        """
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)

        if microseconds:
            return dt.isoformat()
        else:
            return dt.replace(microsecond=0).isoformat()

    @staticmethod
    def from_iso_string(iso_string: str) -> datetime:
        """
        Parse ISO 8601 string to datetime.

        Args:
            iso_string: ISO 8601 formatted string

        Returns:
            Datetime object

        Raises:
            ParsingError: If parsing fails
        """
        try:
            return parser.isoparse(iso_string)
        except Exception as e:
            raise ParsingError(f"Failed to parse ISO string '{iso_string}': {str(e)}")

    @staticmethod
    def to_poetry_date(dt: datetime) -> str:
        """
        Format datetime for poetry display.

        Args:
            dt: Datetime object

        Returns:
            Poetry-formatted date string
        """
        return dt.strftime(DateTimeFormatter.POETRY_DATE_FORMAT)

    @staticmethod
    def to_poetry_datetime(dt: datetime) -> str:
        """
        Format datetime for poetry display with time.

        Args:
            dt: Datetime object

        Returns:
            Poetry-formatted datetime string
        """
        return dt.strftime(DateTimeFormatter.POETRY_DATETIME_FORMAT)

    @staticmethod
    def format_duration(duration: timedelta) -> str:
        """
        Format duration in human-readable form.

        Args:
            duration: Duration to format

        Returns:
            Human-readable duration string
        """
        if duration.total_seconds() < 60:
            return f"{int(duration.total_seconds())} seconds"
        elif duration.total_seconds() < 3600:
            return f"{int(duration.total_seconds() / 60)} minutes"
        elif duration.total_seconds() < 86400:
            return f"{int(duration.total_seconds() / 3600)} hours"
        else:
            return f"{int(duration.total_seconds() / 86400)} days"

    @staticmethod
    def parse_duration(duration_str: str) -> timedelta:
        """
        Parse duration string to timedelta.

        Args:
            duration_str: Duration string (e.g., "2 hours", "30 minutes")

        Returns:
            Timedelta object

        Raises:
            ParsingError: If parsing fails
        """
        try:
            return parser.parse(duration_str)
        except Exception as e:
            raise ParsingError(f"Failed to parse duration '{duration_str}': {str(e)}")


class TimeCalculator:
    """
    Provides time calculation utilities.

    Handles relative time, business days, and poetry-specific calculations.
    """

    @staticmethod
    def time_ago(dt: datetime, reference: Optional[datetime] = None) -> str:
        """
        Calculate time ago from datetime.

        Args:
            dt: Datetime to calculate from
            reference: Reference datetime (uses now if None)

        Returns:
            Human-readable time ago string
        """
        if reference is None:
            reference = datetime.now(timezone.utc)

        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)

        delta = reference - dt
        absolute_delta = abs(delta)

        if absolute_delta.total_seconds() < 60:
            seconds = int(absolute_delta.total_seconds())
            return f"{seconds} second{'s' if seconds != 1 else ''} {'ago' if delta.total_seconds() > 0 else 'from now'}"
        elif absolute_delta.total_seconds() < 3600:
            minutes = int(absolute_delta.total_seconds() / 60)
            return f"{minutes} minute{'s' if minutes != 1 else ''} {'ago' if delta.total_seconds() > 0 else 'from now'}"
        elif absolute_delta.total_seconds() < 86400:
            hours = int(absolute_delta.total_seconds() / 3600)
            return f"{hours} hour{'s' if hours != 1 else ''} {'ago' if delta.total_seconds() > 0 else 'from now'}"
        elif absolute_delta.days < 30:
            days = absolute_delta.days
            return f"{days} day{'s' if days != 1 else ''} {'ago' if delta.total_seconds() > 0 else 'from now'}"
        elif absolute_delta.days < 365:
            months = int(absolute_delta.days / 30)
            return f"{months} month{'s' if months != 1 else ''} {'ago' if delta.total_seconds() > 0 else 'from now'}"
        else:
            years = int(absolute_delta.days / 365)
            return f"{years} year{'s' if years != 1 else ''} {'ago' if delta.total_seconds() > 0 else 'from now'}"

    @staticmethod
    def is_weekend(dt: datetime) -> bool:
        """
        Check if datetime falls on a weekend.

        Args:
            dt: Datetime to check

        Returns:
            True if it's a weekend
        """
        return dt.weekday() >= 5  # Saturday=5, Sunday=6

    @staticmethod
    def is_business_hours(dt: datetime, start_hour: int = 9, end_hour: int = 17) -> bool:
        """
        Check if datetime falls within business hours.

        Args:
            dt: Datetime to check
            start_hour: Business start hour (24-hour format)
            end_hour: Business end hour (24-hour format)

        Returns:
            True if within business hours
        """
        return start_hour <= dt.hour < end_hour and not TimeCalculator.is_weekend(dt)

    @staticmethod
    def add_business_days(dt: datetime, days: int) -> datetime:
        """
        Add business days to datetime.

        Args:
            dt: Starting datetime
            days: Number of business days to add

        Returns:
            Datetime after adding business days
        """
        result = dt
        added_days = 0

        while added_days < days:
            result += timedelta(days=1)
            if not TimeCalculator.is_weekend(result):
                added_days += 1

        return result

    @staticmethod
    def get_age(birth_date: Union[date, datetime], reference: Optional[datetime] = None) -> Dict[str, int]:
        """
        Calculate age from birth date.

        Args:
            birth_date: Birth date or datetime
            reference: Reference datetime (uses now if None)

        Returns:
            Dictionary with years, months, days
        """
        if reference is None:
            reference = datetime.now(timezone.utc)

        # Convert date to datetime if needed
        if isinstance(birth_date, date) and not isinstance(birth_date, datetime):
            birth_date = datetime.combine(birth_date, datetime.min.time())
            if reference.tzinfo:
                birth_date = birth_date.replace(tzinfo=reference.tzinfo)

        if birth_date.tzinfo is None and reference.tzinfo:
            birth_date = birth_date.replace(tzinfo=reference.tzinfo)
        elif birth_date.tzinfo is not None and reference.tzinfo is None:
            reference = reference.replace(tzinfo=birth_date.tzinfo)

        delta = relativedelta(reference, birth_date)

        return {
            'years': delta.years,
            'months': delta.months,
            'days': delta.days
        }

    @staticmethod
    def get_season(dt: datetime, hemisphere: str = 'northern') -> str:
        """
        Get season for datetime.

        Args:
            dt: Datetime to check
            hemisphere: 'northern' or 'southern'

        Returns:
            Season name
        """
        month = dt.month

        if hemisphere.lower() == 'northern':
            if month in [12, 1, 2]:
                return 'Winter'
            elif month in [3, 4, 5]:
                return 'Spring'
            elif month in [6, 7, 8]:
                return 'Summer'
            else:
                return 'Autumn'
        else:
            if month in [12, 1, 2]:
                return 'Summer'
            elif month in [3, 4, 5]:
                return 'Autumn'
            elif month in [6, 7, 8]:
                return 'Winter'
            else:
                return 'Spring'

    @staticmethod
    def get_quarter(dt: datetime) -> int:
        """
        Get quarter for datetime.

        Args:
            dt: Datetime to check

        Returns:
            Quarter number (1-4)
        """
        return (dt.month - 1) // 3 + 1

    @staticmethod
    def get_week_number(dt: datetime) -> int:
        """
        Get ISO week number for datetime.

        Args:
            dt: Datetime to check

        Returns:
            ISO week number
        """
        return dt.isocalendar()[1]

    @staticmethod
    def get_day_of_year(dt: datetime) -> int:
        """
        Get day of year for datetime.

        Args:
            dt: Datetime to check

        Returns:
            Day of year (1-366)
        """
        return dt.timetuple().tm_yday


class PoetryDateTimeUtils:
    """
    Poetry-specific datetime utilities.

    Provides formatting and calculations specific to poetry and translation work.
    """

    @staticmethod
    def format_creation_date(dt: datetime, style: str = 'modern') -> str:
        """
        Format creation date for poetry display.

        Args:
            dt: Creation datetime
            style: 'modern', 'classical', 'academic'

        Returns:
            Formatted date string
        """
        if style == 'modern':
            return DateTimeFormatter.to_poetry_date(dt)
        elif style == 'classical':
            return dt.strftime(DateTimeFormatter.CLASSICAL_FORMAT)
        elif style == 'academic':
            return f"{dt.year:04d}-{dt.month:02d}-{dt.day:02d}"
        else:
            return DateTimeFormatter.to_poetry_date(dt)

    @staticmethod
    def format_translation_date(dt: datetime, include_time: bool = True) -> str:
        """
        Format translation date for display.

        Args:
            dt: Translation datetime
            include_time: Whether to include time

        Returns:
            Formatted date string
        """
        if include_time:
            return DateTimeFormatter.to_poetry_datetime(dt)
        else:
            return DateTimeFormatter.to_poetry_date(dt)

    @staticmethod
    def calculate_translation_speed(
        word_count: int,
        start_time: datetime,
        end_time: Optional[datetime] = None
    ) -> Dict[str, float]:
        """
        Calculate translation speed metrics.

        Args:
            word_count: Number of words translated
            start_time: Translation start time
            end_time: Translation end time (uses now if None)

        Returns:
            Dictionary with speed metrics
        """
        if end_time is None:
            end_time = datetime.now(timezone.utc)

        if start_time.tzinfo is None:
            start_time = start_time.replace(tzinfo=timezone.utc)
        if end_time.tzinfo is None:
            end_time = end_time.replace(tzinfo=timezone.utc)

        duration_seconds = (end_time - start_time).total_seconds()
        duration_hours = duration_seconds / 3600

        if duration_hours > 0:
            words_per_hour = word_count / duration_hours
        else:
            words_per_hour = 0

        if duration_seconds > 0:
            words_per_minute = word_count / (duration_seconds / 60)
        else:
            words_per_minute = 0

        return {
            'word_count': word_count,
            'duration_seconds': duration_seconds,
            'duration_hours': duration_hours,
            'words_per_hour': words_per_hour,
            'words_per_minute': words_per_minute
        }

    @staticmethod
    def get_poetry_period(dt: datetime) -> str:
        """
        Get historical poetry period for a date.

        Args:
            dt: Datetime to classify

        Returns:
            Poetry period name
        """
        year = dt.year

        if year < 500:
            return 'Classical Antiquity'
        elif year < 1000:
            return 'Early Medieval'
        elif year < 1300:
            return 'High Medieval'
        elif year < 1500:
            return 'Late Medieval'
        elif year < 1600:
            return 'Renaissance'
        elif year < 1700:
            return 'Baroque'
        elif year < 1800:
            return 'Enlightenment'
        elif year < 1850:
            return 'Romantic'
        elif year < 1900:
            return 'Victorian'
        elif year < 1950:
            return 'Modernist'
        elif year < 2000:
            return 'Contemporary'
        else:
            return '21st Century'


# Global instances
_timezone_manager: Optional[TimezoneManager] = None


def get_timezone_manager() -> TimezoneManager:
    """
    Get the global timezone manager instance.

    Returns:
        Global TimezoneManager instance
    """
    global _timezone_manager
    if _timezone_manager is None:
        _timezone_manager = TimezoneManager()
    return _timezone_manager


# Convenience functions
def now_utc() -> datetime:
    """Get current UTC datetime."""
    return datetime.now(timezone.utc)


def to_utc(dt: datetime, from_tz: Optional[str] = None) -> datetime:
    """Convert datetime to UTC."""
    manager = get_timezone_manager()
    return manager.to_utc(dt, from_tz)


def from_utc(dt: datetime, to_tz: str) -> datetime:
    """Convert UTC datetime to target timezone."""
    manager = get_timezone_manager()
    return manager.from_utc(dt, to_tz)


def parse_iso_datetime(iso_string: str) -> datetime:
    """Parse ISO 8601 datetime string."""
    return DateTimeFormatter.from_iso_string(iso_string)


def format_iso_datetime(dt: datetime, microseconds: bool = False) -> str:
    """Format datetime as ISO 8601 string."""
    return DateTimeFormatter.to_iso_string(dt, microseconds)


def time_ago(dt: datetime, reference: Optional[datetime] = None) -> str:
    """Calculate time ago from datetime."""
    return TimeCalculator.time_ago(dt, reference)


def format_poetry_date(dt: datetime, style: str = 'modern') -> str:
    """Format date for poetry display."""
    return PoetryDateTimeUtils.format_creation_date(dt, style)


def get_age(birth_date: Union[date, datetime], reference: Optional[datetime] = None) -> Dict[str, int]:
    """Calculate age from birth date."""
    return TimeCalculator.get_age(birth_date, reference)


def is_valid_datetime_string(date_string: str) -> bool:
    """Check if string can be parsed as datetime."""
    try:
        parser.parse(date_string)
        return True
    except:
        return False


def parse_flexible_datetime(date_string: str) -> datetime:
    """
    Parse datetime from flexible string formats.

    Args:
        date_string: Date string in various formats

    Returns:
        Parsed datetime object

    Raises:
        ParsingError: If parsing fails
    """
    try:
        dt = parser.parse(date_string)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except Exception as e:
        raise ParsingError(f"Failed to parse datetime '{date_string}': {str(e)}")
</code>



please help do a code review for the first 2 days' work. They are
**Day 1: Project Scaffolding** 
- [x] Create `src/vpsweb/repository/` and `src/vpsweb/webui/` package structure
- [x] Set up FastAPI application in `webui/main.py` with basic routing
- [x] Configure SQLite database with SQLAlchemy in `repository/database.py`
- [x] Implement Alembic migration system
- [x] Create basic Pydantic models and settings in both modules
and
**Day 2: Core Data Layer**
- [ ] Implement ORM models (4-table schema: poems, translations, ai_logs, human_notes)
- [ ] Create database migration files
- [ ] Build basic CRUD operations
- [ ] Add data validation and constraints
- [ ] Write initial unit tests for models