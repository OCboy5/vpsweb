outputs/json/poets/Ted Hughes/Ted Hughes_september_english_chinese_non_reasoning_20251026_220340_9d8f1b2e.json:
<code>
{
  "workflow_id": "08e6b68c-dfe8-4f86-959e-c0c09d8f1b2e",
  "input": {
    "original_poem": "We sit late, watching the dark slowly unfold:\nNo clock counts this.\nWhen kisses are repeated and the arms hold\nThere is no telling where time is.\n\nIt is midsummer: the leaves hang big and still:\nBehind the eye a star,\nUnder the silk of the wrist a sea, tell\nTime is nowhere.\n\nWe stand; leaves have not timed the summer.\nNo clock now needs\nTell we have only what we remember:\nMinutes uproaring with our heads\n\nLike an unfortunate King's and his Queen's\nWhen the senseless mob rules;\nAnd quietly the trees casting their crowns\nInto the pools.",
    "source_lang": "English",
    "target_lang": "Chinese",
    "metadata": {
      "id": "01K8GD5CA6GMD6BV80QE4B237J",
      "title": "September",
      "author": "Ted Hughes"
    }
  },
  "initial_translation": {
    "initial_translation": "\n我们坐到深夜，凝望黑暗缓缓展开：\n没有时钟为此计数；\n当亲吻一再重复，双臂紧紧相拥，\n便无从知晓时光去向。\n\n这是仲夏：树叶硕大而静止：\n眼眸深处有一颗星，\n腕脉丝绸之下藏着一片海，诉说\n时间已不复存在。\n\n我们起身；落叶未曾标记夏日的流逝。\n此刻无需钟表提醒：\n我们仅拥有记忆中的片刻——\n那些如国王与王后般不幸的瞬间，\n\n当愚昧的暴民掌权；\n而树木悄然将王冠卸下，\n投入池中。",
    "initial_translation_notes": "\n本诗《九月》出自英国诗人特德·休斯（Ted Hughes）之手，其诗风以原始力量、自然意象与心理深度著称。他常以自然景物为载体，揭示人类情感与时间的神秘流动。本诗虽题为“September”，但内容却指向“midsummer”（仲夏），这看似矛盾的命名实则暗示时间感知的错位与主观性——或许“九月”在此并非实指，而是象征季节转折、情感沉淀的临界点。因此，我保留“九月”为题，既尊重原题形式，又赋予其中文语境下的诗意张力：九月在中国文化中亦常与秋思、离别、回忆相联，恰好呼应诗中对记忆与时间消逝的沉思。\n\n在翻译过程中，我面临如下挑战与抉择：\n\n1. **开篇句的多重解读与节奏处理**  \n原文：“We sit late, watching the dark slowly unfold: / No clock counts this.”  \n我曾尝试三种译法：\n- 我们坐至深夜，看黑暗徐徐铺展：/ 没有钟表为此计时。\n- 我们久久静坐，目送夜色渐次展开：/ 此刻不在钟表刻度之上。\n- 我们坐到深夜，凝望黑暗缓缓展开：/ 没有时钟为此计数。\n\n最终选择第三版，因其在节奏上更贴近原文的平缓陈述，动词“凝望”较“目送”更显专注，“缓缓展开”对应“slowly unfold”具画面感，“没有时钟为此计数”以“计数”呼应“counts”，比“计时”更具文学性与抽象意味。\n\n2. **时间主题的哲学性表达**  \n诗中反复强调时间的消隐（“There is no telling where time is”, “Time is nowhere”），这并非物理时间的消失，而是情感沉浸中时间感知的断裂。中文“时间已不复存在”较直译“时间无处可寻”更富诗意，且与下文“无需钟表提醒”形成逻辑递进。\n\n3. **意象的保留与文化适配**  \n“Behind the eye a star, / Under the silk of the wrist a sea” 是典型的休斯式超现实身体意象。我译为“眼眸深处有一颗星，/ 腕脉丝绸之下藏着一片海”，保留“星”与“海”的象征（内在光芒与潜意识之深），并以“腕脉丝绸”传达“silk of the wrist”中肌肤细腻如丝的触感，同时暗示血脉流动，与“海”形成生命律动的隐喻。\n\n4. **结尾的历史隐喻与意象转化**  \n“Like an unfortunate King's and his Queen's / When the senseless mob rules” 引用欧洲历史中君主被暴民推翻的悲剧场景，象征美好事物在混乱中陨落。中文若直译“暴民”易带政治敏感，故以“愚昧的暴民”适度弱化情绪，保留“国王与王后”的文化通识性，并通过“不幸的瞬间”将具体历史事件升华为普遍情感体验。\n\n5. **形式与节奏的调适**  \n原诗为自由体，无固定格律，但句式长短交错，富有内在节奏。中文翻译采用四字、五字、七字句混合，模拟其呼吸感。如“树木悄然将王冠卸下，/ 投入池中”，以“卸下”对应“casting”，赋予树木拟人化的庄严感，“王冠”呼应“crowns”，将落叶比作君王退位，延续诗中王权隐喻，增强悲剧美感。\n\n6. **文化本质的保留与诗意再创造**  \n全诗核心是“记忆即存在”——“we have only what we remember”。此句我译为“我们仅拥有记忆中的片刻”，以“片刻”呼应前文“Minutes uproaring”，将喧嚣的瞬间与静默的回忆并置，强化情感对比。虽中文无时态变化，但通过“已不复存在”“未曾标记”等表达，传达时间流逝的不可逆。\n\n综上，本译文力求在忠实原意的基础上，重现休斯诗中那种静谧而深邃的时空感，使中文读者既能感知其自然意象的质感，亦能体会其哲思的重量。",
    "translated_poem_title": "九月",
    "translated_poet_name": "特德·休斯",
    "timestamp": "2025-10-26T22:02:17.716124",
    "model_info": {
      "provider": "tongyi",
      "model": "qwen-plus-latest",
      "temperature": "0.7"
    },
    "tokens_used": 2286,
    "prompt_tokens": 1142,
    "completion_tokens": 1144,
    "duration": 48.306323289871216,
    "cost": 0.0032015999999999998
  },
  "editor_review": {
    "editor_suggestions": "\nSuggestions for Improving the Translation of \"九月\" by 特德·休斯:\n1. Line 1: “我们坐到深夜，凝望黑暗缓缓展开：/ 没有时钟为此计数” — “坐到深夜”隐含“从某时坐至深夜”，而原文“we sit late”更中性，仅强调“坐得很晚”，未必指从傍晚开始。此外，“计数”虽呼应“counts”，但“count”在此更宜理解为“衡量”或“记录”，而非数学意义上的“计数”。建议改为“没有时钟能衡量此刻”，以更准确传达“时间无法被量化”的哲思。示例：“我们久久坐着，凝望黑暗缓缓展开：/ 没有时钟能衡量此刻”。\n2. Line 3–4: “当亲吻一再重复，双臂紧紧相拥，/ 便无从知晓时光去向” — “双臂紧紧相拥”语义略显重复（“相拥”已含“双臂”动作），且“一再重复”稍显口语化，未能完全体现“repeated”在诗中那种近乎仪式性的累积感。建议调整为“当吻一次次落下，双臂环抱不离，/ 时光的去向便无从辨认”，增强节奏与诗意密度，同时“落下”呼应“kisses”作为名词的具象化。\n3. Stanza 2, Line 1: “这是仲夏：树叶硕大而静止” — “硕大”虽达意，但偏重“大”而弱化“饱满、繁茂”的质感，“big”在此更强调生命力的充盈。建议用“浓密”或“丰盈”替代，如“树叶丰盈而静止”，以更贴合midsummer的生态意象与视觉张力。\n4. Stanza 2, Lines 2–4: “眼眸深处有一颗星，/ 腕脉丝绸之下藏着一片海，诉说/ 时间已不复存在” — “腕脉丝绸”为创造性译法，但“腕脉”易被误解为医学术语，且“丝绸”前置造成“丝绸之下藏着海”的空间错乱（原句“under the silk of the wrist”意为“在如丝般柔滑的手腕肌肤之下”）。建议调整为“腕间丝绸下藏着一片海”，或更流畅地处理为“眼底有星闪烁，/ 腕肤如丝，其下暗涌成海”，既保留意象又理顺逻辑，增强诗意连贯性。\n5. Stanza 3, Line 1: “我们起身；落叶未曾标记夏日的流逝” — 原文“Leaves have not timed the summer”中“leaves”指当下仍在枝头的叶子，并非“落叶”。“落叶”一词误导读者以为秋天已至，与“midsummer”矛盾。此为严重误译。应改为“树叶未曾为夏日计时”或“绿叶未尝标记夏日的流转”，以忠实原意并维持时间错位的哲学张力。\n6. Stanza 3, Line 3: “我们仅拥有记忆中的片刻——” — “片刻”对应“minutes”，但“minutes uproaring”强调的是“喧嚣、激烈、充满张力的瞬间”，而“片刻”在中文中偏中性甚至微小。建议用“那些喧腾的瞬间”或“那些轰鸣的时刻”，以保留“uproaring”的听觉与情感强度。示例：“我们仅拥有的，是记忆中轰鸣的时刻——”。\n7. Stanza 3, Lines 4–5: “那些如国王与王后般不幸的瞬间，/ 当愚昧的暴民掌权” — “如国王与王后般不幸的瞬间”结构冗长，且“如……般”削弱了比喻的直接性。原文“Like an unfortunate King's and his Queen's”是明喻，修饰“minutes”，宜更紧凑处理。建议改为“如那不幸君王与王后的时刻，/ 暴民愚昧，执掌朝纲”，增强历史画面感与节奏力度。“掌权”改为“执掌朝纲”或“当道”，更具文言诗意，契合“国王与王后”的古典语境。\n8. Final Couplet: “而树木悄然将王冠卸下，/ 投入池中” — “卸下”略显主动，而“casting”更偏向“抛掷、洒落”的自然动作，带有被动与宿命感。建议用“抛落”或“洒下”以增强动态与悲怆感。示例：“而树木静静将王冠抛落，/ 沉入池中”，“沉入”比“投入”更具终结感，呼应“crowns”作为生命象征的陨落。\n9. Punctuation and Line Breaks: 全诗使用中文冒号与句号，但原诗为自由体，标点服务于呼吸与停顿，非语法完整。建议在“缓缓展开：”后保留冒号以延续张力，但“诉说/ 时间已不复存在”中“诉说”后宜加逗号或破折号，否则语义断裂。可改为“诉说，/ 时间已不复存在”或“诉说——/ 时间已不复存在”，以增强语气停顿与抒情层次。\n10. Title: “九月” — 虽译者注解释其象征意义，但读者初读易产生季节错乱（诗中明确为midsummer）。建议加副标题或注释，如“九月（仲夏）”，或在译者序中说明，以避免误解。若坚持单标题，可考虑“九月：仲夏”以平衡形式与信息。\n\nConclude with a brief overall assessment (2-3 sentences) of the translation's current quality and its potential for improvement.\n该译文整体忠实于原诗的情感基调与哲学意蕴，语言富有诗意，尤其在意象转化与节奏把控上展现了高度的文学自觉。然个别关键意象存在误读（如“落叶”），部分措辞可进一步精炼以增强准确性与表现力。经适度修订后，有望成为兼具学术严谨性与艺术感染力的经典译本。",
    "timestamp": "2025-10-26T22:03:00.742411",
    "model_info": {
      "provider": "tongyi",
      "model": "qwen-plus-latest",
      "temperature": "0.3"
    },
    "tokens_used": 3630
  },
  "revised_translation": {
    "revised_translation": "\n我们久久坐着，凝望黑暗缓缓展开：\n没有时钟能衡量此刻；\n当吻一次次落下，双臂环抱不离，\n时光的去向便无从辨认。\n\n这是仲夏：树叶丰盈而静止：\n眼底有星闪烁，\n腕肤如丝，其下暗涌成海——\n诉说，时间已不复存在。\n\n我们起身；绿叶未尝标记夏日的流转。\n此刻无需钟表提醒：\n我们仅拥有的，是记忆中轰鸣的时刻——\n如那不幸君王与王后的瞬间，\n\n暴民愚昧，执掌朝纲；\n而树木静静将王冠抛落，\n沉入池中。",
    "revised_translation_notes": "\n1. Major revisions and the reasoning behind them:\n\n- Line 1: Revised “我们坐到深夜” to “我们久久坐着” to better reflect the neutral, durational sense of “we sit late” without implying a temporal starting point. Changed “没有时钟为此计数” to “没有时钟能衡量此刻”， aligning with the expert suggestion that “count” here implies “measure” rather than “count numerically,” enhancing the philosophical tone.\n\n- Lines 3–4: Replaced “亲吻一再重复，双臂紧紧相拥” with “当吻一次次落下，双臂环抱不离”， improving poetic rhythm and avoiding redundancy (“双臂相拥” is tautological). “落下” gives “kisses” a more vivid, physical presence, echoing the original’s concrete noun usage.\n\n- Stanza 2, Line 1: Changed “硕大” to “丰盈”， capturing the lush, vital quality of “big and still” leaves in midsummer, as suggested. “丰盈” conveys fullness of life, not just size.\n\n- Lines 2–4 of Stanza 2: Revised “腕脉丝绸之下藏着一片海” to “腕肤如丝，其下暗涌成海”， resolving the spatial confusion and medical connotation of “腕脉.” The new phrasing clarifies that the silk refers to the skin’s texture. Added em dash after “海” and comma after “诉说” to enhance pause and lyrical flow, per suggestion 9.\n\n- Stanza 3, Line 1: Corrected “落叶未曾标记夏日的流逝” to “绿叶未尝标记夏日的流转”， fixing a critical error. “落叶” wrongly implies autumn; original “leaves” are still on the trees. “绿叶” preserves the midsummer setting, and “流转” adds poetic nuance to “timed.”\n\n- Line 3: Changed “记忆中的片刻” to “记忆中轰鸣的时刻”， replacing the neutral “片刻” with “轰鸣的时刻” to reflect the emotional and auditory intensity of “minutes uproaring.” “轰鸣” captures the chaos and volume implied.\n\n- Lines 4–5: Revised “那些如国王与王后般不幸的瞬间” to “如那不幸君王与王后的瞬间”， tightening syntax and enhancing poetic diction. “君王与王后” sounds more regal than “国王与王后.” “暴民愚昧，执掌朝纲” replaces “当愚昧的暴民掌权,” using classical phrasing (“执掌朝纲”) to match the royal metaphor and elevate tone.\n\n- Final Couplet: Changed “卸下” to “抛落” and “投入” to “沉入,” per suggestion 8. “抛落” conveys the involuntary, fated motion of “casting,” while “沉入” adds weight and finality, reinforcing the image of crowns (symbolizing life, authority) descending into water as a quiet elegy.\n\n- Title: Adopted “九月（仲夏）” to resolve the seasonal dissonance. Though “September” may carry symbolic weight (e.g., transition, memory), the poem’s imagery is unmistakably midsummer. The parenthetical clarifies without sacrificing poetic ambiguity.\n\n2. Expert suggestions not implemented:\n\n- None. All ten expert suggestions were evaluated and either directly implemented or refined for greater poetic precision. The only deviation is in phrasing execution (e.g., “腕肤如丝” instead of “腕间丝绸下”), but the intent and correction are fully honored.\n\n3. Balancing original essence and Chinese effectiveness:\n\nThe revision preserves Hughes’ contemplative, almost mystical tone, where time dissolves in intimacy and nature. The philosophical core—time as unmeasurable, memory as both refuge and chaos—is maintained. Poetic devices like metaphor (“star behind the eye,” “sea under the wrist”) are rendered with clarity and lyrical grace in Chinese, avoiding over-literalism while respecting their symbolic depth. Rhythm is balanced between natural speech and poetic cadence, with line breaks mirroring the original’s pauses.\n\n4. Challenges and resolutions:\n\n- The phrase “under the silk of the wrist” posed a syntactic and imagistic challenge. Direct translation risked absurdity (“丝绸之下” implying fabric). Resolved by rephrasing as “腕肤如丝，其下暗涌成海,” using simile to clarify and enrich.\n\n- “Minutes uproaring with our heads” is syntactically ambiguous in English. Interpreted as “moments filled with the roaring within our minds,” hence “轰鸣的时刻” — emphasizing internal tumult.\n\n- Maintaining the free verse form while ensuring Chinese poetic fluency required careful lineation. Punctuation (em dash, comma) was used strategically to guide breath and emphasis without violating Chinese norms.\n\nThe revised translation achieves greater accuracy, poetic resonance, and philosophical clarity, transforming a strong initial effort into a more authoritative and moving rendition.",
    "refined_translated_poem_title": "九月（仲夏）",
    "refined_translated_poet_name": "特德·休斯",
    "timestamp": "2025-10-26T22:03:40.793716",
    "model_info": {
      "provider": "tongyi",
      "model": "qwen-plus-latest",
      "temperature": "0.2"
    },
    "tokens_used": 3740,
    "prompt_tokens": 2489,
    "completion_tokens": 1251,
    "duration": 40.02924585342407,
    "cost": 0.004493199999999999
  },
  "total_tokens": 9656,
  "duration_seconds": 131.40507912635803
}
</code>

src/vpsweb/core/workflow.py:
<code>
"""
Workflow Orchestrator for Vox Poetica Studio Web.

This module implements the main Translation→Editor→Translation workflow orchestrator
that coordinates the complete poetry translation process following the vpts.yml specification.
"""

import time
import logging
import uuid
from typing import Dict, Any, Optional, Callable, Awaitable
from datetime import datetime

from ..services.llm.factory import LLMFactory
from ..services.prompts import PromptService
from ..core.executor import StepExecutor
from ..models.config import WorkflowConfig, WorkflowMode
from ..models.translation import (
    TranslationInput,
    InitialTranslation,
    EditorReview,
    RevisedTranslation,
    TranslationOutput,
)
from ..services.parser import OutputParser
from ..utils.progress import ProgressTracker, StepStatus

logger = logging.getLogger(__name__)


class WorkflowError(Exception):
    """Base exception for workflow execution errors."""

    pass


class StepExecutionError(WorkflowError):
    """Raised when a specific workflow step fails."""

    pass


class ConfigurationError(WorkflowError):
    """Raised when workflow configuration is invalid."""

    pass


class TranslationWorkflow:
    """
    Main orchestrator for the T-E-T translation workflow.

    This class coordinates the complete translation workflow:
    1. Initial Translation
    2. Editor Review
    3. Translator Revision
    """

    def __init__(
        self,
        config: WorkflowConfig,
        providers_config,
        workflow_mode: WorkflowMode = WorkflowMode.HYBRID,
        system_config: Optional[Dict[str, Any]] = None,
    ):
        """
        Initialize the translation workflow.

        Args:
            config: Workflow configuration with step configurations
            providers_config: Provider configurations for LLM factory
            workflow_mode: Workflow mode to use (reasoning, non_reasoning, hybrid)
            system_config: System configuration with preview lengths and other settings
        """
        self.config = config
        self.workflow_mode = workflow_mode
        self.system_config = system_config or {}

        # Initialize services
        self.llm_factory = LLMFactory(providers_config)
        self.prompt_service = PromptService()
        self.step_executor = StepExecutor(self.llm_factory, self.prompt_service)

        # Initialize progress callback (optional)
        self.progress_callback: Optional[
            Callable[[str, Dict[str, Any]], Awaitable[None]]
        ] = None

        # Get the appropriate workflow steps for the mode
        self.workflow_steps = config.get_workflow_steps(workflow_mode)

        logger.info(f"Initialized TranslationWorkflow: {config.name} v{config.version}")
        logger.info(f"Workflow mode: {workflow_mode.value}")
        logger.info(f"Available steps: {list(self.workflow_steps.keys())}")

    async def execute(
        self, input_data: TranslationInput, show_progress: bool = True
    ) -> TranslationOutput:
        """
        Execute complete translation workflow.

        Args:
            input_data: Translation input with poem and language information
            show_progress: Whether to display progress updates

        Returns:
            Complete translation output with all intermediate results

        Raises:
            WorkflowError: If workflow execution fails
        """
        workflow_id = str(uuid.uuid4())
        start_time = time.time()
        log_entries = []

        logger.info(f"Starting translation workflow {workflow_id}")
        logger.info(f"Translation: {input_data.source_lang} → {input_data.target_lang}")
        logger.info(f"Poem length: {len(input_data.original_poem)} characters")

        # Initialize progress tracker
        progress_tracker: Optional[ProgressTracker] = None
        if show_progress:
            progress_tracker = ProgressTracker(
                ["initial_translation", "editor_review", "translator_revision"]
            )

        try:
            # Step 1: Initial Translation
            log_entries.append(
                f"=== STEP 1: INITIAL TRANSLATION ({self.workflow_mode.value.upper()} MODE) ==="
            )
            # Get input preview length from config
            input_preview_length = (
                self.system_config.get("system", {})
                .get("preview_lengths", {})
                .get("input_preview", 100)
            )
            log_entries.append(
                f"Input: {input_data.original_poem[:input_preview_length]}..."
            )

            if progress_tracker:
                step_config = self.workflow_steps["initial_translation"]
                model_info = {
                    "provider": step_config.provider,
                    "model": step_config.model,
                    "temperature": str(step_config.temperature),
                }
                progress_tracker.start_step("initial_translation", model_info)

            # Call progress callback when Step 1 starts
            if self.progress_callback:
                await self.progress_callback(
                    "Initial Translation",
                    {"status": "started", "message": "Starting initial translation..."},
                )

            step_start_time = time.time()
            initial_translation = await self._initial_translation(input_data)
            step_duration = time.time() - step_start_time
            initial_translation.duration = step_duration

            # Calculate cost for this step
            step_config = self.workflow_steps["initial_translation"]
            # Use actual token counts from API response
            input_tokens = getattr(initial_translation, "prompt_tokens", 0) or 0
            output_tokens = getattr(initial_translation, "completion_tokens", 0) or 0
            initial_translation.cost = self._calculate_step_cost(
                step_config.provider, step_config.model, input_tokens, output_tokens
            )
            log_entries.append(
                f"Initial translation completed: {initial_translation.tokens_used} tokens"
            )
            # Get response preview length from config
            response_preview_length = (
                self.system_config.get("system", {})
                .get("preview_lengths", {})
                .get("response_preview", 100)
            )
            log_entries.append(
                f"Translation: {initial_translation.initial_translation[:response_preview_length]}..."
            )

            if progress_tracker:
                progress_tracker.complete_step(
                    "initial_translation",
                    {
                        "original_poem": input_data.original_poem,
                        "initial_translation": initial_translation.initial_translation,
                        "initial_translation_notes": initial_translation.initial_translation_notes,
                        "tokens_used": initial_translation.tokens_used,
                        "prompt_tokens": getattr(
                            initial_translation, "prompt_tokens", None
                        ),
                        "completion_tokens": getattr(
                            initial_translation, "completion_tokens", None
                        ),
                        "duration": getattr(initial_translation, "duration", None),
                        "cost": getattr(initial_translation, "cost", None),
                        "workflow_mode": self.workflow_mode.value,
                        "model_info": initial_translation.model_info,
                    },
                )

  
            # Call progress callback after Step 1 completion
            if self.progress_callback:
                await self.progress_callback(
                    "Initial Translation",
                    {
                        "status": "completed",
                        "tokens_used": initial_translation.tokens_used,
                        "duration": getattr(initial_translation, "duration", None),
                        "cost": getattr(initial_translation, "cost", None),
                        "model_info": getattr(initial_translation, "model_info", None),
                    },
                )

            # Step 2: Editor Review
            log_entries.append(
                f"\n=== STEP 2: EDITOR REVIEW ({self.workflow_mode.value.upper()} MODE) ==="
            )

            logger.info(
                f"Starting editor review with {initial_translation.tokens_used} tokens from initial translation"
            )

            if progress_tracker:
                step_config = self.workflow_steps["editor_review"]
                model_info = {
                    "provider": step_config.provider,
                    "model": step_config.model,
                    "temperature": str(step_config.temperature),
                }
                progress_tracker.start_step("editor_review", model_info)

            # Call progress callback when Step 2 starts
            if self.progress_callback:
                await self.progress_callback(
                    "Editor Review",
                    {"status": "started", "message": "Starting editor review..."},
                )

            step_start_time = time.time()
            editor_review = await self._editor_review(input_data, initial_translation)
            step_duration = time.time() - step_start_time
            editor_review.duration = step_duration

            # Calculate cost for this step
            step_config = self.workflow_steps["editor_review"]
            # Use actual token counts from API response
            input_tokens = getattr(editor_review, "prompt_tokens", 0) or 0
            output_tokens = getattr(editor_review, "completion_tokens", 0) or 0
            editor_review.cost = self._calculate_step_cost(
                step_config.provider, step_config.model, input_tokens, output_tokens
            )
            logger.info(f"Editor review step completed successfully")
            log_entries.append(
                f"Editor review completed: {editor_review.tokens_used} tokens"
            )
            log_entries.append(
                f"Review length: {len(editor_review.editor_suggestions)} characters"
            )
            # Get editor preview length from config
            editor_preview_length = (
                self.system_config.get("system", {})
                .get("preview_lengths", {})
                .get("editor_preview", 200)
            )
            log_entries.append(
                f"Review preview: {editor_review.editor_suggestions[:editor_preview_length]}..."
            )

            if progress_tracker:
                progress_tracker.complete_step(
                    "editor_review",
                    {
                        "editor_suggestions": editor_review.editor_suggestions,
                        "tokens_used": editor_review.tokens_used,
                        "prompt_tokens": getattr(editor_review, "prompt_tokens", None),
                        "completion_tokens": getattr(
                            editor_review, "completion_tokens", None
                        ),
                        "duration": getattr(editor_review, "duration", None),
                        "cost": getattr(editor_review, "cost", None),
                        "workflow_mode": self.workflow_mode.value,
                        "model_info": editor_review.model_info,
                    },
                )

            # Call progress callback after Step 2 completion
            if self.progress_callback:
                await self.progress_callback(
                    "Editor Review",
                    {
                        "status": "completed",
                        "tokens_used": editor_review.tokens_used,
                        "duration": getattr(editor_review, "duration", None),
                        "cost": getattr(editor_review, "cost", None),
                        "model_info": getattr(editor_review, "model_info", None),
                    },
                )

            # Step 3: Translator Revision
            log_entries.append(
                f"\n=== STEP 3: TRANSLATOR REVISION ({self.workflow_mode.value.upper()} MODE) ==="
            )

            if progress_tracker:
                step_config = self.workflow_steps["translator_revision"]
                model_info = {
                    "provider": step_config.provider,
                    "model": step_config.model,
                    "temperature": str(step_config.temperature),
                }
                progress_tracker.start_step("translator_revision", model_info)

            # Call progress callback when Step 3 starts
            if self.progress_callback:
                await self.progress_callback(
                    "Translator Revision",
                    {"status": "started", "message": "Starting translator revision..."},
                )

            step_start_time = time.time()
            revised_translation = await self._translator_revision(
                input_data, initial_translation, editor_review
            )
            step_duration = time.time() - step_start_time
            revised_translation.duration = step_duration

            # Calculate cost for this step
            step_config = self.workflow_steps["translator_revision"]
            # Use actual token counts from API response
            input_tokens = getattr(revised_translation, "prompt_tokens", 0) or 0
            output_tokens = getattr(revised_translation, "completion_tokens", 0) or 0
            revised_translation.cost = self._calculate_step_cost(
                step_config.provider, step_config.model, input_tokens, output_tokens
            )
            log_entries.append(
                f"Translator revision completed: {revised_translation.tokens_used} tokens"
            )
            log_entries.append(
                f"Revised translation length: {len(revised_translation.revised_translation)} characters"
            )

            if progress_tracker:
                progress_tracker.complete_step(
                    "translator_revision",
                    {
                        "revised_translation": revised_translation.revised_translation,
                        "revised_translation_notes": revised_translation.revised_translation_notes,
                        "tokens_used": revised_translation.tokens_used,
                        "prompt_tokens": getattr(
                            revised_translation, "prompt_tokens", None
                        ),
                        "completion_tokens": getattr(
                            revised_translation, "completion_tokens", None
                        ),
                        "duration": getattr(revised_translation, "duration", None),
                        "cost": getattr(revised_translation, "cost", None),
                        "workflow_mode": self.workflow_mode.value,
                        "model_info": revised_translation.model_info,
                    },
                )

            # Call progress callback after Step 3 completion
            if self.progress_callback:
                await self.progress_callback(
                    "Translator Revision",
                    {
                        "status": "completed",
                        "tokens_used": revised_translation.tokens_used,
                        "duration": getattr(revised_translation, "duration", None),
                        "cost": getattr(revised_translation, "cost", None),
                        "model_info": getattr(revised_translation, "model_info", None),
                    },
                )

            # Calculate total cost
            total_cost = self._calculate_total_cost(
                initial_translation, editor_review, revised_translation
            )

            # Aggregate results
            duration = time.time() - start_time
            total_tokens = (
                initial_translation.tokens_used
                + editor_review.tokens_used
                + revised_translation.tokens_used
            )

            logger.info(
                f"Workflow {workflow_id} completed successfully in {duration:.2f}s"
            )
            logger.info(f"Total tokens used: {total_tokens}")

            # Calculate total cost
            total_cost = self._calculate_total_cost(
                initial_translation, editor_review, revised_translation
            )

            return self._aggregate_output(
                workflow_id=workflow_id,
                input_data=input_data,
                initial_translation=initial_translation,
                editor_review=editor_review,
                revised_translation=revised_translation,
                log_entries=log_entries,
                total_tokens=total_tokens,
                duration=duration,
                total_cost=total_cost,
            )

        except Exception as e:
            # Call progress callback on workflow failure
            if self.progress_callback:
                # Try to determine which step failed
                failed_step = "Unknown"
                if progress_tracker:
                    for step_name in reversed(progress_tracker.step_order):
                        step = progress_tracker.steps[step_name]
                        if step.status == StepStatus.IN_PROGRESS:
                            failed_step = step_name.replace("_", " ").title()
                            break

                await self.progress_callback(
                    failed_step, {"status": "failed", "error": str(e)}
                )

            if progress_tracker:
                # Determine which step failed based on current progress
                for step_name in reversed(progress_tracker.step_order):
                    step = progress_tracker.steps[step_name]
                    if step.status == StepStatus.IN_PROGRESS:
                        progress_tracker.fail_step(step_name, str(e))
                        break

            logger.error(f"Workflow {workflow_id} failed: {e}")
            raise WorkflowError(f"Translation workflow failed: {e}")

    async def _initial_translation(
        self, input_data: TranslationInput
    ) -> InitialTranslation:
        """
        Execute initial translation step.

        Args:
            input_data: Translation input data

        Returns:
            Initial translation with notes

        Raises:
            StepExecutionError: If translation step fails
        """
        try:
            step_config = self.workflow_steps["initial_translation"]

            # Prepare input context
            input_context = {
                "original_poem": input_data.original_poem,
                "source_lang": input_data.source_lang,
                "target_lang": input_data.target_lang,
            }

            # Execute step
            result = await self.step_executor.execute_initial_translation(
                input_data, step_config
            )

            # Extract translation and notes from XML
            if "output" in result:
                output_data = result["output"]
                translation = output_data.get("initial_translation", "")
                notes = output_data.get("initial_translation_notes", "")
                translated_poem_title = output_data.get("translated_poem_title", "")
                translated_poet_name = output_data.get("translated_poet_name", "")
            else:
                # Fallback: try to extract from raw response
                raw_content = (
                    result.get("metadata", {})
                    .get("raw_response", {})
                    .get("content_preview", "")
                )
                extracted = OutputParser.parse_initial_translation_xml(raw_content)
                translation = extracted.get("initial_translation", "")
                notes = extracted.get("initial_translation_notes", "")
                translated_poem_title = extracted.get("translated_poem_title", "")
                translated_poet_name = extracted.get("translated_poet_name", "")

            # Create InitialTranslation model
            usage = result.get("metadata", {}).get("usage", {})
            return InitialTranslation(
                initial_translation=translation,
                initial_translation_notes=notes,
                translated_poem_title=translated_poem_title,
                translated_poet_name=translated_poet_name,
                model_info={
                    "provider": step_config.provider,
                    "model": step_config.model,
                    "temperature": str(step_config.temperature),
                },
                tokens_used=usage.get("tokens_used", 0),
                prompt_tokens=usage.get("prompt_tokens"),
                completion_tokens=usage.get("completion_tokens"),
            )

        except Exception as e:
            logger.error(f"Initial translation step failed: {e}")
            raise StepExecutionError(f"Initial translation failed: {e}")

    async def _editor_review(
        self, input_data: TranslationInput, initial_translation: InitialTranslation
    ) -> EditorReview:
        """
        Execute editor review step.

        Args:
            input_data: Original translation input
            initial_translation: Initial translation to review

        Returns:
            Editor review with suggestions

        Raises:
            StepExecutionError: If review step fails
        """
        try:
            step_config = self.workflow_steps["editor_review"]

            # Execute step
            result = await self.step_executor.execute_editor_review(
                initial_translation, input_data, step_config
            )

            # Extract editor text from result
            editor_suggestions = ""
            if "output" in result:
                output_data = result["output"]
                # Try different possible field names
                editor_suggestions = output_data.get("editor_suggestions", "")

            if not editor_suggestions:
                # Fallback: use content directly
                editor_suggestions = (
                    result.get("metadata", {})
                    .get("raw_response", {})
                    .get("content_preview", "")
                )

            # Create EditorReview model
            usage = result.get("metadata", {}).get("usage", {})
            return EditorReview(
                editor_suggestions=editor_suggestions,
                model_info={
                    "provider": step_config.provider,
                    "model": step_config.model,
                    "temperature": str(step_config.temperature),
                },
                tokens_used=usage.get("tokens_used", 0),
                prompt_tokens=usage.get("prompt_tokens"),
                completion_tokens=usage.get("completion_tokens"),
            )

        except Exception as e:
            logger.error(f"Editor review step failed: {e}")
            raise StepExecutionError(f"Editor review failed: {e}")

    async def _translator_revision(
        self,
        input_data: TranslationInput,
        initial_translation: InitialTranslation,
        editor_review: EditorReview,
    ) -> RevisedTranslation:
        """
        Execute translator revision step.

        Args:
            input_data: Original translation input
            initial_translation: Initial translation
            editor_review: Editor review with suggestions

        Returns:
            Revised translation with notes

        Raises:
            StepExecutionError: If revision step fails
        """
        try:
            step_config = self.workflow_steps["translator_revision"]

            # Execute step
            result = await self.step_executor.execute_translator_revision(
                editor_review, input_data, initial_translation, step_config
            )

            # Extract revised translation and notes from XML
            if "output" in result:
                output_data = result["output"]
                translation = output_data.get("revised_translation", "")
                notes = output_data.get("revised_translation_notes", "")
                refined_translated_poem_title = output_data.get(
                    "refined_translated_poem_title", ""
                )
                refined_translated_poet_name = output_data.get(
                    "refined_translated_poet_name", ""
                )
            else:
                # Fallback: try to extract from raw response
                raw_content = (
                    result.get("metadata", {})
                    .get("raw_response", {})
                    .get("content_preview", "")
                )
                extracted = OutputParser.parse_revised_translation_xml(raw_content)
                translation = extracted.get("revised_translation", "")
                notes = extracted.get("revised_translation_notes", "")
                refined_translated_poem_title = extracted.get(
                    "refined_translated_poem_title", ""
                )
                refined_translated_poet_name = extracted.get(
                    "refined_translated_poet_name", ""
                )

            # Create RevisedTranslation model
            usage = result.get("metadata", {}).get("usage", {})
            return RevisedTranslation(
                revised_translation=translation,
                revised_translation_notes=notes,
                refined_translated_poem_title=refined_translated_poem_title,
                refined_translated_poet_name=refined_translated_poet_name,
                model_info={
                    "provider": step_config.provider,
                    "model": step_config.model,
                    "temperature": str(step_config.temperature),
                },
                tokens_used=usage.get("tokens_used", 0),
                prompt_tokens=usage.get("prompt_tokens"),
                completion_tokens=usage.get("completion_tokens"),
            )

        except Exception as e:
            logger.error(f"Translator revision step failed: {e}")
            raise StepExecutionError(f"Translator revision failed: {e}")

    def _aggregate_output(
        self,
        workflow_id: str,
        input_data: TranslationInput,
        initial_translation: InitialTranslation,
        editor_review: EditorReview,
        revised_translation: RevisedTranslation,
        log_entries: list,
        total_tokens: int,
        duration: float,
        total_cost: float,
    ) -> TranslationOutput:
        """
        Aggregate all workflow results into final output.

        Args:
            workflow_id: Unique workflow identifier
            input_data: Original input data
            initial_translation: Initial translation result
            editor_review: Editor review result
            revised_translation: Revised translation result
            log_entries: List of log entries
            total_tokens: Total tokens used
            duration: Total execution time

        Returns:
            Complete translation output
        """
        # Combine all log entries
        full_log = "\n".join(log_entries)

        # Add summary to log
        full_log += f"\n\n=== WORKFLOW SUMMARY ==="
        full_log += f"\nWorkflow ID: {workflow_id}"
        full_log += f"\nWorkflow Mode: {self.workflow_mode.value}"
        full_log += f"\nTotal tokens: {total_tokens}"
        full_log += f"\nDuration: {duration:.2f}s"
        full_log += f"\nCompleted: {datetime.now().isoformat()}"

        return TranslationOutput(
            workflow_id=workflow_id,
            input=input_data,
            initial_translation=initial_translation,
            editor_review=editor_review,
            revised_translation=revised_translation,
            total_tokens=total_tokens,
            duration_seconds=duration,
            workflow_mode=self.workflow_mode.value,
            total_cost=total_cost,
        )

    def _calculate_total_cost(
        self, initial_translation, editor_review, revised_translation
    ):
        """Calculate total cost of the workflow."""
        total_cost = 0.0

        # Calculate cost for each step if we have the pricing info
        for step_result in [initial_translation, editor_review, revised_translation]:
            if hasattr(step_result, "cost") and step_result.cost is not None:
                total_cost += step_result.cost

        return total_cost

    def _calculate_step_cost(
        self, provider: str, model: str, input_tokens: int, output_tokens: int
    ):
        """Calculate cost for a single step."""
        try:
            # Get pricing from configuration
            providers_config = self.llm_factory.providers_config

            if hasattr(providers_config, "pricing") and providers_config.pricing:
                pricing = providers_config.pricing

                # Get pricing for this provider and model
                if provider in pricing and model in pricing[provider]:
                    model_pricing = pricing[provider][model]
                    # Pricing is RMB per 1K tokens
                    input_cost = (input_tokens / 1000) * model_pricing.get("input", 0)
                    output_cost = (output_tokens / 1000) * model_pricing.get(
                        "output", 0
                    )
                    return input_cost + output_cost

            return 0.0
        except Exception as e:
            logger.warning(f"Failed to calculate cost for {provider}/{model}: {e}")
            return 0.0

    def __repr__(self) -> str:
        """String representation of the workflow."""
        return f"TranslationWorkflow(name='{self.config.name}', version='{self.config.version}')"

</code>

src/vpsweb/utils/storage.py:
<code>
"""
Storage Handler for Vox Poetica Studio Web.

This module provides file-based storage for translation outputs with proper
serialization/deserialization of Pydantic models and comprehensive error handling.
"""

import json
import logging
from pathlib import Path
from typing import List, Optional, Dict
from datetime import datetime

from ..models.translation import TranslationOutput
from .markdown_export import MarkdownExporter
from .filename_utils import (
    extract_poet_and_title,
    generate_translation_filename,
)

logger = logging.getLogger(__name__)


class StorageError(Exception):
    """Base exception for storage operations."""

    pass


class SaveError(StorageError):
    """Raised when saving a translation fails."""

    pass


class LoadError(StorageError):
    """Raised when loading a translation fails."""

    pass


class StorageHandler:
    """
    Handles storage operations for translation outputs.

    Provides methods for saving, loading, and listing translation outputs
    with proper file naming, JSON serialization, and error handling.
    """

    def __init__(self, output_dir: str = "outputs"):
        """
        Initialize the storage handler.

        Args:
            output_dir: Directory where translation files will be stored.
                       Will be created if it doesn't exist.

        Raises:
            StorageError: If the output directory cannot be created
        """
        self.output_dir = Path(output_dir)
        self.json_dir = self.output_dir / "json"

        try:
            # Create output directories if they don't exist
            self.output_dir.mkdir(parents=True, exist_ok=True)
            self.json_dir.mkdir(parents=True, exist_ok=True)
            logger.info(
                f"Storage handler initialized with output directory: {self.output_dir.absolute()}"
            )
            logger.info(f"JSON files will be stored in: {self.json_dir.absolute()}")

            # Initialize markdown exporter
            self.markdown_exporter = MarkdownExporter(output_dir)

        except Exception as e:
            logger.error(f"Failed to create output directory '{self.output_dir}': {e}")
            raise StorageError(
                f"Could not create output directory '{self.output_dir}': {e}"
            )

    def save_translation(
        self,
        output: TranslationOutput,
        workflow_mode: str = None,
        include_mode_tag: bool = False,
    ) -> Path:
        """
        Save a translation output to a timestamped JSON file.

        Args:
            output: TranslationOutput instance to save
            workflow_mode: Workflow mode used for the translation
            include_mode_tag: Whether to include workflow mode in filename

        Returns:
            Path to the saved file

        Raises:
            SaveError: If saving fails due to file I/O or serialization issues
        """
        try:
            # Generate timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # Extract poet and title information
            poet, title = extract_poet_and_title(
                output.input.original_poem, output.input.metadata
            )

            # Generate new descriptive filename
            filename = generate_translation_filename(
                poet=poet,
                title=title,
                source_lang=output.input.source_lang,
                target_lang=output.input.target_lang,
                timestamp=timestamp,
                workflow_id=output.workflow_id,
                workflow_mode=workflow_mode,
                file_format="json",
                is_log=False,
            )

            file_path = self.json_dir / filename

            # Convert to dictionary for JSON serialization
            output_dict = output.to_dict()

            # Save as formatted JSON
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(output_dict, f, ensure_ascii=False, indent=2)

            logger.info(f"Translation saved to: {file_path}")
            logger.debug(
                f"Workflow ID: {output.workflow_id}, Total tokens: {output.total_tokens}"
            )
            logger.info(f"Generated filename with poet '{poet}' and title '{title}'")

            return file_path

        except Exception as e:
            logger.error(
                f"JSON serialization failed for workflow {output.workflow_id}: {e}"
            )
            raise SaveError(f"Failed to serialize translation output: {e}")
        except IOError as e:
            logger.error(f"File I/O error while saving translation: {e}")
            raise SaveError(f"Failed to write translation file: {e}")
        except Exception as e:
            logger.error(f"Unexpected error while saving translation: {e}")
            raise SaveError(f"Failed to save translation: {e}")

    def load_translation(self, file_path: Path) -> TranslationOutput:
        """
        Load a translation output from a JSON file.

        Args:
            file_path: Path to the translation JSON file

        Returns:
            TranslationOutput instance

        Raises:
            LoadError: If loading fails due to file I/O, JSON parsing, or validation issues
        """
        try:
            # Validate file exists and is readable
            if not file_path.exists():
                raise LoadError(f"Translation file not found: {file_path}")

            if not file_path.is_file():
                raise LoadError(f"Path is not a file: {file_path}")

            # Load JSON data
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # Parse into TranslationOutput model
            translation_output = TranslationOutput.from_dict(data)

            logger.info(f"Translation loaded from: {file_path}")
            logger.debug(f"Workflow ID: {translation_output.workflow_id}")

            return translation_output

        except Exception as e:
            logger.error(f"JSON parsing failed for file {file_path}: {e}")
            raise LoadError(f"Invalid JSON format in translation file: {e}")
        except KeyError as e:
            logger.error(f"Missing required field in translation file {file_path}: {e}")
            raise LoadError(f"Translation file missing required field: {e}")
        except ValueError as e:
            logger.error(f"Data validation failed for file {file_path}: {e}")
            raise LoadError(f"Translation data validation failed: {e}")
        except IOError as e:
            logger.error(f"File I/O error while loading translation: {e}")
            raise LoadError(f"Failed to read translation file: {e}")
        except Exception as e:
            logger.error(f"Unexpected error while loading translation: {e}")
            raise LoadError(f"Failed to load translation: {e}")

    def list_translations(self) -> List[Path]:
        """
        List all translation files in the JSON subdirectory.

        Returns:
            List of Path objects for all translation JSON files

        Raises:
            StorageError: If directory listing fails
        """
        try:
            # Find all JSON files in json subdirectory
            # Support both new naming (poet-led) and legacy naming (translation_-prefixed)
            translation_files = list(self.json_dir.glob("*.json"))

            # Sort by modification time (newest first)
            translation_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

            logger.debug(
                f"Found {len(translation_files)} translation files in {self.json_dir}"
            )

            return translation_files

        except Exception as e:
            logger.error(f"Failed to list translation files in {self.json_dir}: {e}")
            raise StorageError(f"Failed to list translation files: {e}")

    def get_translation_by_id(self, workflow_id: str) -> Optional[TranslationOutput]:
        """
        Find and load a translation by workflow ID.

        Args:
            workflow_id: The workflow ID to search for

        Returns:
            TranslationOutput if found, None otherwise

        Raises:
            StorageError: If search operation fails
        """
        try:
            translation_files = self.list_translations()

            for file_path in translation_files:
                try:
                    translation = self.load_translation(file_path)
                    if translation.workflow_id == workflow_id:
                        logger.info(
                            f"Found translation with workflow ID: {workflow_id}"
                        )
                        return translation
                except LoadError:
                    # Skip files that can't be loaded
                    continue

            logger.debug(f"No translation found with workflow ID: {workflow_id}")
            return None

        except Exception as e:
            logger.error(f"Failed to search for translation with ID {workflow_id}: {e}")
            raise StorageError(f"Failed to search for translation: {e}")

    def delete_translation(self, file_path: Path) -> bool:
        """
        Delete a translation file.

        Args:
            file_path: Path to the translation file to delete

        Returns:
            True if deletion was successful, False otherwise

        Raises:
            StorageError: If deletion fails
        """
        try:
            # Ensure the file is within our output directory for safety
            if not file_path.is_relative_to(self.output_dir):
                logger.warning(
                    f"Attempted to delete file outside output directory: {file_path}"
                )
                return False

            if file_path.exists() and file_path.is_file():
                file_path.unlink()
                logger.info(f"Translation file deleted: {file_path}")
                return True
            else:
                logger.warning(f"Translation file not found for deletion: {file_path}")
                return False

        except Exception as e:
            logger.error(f"Failed to delete translation file {file_path}: {e}")
            raise StorageError(f"Failed to delete translation file: {e}")

    def save_translation_with_markdown(
        self,
        output: TranslationOutput,
        workflow_mode: str = None,
        include_mode_tag: bool = False,
    ) -> Dict[str, Path]:
        """
        Save a translation output to both JSON and markdown files.

        Args:
            output: TranslationOutput instance to save
            workflow_mode: Workflow mode used for the translation
            include_mode_tag: Whether to include workflow mode in filename

        Returns:
            Dictionary with paths to saved files:
            {
                'json': Path to JSON file,
                'markdown_final': Path to final translation markdown,
                'markdown_log': Path to full log markdown
            }

        Raises:
            SaveError: If saving fails due to file I/O or serialization issues
        """
        try:
            # Save JSON file (existing functionality)
            json_path = self.save_translation(output, workflow_mode, include_mode_tag)

            # Save markdown files (new functionality)
            markdown_paths = self.markdown_exporter.export_both(output)

            result = {
                "json": json_path,
                "markdown_final": Path(markdown_paths["final_translation"]),
                "markdown_log": Path(markdown_paths["full_log"]),
            }

            logger.info(f"Translation saved to multiple formats:")
            logger.info(f"  JSON: {result['json']}")
            logger.info(f"  Markdown (final): {result['markdown_final']}")
            logger.info(f"  Markdown (log): {result['markdown_log']}")

            return result

        except Exception as e:
            logger.error(f"Failed to save translation with markdown: {e}")
            raise SaveError(f"Failed to save translation with markdown: {e}")

    def get_storage_info(self) -> dict:
        """
        Get information about the storage directory.

        Returns:
            Dictionary with storage statistics

        Raises:
            StorageError: If statistics collection fails
        """
        try:
            translation_files = self.list_translations()
            total_size = sum(
                file_path.stat().st_size for file_path in translation_files
            )

            return {
                "output_directory": str(self.output_dir.absolute()),
                "total_files": len(translation_files),
                "total_size_bytes": total_size,
                "total_size_mb": round(total_size / (1024 * 1024), 2),
                "oldest_file": (
                    min(translation_files, key=lambda x: x.stat().st_mtime).name
                    if translation_files
                    else None
                ),
                "newest_file": (
                    max(translation_files, key=lambda x: x.stat().st_mtime).name
                    if translation_files
                    else None
                ),
            }

        except Exception as e:
            logger.error(f"Failed to get storage info: {e}")
            raise StorageError(f"Failed to get storage information: {e}")

    # Enhanced Poet-Based Organization Methods
    def save_translation_with_poet_dir(
        self,
        output: TranslationOutput,
        poet_name: str,
        workflow_mode: str = None,
        include_mode_tag: bool = False,
    ) -> Path:
        """
        Save a translation output to a poet-based subdirectory structure.

        Structure: outputs/json/poets/{poet_name}/{filename}.json
        Also saves latest copy to outputs/json/recent/ for fast access

        Args:
            output: TranslationOutput instance to save
            poet_name: Name of the poet for subdirectory organization
            workflow_mode: Workflow mode used for the translation
            include_mode_tag: Whether to include workflow mode in filename

        Returns:
            Path to the saved file in poet subdirectory

        Raises:
            SaveError: If saving fails due to file I/O or serialization issues
        """
        try:
            # Create poet subdirectory structure
            poets_dir = self.json_dir / "poets"
            poet_dir = poets_dir / poet_name
            recent_dir = self.json_dir / "recent"

            # Create directories if they don't exist
            poets_dir.mkdir(exist_ok=True)
            poet_dir.mkdir(exist_ok=True)
            recent_dir.mkdir(exist_ok=True)

            # Generate timestamp and filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            poet, title = extract_poet_and_title(
                output.input.original_poem, output.input.metadata
            )

            # Use provided poet_name or extracted poet
            effective_poet_name = poet_name or poet

            filename = generate_translation_filename(
                poet=effective_poet_name,
                title=title,
                source_lang=output.input.source_lang,
                target_lang=output.input.target_lang,
                timestamp=timestamp,
                workflow_id=output.workflow_id,
                workflow_mode=workflow_mode,
                file_format="json",
                is_log=False,
            )

            # Save to poet subdirectory
            poet_file_path = poet_dir / filename

            # Convert to dictionary for JSON serialization
            output_dict = output.to_dict()

            # Save to poet subdirectory
            with open(poet_file_path, "w", encoding="utf-8") as f:
                json.dump(output_dict, f, ensure_ascii=False, indent=2)

            # Also save to recent directory (keep latest 20)
            recent_filename = f"latest_{output.workflow_id}_{timestamp}.json"
            recent_file_path = recent_dir / recent_filename

            with open(recent_file_path, "w", encoding="utf-8") as f:
                json.dump(output_dict, f, ensure_ascii=False, indent=2)

            # Clean up old recent files (keep only 20 most recent)
            self._cleanup_recent_files(recent_dir, limit=20)

            logger.info(f"Translation saved to poet directory: {poet_file_path}")
            logger.info(f"Also saved to recent directory: {recent_file_path}")

            return poet_file_path

        except Exception as e:
            logger.error(f"Failed to save translation with poet directory: {e}")
            raise SaveError(f"Failed to save translation with poet directory: {e}")

    def get_poet_directories(self) -> List[str]:
        """
        Get list of all poet subdirectories.

        Returns:
            List of poet names that have subdirectories

        Raises:
            StorageError: If directory listing fails
        """
        try:
            poets_dir = self.json_dir / "poets"

            if not poets_dir.exists():
                return []

            poet_dirs = []
            for item in poets_dir.iterdir():
                if item.is_dir():
                    poet_dirs.append(item.name)

            return sorted(poet_dirs)

        except Exception as e:
            logger.error(f"Failed to get poet directories: {e}")
            raise StorageError(f"Failed to get poet directories: {e}")

    def get_recent_files(self, limit: int = 20) -> List[Path]:
        """
        Get list of most recent translation files.

        Args:
            limit: Maximum number of files to return

        Returns:
            List of paths to recent files, sorted by modification time

        Raises:
            StorageError: If file listing fails
        """
        try:
            recent_dir = self.json_dir / "recent"

            if not recent_dir.exists():
                return []

            files = []
            for file_path in recent_dir.glob("*.json"):
                if file_path.is_file():
                    files.append(file_path)

            # Sort by modification time (newest first)
            files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

            return files[:limit]

        except Exception as e:
            logger.error(f"Failed to get recent files: {e}")
            raise StorageError(f"Failed to get recent files: {e}")

    def get_poet_files(self, poet_name: str) -> List[Path]:
        """
        Get all translation files for a specific poet.

        Args:
            poet_name: Name of the poet

        Returns:
            List of paths to poet's translation files

        Raises:
            StorageError: If file listing fails
        """
        try:
            poet_dir = self.json_dir / "poets" / poet_name

            if not poet_dir.exists():
                return []

            files = []
            for file_path in poet_dir.glob("*.json"):
                if file_path.is_file():
                    files.append(file_path)

            # Sort by modification time (newest first)
            files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

            return files

        except Exception as e:
            logger.error(f"Failed to get poet files for '{poet_name}': {e}")
            raise StorageError(f"Failed to get poet files for '{poet_name}': {e}")

    def _cleanup_recent_files(self, recent_dir: Path, limit: int = 20):
        """
        Clean up recent directory to keep only the specified number of files.

        Args:
            recent_dir: Path to recent directory
            limit: Maximum number of files to keep
        """
        try:
            files = []
            for file_path in recent_dir.glob("*.json"):
                if file_path.is_file():
                    files.append(file_path)

            # Sort by modification time (newest first)
            files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

            # Remove excess files
            for file_path in files[limit:]:
                file_path.unlink()
                logger.debug(f"Removed old recent file: {file_path}")

        except Exception as e:
            logger.warning(f"Failed to cleanup recent files: {e}")

    def __repr__(self) -> str:
        """String representation of the storage handler."""
        return f"StorageHandler(output_dir='{self.output_dir}')"

</code>

src/vpsweb/models/translation.py:
<code>
"""
Translation data models for Vox Poetica Studio Web.

This module contains Pydantic models for the complete translation workflow,
matching the exact structure specified in vpts.yml.
"""

from typing import Optional, List, Dict, Any
from pydantic import BaseModel, Field, field_validator, ConfigDict
from datetime import datetime
from enum import Enum
import json


class Language(str, Enum):
    """Supported languages for translation."""

    ENGLISH = "English"
    CHINESE = "Chinese"
    POLISH = "Polish"


class TranslationInput(BaseModel):
    """Input for translation workflow, matching vpts.yml specification."""

    original_poem: str = Field(
        ...,
        min_length=1,
        max_length=2000,  # From vpts.yml: max_length: 2000
        description="Original poem text for translation",
    )
    source_lang: Language = Field(
        ..., description="Source language (English, Chinese, or Polish)"
    )
    target_lang: Language = Field(
        ..., description="Target language (English or Chinese only)"
    )
    metadata: Optional[Dict[str, Any]] = Field(
        None, description="Additional metadata about the poem (author, title, etc.)"
    )

    @field_validator("source_lang", "target_lang")
    @classmethod
    def validate_languages(cls, v):
        """Validate language choices against vpts.yml specification."""
        return v

    @field_validator("target_lang")
    @classmethod
    def validate_target_language(cls, v, info):
        """Ensure target language is only English or Chinese (from vpts.yml)."""
        if v not in [Language.ENGLISH, Language.CHINESE]:
            raise ValueError("Target language must be either English or Chinese")
        return v

    @field_validator("target_lang")
    @classmethod
    def validate_source_target_distinct(cls, v, info):
        """Ensure source and target languages are different."""
        if "source_lang" in info.data and v == info.data["source_lang"]:
            raise ValueError("Source and target languages must be different")
        return v

    model_config = ConfigDict(use_enum_values=True)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return self.model_dump()

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "TranslationInput":
        """Create from dictionary."""
        return cls(**data)


class InitialTranslation(BaseModel):
    """Output from initial translation step with XML structure from vpts.yml."""

    initial_translation: str = Field(..., description="The translated poem text")
    initial_translation_notes: str = Field(
        ...,
        description="Translator's explanation of translation choices (200-300 words)",
    )
    translated_poem_title: str = Field(
        ..., description="The translated poem title in target language"
    )
    translated_poet_name: str = Field(
        ..., description="The translated poet name in target language"
    )
    timestamp: datetime = Field(
        default_factory=datetime.now,
        description="Timestamp when translation was created",
    )
    model_info: Dict[str, str] = Field(
        ..., description="Model provider and version information"
    )
    tokens_used: int = Field(
        ..., ge=0, description="Number of tokens used for this translation"
    )
    prompt_tokens: Optional[int] = Field(
        None, ge=0, description="Number of input tokens used for this translation"
    )
    completion_tokens: Optional[int] = Field(
        None, ge=0, description="Number of output tokens used for this translation"
    )
    duration: Optional[float] = Field(
        None, ge=0.0, description="Time taken for initial translation in seconds"
    )
    cost: Optional[float] = Field(
        None, ge=0.0, description="Cost in RMB for this translation step"
    )

    @field_validator("initial_translation_notes")
    @classmethod
    def validate_notes_length(cls, v):
        """Validate notes length matches vpts.yml specification (200-300 words)."""
        # Note: Removed warning for shorter notes during development
        return v

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary with ISO format timestamp."""
        data = self.model_dump()
        data["timestamp"] = self.timestamp.isoformat()
        return data

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "InitialTranslation":
        """Create from dictionary, parsing ISO timestamp."""
        if "timestamp" in data and isinstance(data["timestamp"], str):
            data["timestamp"] = datetime.fromisoformat(data["timestamp"])
        return cls(**data)


class EditorReview(BaseModel):
    """Output from editor review step with structured suggestions from vpts.yml."""

    editor_suggestions: str = Field(
        default="", description="Structured editor suggestions extracted from XML"
    )
    timestamp: datetime = Field(
        default_factory=datetime.now, description="Timestamp when review was created"
    )
    model_info: Dict[str, str] = Field(
        ..., description="Model provider and version information"
    )
    tokens_used: int = Field(
        ..., ge=0, description="Number of tokens used for this review"
    )
    prompt_tokens: Optional[int] = Field(
        None, ge=0, description="Number of input tokens used for this review"
    )
    completion_tokens: Optional[int] = Field(
        None, ge=0, description="Number of output tokens used for this review"
    )
    duration: Optional[float] = Field(
        None, ge=0.0, description="Time taken for editor review in seconds"
    )
    cost: Optional[float] = Field(
        None, ge=0.0, description="Cost in RMB for this editor review step"
    )

    def get_suggestions_list(self) -> List[str]:
        """Extract numbered suggestions from the editor's text."""
        import re

        text_to_search = self.editor_suggestions
        # Look for numbered suggestions like "1. [suggestion text]"
        suggestions = re.findall(r"^\s*(\d+)\.\s*(.+)$", text_to_search, re.MULTILINE)
        return [suggestion[1].strip() for suggestion in suggestions]

    def get_overall_assessment(self) -> str:
        """Extract the overall assessment from the editor's text."""
        import re

        text_to_search = self.editor_suggestions
        # Look for the overall assessment after the suggestions
        assessment_match = re.search(
            r"overall assessment:?\s*(.+)", text_to_search, re.IGNORECASE | re.DOTALL
        )
        if assessment_match:
            return assessment_match.group(1).strip()
        return ""

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary with ISO format timestamp."""
        data = {
            "editor_suggestions": self.editor_suggestions,
            "timestamp": self.timestamp.isoformat(),
            "model_info": self.model_info,
            "tokens_used": self.tokens_used,
        }
        return data

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "EditorReview":
        """Create from dictionary, parsing ISO timestamp."""
        if "timestamp" in data and isinstance(data["timestamp"], str):
            data["timestamp"] = datetime.fromisoformat(data["timestamp"])
        return cls(**data)


class RevisedTranslation(BaseModel):
    """Output from translator revision step with XML structure from vpts.yml."""

    revised_translation: str = Field(..., description="The final revised translation")
    revised_translation_notes: str = Field(
        ..., description="Explanation of key changes and decisions (200-300 words)"
    )
    refined_translated_poem_title: str = Field(
        ..., description="The refined translated poem title in target language"
    )
    refined_translated_poet_name: str = Field(
        ..., description="The refined translated poet name in target language"
    )
    timestamp: datetime = Field(
        default_factory=datetime.now, description="Timestamp when revision was created"
    )
    model_info: Dict[str, str] = Field(
        ..., description="Model provider and version information"
    )
    tokens_used: int = Field(
        ..., ge=0, description="Number of tokens used for this revision"
    )
    prompt_tokens: Optional[int] = Field(
        None, ge=0, description="Number of input tokens used for this revision"
    )
    completion_tokens: Optional[int] = Field(
        None, ge=0, description="Number of output tokens used for this revision"
    )
    duration: Optional[float] = Field(
        None, ge=0.0, description="Time taken for translator revision in seconds"
    )
    cost: Optional[float] = Field(
        None, ge=0.0, description="Cost in RMB for this translator revision step"
    )

    @field_validator("revised_translation_notes")
    @classmethod
    def validate_revision_notes_length(cls, v):
        """Validate notes length matches vpts.yml specification (200-300 words)."""
        word_count = len(v.split())
        # Allow any length of revision notes without warning
        return v

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary with ISO format timestamp."""
        data = self.model_dump()
        data["timestamp"] = self.timestamp.isoformat()
        return data

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "RevisedTranslation":
        """Create from dictionary, parsing ISO timestamp."""
        if "timestamp" in data and isinstance(data["timestamp"], str):
            data["timestamp"] = datetime.fromisoformat(data["timestamp"])
        return cls(**data)


class TranslationOutput(BaseModel):
    """Complete workflow output matching the vpts.yml congregation format."""

    workflow_id: str = Field(
        ..., description="Unique identifier for this translation workflow"
    )
    input: TranslationInput = Field(..., description="Original input to the workflow")
    initial_translation: InitialTranslation = Field(
        ..., description="Initial translation with notes"
    )
    editor_review: EditorReview = Field(
        ..., description="Editor's detailed suggestions and feedback"
    )
    revised_translation: RevisedTranslation = Field(
        ..., description="Final revised translation with notes"
    )
    total_tokens: int = Field(
        ..., ge=0, description="Total tokens used across all workflow steps"
    )
    duration_seconds: float = Field(
        ..., ge=0.0, description="Total workflow execution time in seconds"
    )
    workflow_mode: Optional[str] = Field(
        None,
        description="Workflow mode used for this translation (reasoning, non_reasoning, hybrid)",
    )
    total_cost: Optional[float] = Field(
        None, ge=0.0, description="Total cost in RMB for the entire workflow"
    )

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary with proper serialization."""
        return {
            "workflow_id": self.workflow_id,
            "input": self.input.to_dict(),
            "initial_translation": self.initial_translation.to_dict(),
            "editor_review": self.editor_review.to_dict(),
            "revised_translation": self.revised_translation.to_dict(),
            "total_tokens": self.total_tokens,
            "duration_seconds": self.duration_seconds,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "TranslationOutput":
        """Create from dictionary with proper deserialization."""
        return cls(
            workflow_id=data["workflow_id"],
            input=TranslationInput.from_dict(data["input"]),
            initial_translation=InitialTranslation.from_dict(
                data["initial_translation"]
            ),
            editor_review=EditorReview.from_dict(data["editor_review"]),
            revised_translation=RevisedTranslation.from_dict(
                data["revised_translation"]
            ),
            total_tokens=data["total_tokens"],
            duration_seconds=data["duration_seconds"],
        )

    def save_to_file(self, filepath: str, format: str = "json") -> None:
        """Save the translation output to a file."""
        if format.lower() == "json":
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(self.to_dict(), f, ensure_ascii=False, indent=2)
        else:
            raise ValueError(f"Unsupported format: {format}")

    @classmethod
    def load_from_file(cls, filepath: str, format: str = "json") -> "TranslationOutput":
        """Load translation output from a file."""
        if format.lower() == "json":
            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)
            return cls.from_dict(data)
        else:
            raise ValueError(f"Unsupported format: {format}")

</code>



the translation results are in the .json file. one issue is that in "editor_review" json structure, it only has "tokens_used": 3630, but no "prompt_tokens","completion_tokens", "duration", and "cost", as for "initial_translation" and "revised_translation". I include here 3 python scripts implementing the logic.