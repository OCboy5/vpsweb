config/models.yaml:
<code>
# Vox Poetica Studio Web - LLM Provider Configuration
# Enhanced with model classification and reasoning capabilities

providers:
  tongyi:
    api_key_env: "TONGYI_API_KEY"
    base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
    type: "openai_compatible"
    models:
      - "qwen3-max-latest"
      - "qwen-plus-latest"
    default_model: "qwen-plus-latest"
    capabilities:
      reasoning: false

  deepseek:
    api_key_env: "DEEPSEEK_API_KEY"
    base_url: "https://api.deepseek.com/v1"
    type: "openai_compatible"
    models:
      - "deepseek-reasoner"      # Reasoning model
      - "deepseek-chat"          # Non-reasoning model
    default_model: "deepseek-reasoner"
    capabilities:
      reasoning: true
  
  siliconflow:
    api_key_env: "SILICONFLOW_API_KEY"
    base_url: "https://api.siliconflow.cn/v1/"
    type: "openai_compatible"
    models:
      - "deepseek-ai/DeepSeek-V3.2-Exp"  # Reasoning model
    default_model: "deepseek-ai/DeepSeek-V3.2-Exp"
    capabilities:
      reasoning: true

# Model classification for automatic prompt selection
model_classification:
  reasoning_models:
    - "deepseek-reasoner"
    - "deepseek-ai/DeepSeek-V3.2-Exp"
  non_reasoning_models:
    - "qwen3-max-latest"
    - "qwen-plus-latest"
    - "deepseek-chat"

# Global provider settings
provider_settings:
  timeout: 180.0
  max_retries: 3
  retry_delay: 1.0
  request_timeout: 30.0
  connection_pool_size: 10

# Reasoning model specific settings
reasoning_settings:
  timeout: 300.0              # Extended for reasoning time
  max_retries: 2              # Fewer retries (expensive)
  request_timeout: 60.0       # Longer individual requests

# Pricing information (RMB per 1K tokens)
pricing:
  tongyi:
    qwen3-max-latest:
      input: 0.006      # ¥0.006 per 1K input tokens
      output: 0.024     # ¥0.024 per 1K output tokens
    qwen-plus-latest:
      input: 0.0008      # ¥0.0008 per 1K input tokens
      output: 0.002     # ¥0.002 per 1K output tokens
  deepseek:
    deepseek-reasoner:
      input: 0.002      # ¥0.002 per 1K input tokens
      output: 0.003     # ¥0.003 per 1K output tokens
    deepseek-chat:
      input: 0.002      # ¥0.002 per 1K input tokens
      output: 0.003     # ¥0.003 per 1K output tokens
  siliconflow:
    deepseek-ai/DeepSeek-V3.2-Exp:
      input: 0.002      # ¥0.002 per 1K input tokens
      output: 0.003     # ¥0.003 per 1K output tokens

# WeChat Translation Notes LLM Configuration
wechat_translation_notes:
  # Primary provider for translation notes synthesis
  primary_provider: "deepseek"
  primary_model: "deepseek-reasoner"

  # Fallback provider if primary fails
  fallback_provider: "tongyi"
  fallback_model: "qwen-plus-latest"

  # Model configurations for different types
  models:
    reasoning:
      provider: "deepseek"
      model: "deepseek-reasoner"
      prompt_template: "wechat_article_notes_reasoning"
      temperature: 0.1
      max_tokens: 8192
      timeout: 180

    non_reasoning:
      provider: "tongyi"
      model: "qwen-plus-latest"
      prompt_template: "wechat_article_notes_nonreasoning"
      temperature: 0.3
      max_tokens: 8192
      timeout: 60

# BBR Generation Configuration
bbr_generation:
  provider: "tongyi"
  model: "qwen-plus-latest"
  temperature: 0.3
  max_tokens: 16384
  prompt_template: "background_briefing_report"
  timeout: 180.0
  retry_attempts: 2

  
</code>

config/wechat.yaml:
<code>
# WeChat Official Account Configuration
# This file contains configuration for WeChat article publishing functionality

# WeChat API Credentials (REQUIRED)
# Get these from your WeChat Official Account backend: https://mp.weixin.qq.com/
appid: "${WECHAT_APPID}"
secret: "${WECHAT_SECRET}"

# API Configuration
base_url: "https://api.weixin.qq.com"

# Token Management
token_cache_path: "outputs/.cache/wechat_token.json"

# Request Timeouts (seconds)
timeouts:
  connect: 5.0
  read: 20.0

# Retry Configuration
retry:
  attempts: 3
  backoff: "exponential"
  base_delay: 0.5

# Article Generation Settings
article_generation:
  include_translation_notes: true
  copyright_text: "【著作权声明】\n本译文与译注完全由知韵(VoxPoetica)AI工具生成制作，仅供学习交流使用。原作品版权归原作者所有，如有侵权请联系删除。翻译内容未经授权，不得转载、不得用于商业用途。若需引用，请注明出处。"
  #article_template: "default"  # References config/html_templates/wechat_articles/default.html
  article_template: "codebuddy"  # References config/html_templates/wechat_articles/codebuddy.html

  # Cover image configuration
  default_cover_image_path: "config/html_templates/cover_image_big.jpg"  # Default cover image path (relative to project root)
  default_local_cover_image_name: "cover_image_big.jpg"  # Default local cover image filename in wechat_articles/slug directory

  # LLM configuration for translation notes synthesis
  # Note: LLM model configuration is now in config/models.yaml under wechat_translation_notes
  model_type: "non_reasoning"  # Model type: reasoning or non_reasoning

# Publishing Settings
publishing:
  # Save to draft folder by default (safer for manual review)
  save_as_draft: true
  # Enable comments on published articles
  enable_comments: true
  # Allow only fans to comment
  fans_only_comments: false
  # Auto-publish from draft (disable for manual control)
  auto_publish: false

# Output Settings
output:
  # Default output directory for WeChat articles
  default_directory: "outputs/wechat_articles"

  # File naming patterns
  filename_patterns:
    html: "article.html"
    metadata: "metadata.json"
    publish_result: "publish_result.json"

# Content Settings
content:
  # Default author name
  default_author: "知韵VoxPoetica"

  # Title format
  title_format: "【知韵译诗】{poem_title}（{poet_name}）"

  # Digest settings
  digest:
    min_length: 100
    max_length: 120
    auto_generate: true

# WeChat API Error Codes
api_error_codes:
  invalid_access_token: 40001
  rate_limit_exceeded: 45009
  invalid_media_id: 40007

# Development Settings
development:
  # Enable debug logging
  debug: false

  # Dry run mode (no actual API calls)
  dry_run: false

  # Verbose output
  verbose: false

  # Mock API responses for testing
  mock_api: false
</code>

config/repository.yaml:
<code>
# VPSWeb Repository System Configuration
# Version: 0.3 - Localhost-only deployment

repository:
  # SQLite database configuration (WAL mode for performance)
  database_url: "sqlite+aiosqlite:///./repository_root/repo.db"
  repo_root: "./repository_root"
  auto_create_dirs: true
  enable_wal_mode: true  # Write-Ahead Logging for better concurrency

# Security configuration (localhost-only, v0.3)
security:
  # No authentication required for localhost deployment
  # Authentication (Argon2 + BasicAuth) moved to v0.4+
  require_auth: false
  session_timeout: 3600  # 1 hour (for future v0.4+)
  api_key_required: false

# Server configuration (localhost-only)
server:
  host: "127.0.0.1"
  port: 8000
  reload: false
  debug: false
  workers: 1  # Single worker for SQLite

# Data configuration
data:
  default_language: "en"
  supported_languages: ["en", "zh-Hans", "zh-Hant"]
  default_license: "CC-BY-4.0"
  max_poem_length: 10000
  max_translation_length: 20000
  enable_validation: true

# Background task configuration
background_tasks:
  enabled: true
  max_concurrent_jobs: 3
  job_timeout: 300  # 5 minutes
  cleanup_interval: 3600  # 1 hour

  # Enhanced task management
  max_retry_attempts: 3
  retry_delay: 60  # Base retry delay in seconds
  max_queue_size: 100
  queue_check_interval: 1.0  # Queue processing interval in seconds
  task_retention_hours: 24

  # Task-specific timeouts
  task_timeouts:
    translation: 600  # 10 minutes
    maintenance: 300  # 5 minutes
    cleanup: 120  # 2 minutes
    backup: 1800  # 30 minutes

  # Resource limits
  max_memory_mb: 512
  max_cpu_percent: 80.0

  # Queue management
  enable_priority_queue: true
  pause_on_overload: true
  overload_threshold: 0.9

# Logging configuration
logging:
  level: "INFO"
  format: "json"  # Structured JSON logging
  file_path: "./repository_root/logs/repository.log"
  max_file_size: "10MB"
  backup_count: 5

# Integration configuration
integration:
  vpsweb:
    # Use existing vpsweb configuration
    config_path: "config"
    workflow_mode: "hybrid"  # Default workflow mode
    timeout: 300  # 5 minutes for translation jobs

# Web UI configuration
web_ui:
  title: "VPSWeb Repository"
  description: "Central repository for poetry translations"
  theme: "default"  # Basic theme, advanced themes in v0.4+
  items_per_page: 20
  enable_search: true
  enable_comparison: true

# Performance configuration
performance:
  database_pool_size: 1  # SQLite doesn't need pooling
  query_timeout: 30
  enable_query_cache: true
  cache_size: 1000

  # Performance monitoring
  enable_metrics: true
  metrics_retention_days: 7

  # Memory management
  gc_threshold: 1000
  max_memory_usage: 1024  # MB

# Task monitoring configuration
task_monitoring:
  enable_health_checks: true
  health_check_interval: 60  # seconds
  enable_task_metrics: true
  metrics_history_size: 1000

  # Alert thresholds
  failed_task_threshold: 5
  queue_size_warning: 50
  memory_usage_warning: 0.8

  # Task lifecycle monitoring
  track_task_lifecycle: true
  enable_task_tracing: false

# System resource configuration
system_resources:
  enable_resource_monitoring: true
  resource_check_interval: 30  # seconds

  # CPU thresholds
  max_cpu_usage: 80.0  # percentage
  cpu_warning_threshold: 70.0

  # Memory thresholds
  max_memory_usage_percent: 85.0
  memory_warning_threshold: 75.0

  # Disk thresholds
  max_disk_usage_percent: 90.0
  disk_warning_threshold: 80.0
</code>

config/default.yaml:
<code>
# Vox Poetica Studio Web - Main Configuration
# Enhanced with workflow mode support for reasoning and non-reasoning models

# Workflow mode selection: 'reasoning', 'non_reasoning', or 'hybrid'
workflow_mode: "hybrid"  # Default to balanced approach

# Translation Strategy Dials (Global Configuration)
translation_strategy:
  # Cultural adaptation approach
  adaptation_level: "balanced"  # options: "foreignizing" | "balanced" | "domesticating"

  # How to handle repetitions in the original poem
  repetition_policy: "strict"  # options: "strict" | "adaptive"

  # Policy for semantic additions (when translator adds meaning not in source)
  additions_policy: "forbid"  # options: "forbid" | "allow_with_alt"

  # Prosody and rhythm target for translation
  prosody_target: "free verse, cadence-aware"  # e.g., "free verse, cadence-aware", "preserve rhyme", "free verse"

  # Optional exemplars (few-shot examples) - typically left empty
  few_shots: ""  # Can contain example translations for few-shot learning

workflow:
  name: "vox_poetica_translation"
  version: "2.0.0"

  # Pure Reasoning Workflow Configuration
  reasoning_workflow:
    initial_translation:
      provider: "deepseek"
      model: "deepseek-reasoner"
      temperature: 0.2
      max_tokens: 16384
      prompt_template: "initial_translation_reasoning"
      timeout: 300.0
      retry_attempts: 2
      stop: ["</initial_translation_notes>"]

    # editor_review:
    #   provider: "deepseek"
    #   model: "deepseek-reasoner"
    #   temperature: 0.1
    #   max_tokens: 16384
    #   prompt_template: "editor_review_reasoning"
    #   timeout: 300.0
    #   retry_attempts: 2
    #   stop: ["</editor_suggestions>"]

    editor_review:
      provider: "siliconflow"
      model: "deepseek-ai/DeepSeek-V3.2-Exp"
      temperature: 0.1
      max_tokens: 16384
      prompt_template: "editor_review_reasoning"
      timeout: 300.0
      retry_attempts: 2
      stop: ["</editor_suggestions>"]  

    translator_revision:
      provider: "deepseek"
      model: "deepseek-reasoner"
      temperature: 0.15
      max_tokens: 16384
      prompt_template: "translator_revision_reasoning"
      timeout: 300.0
      retry_attempts: 2
      stop: ["</revised_translation_notes>"]

  # Pure Non-Reasoning Workflow Configuration
  non_reasoning_workflow:
    initial_translation:
      provider: "tongyi"
      model: "qwen-plus-latest"
      temperature: 0.7
      max_tokens: 8192
      prompt_template: "initial_translation_nonreasoning"
      timeout: 180.0
      retry_attempts: 3
      stop: ["</initial_translation_notes>"]

    editor_review:
      provider: "tongyi"
      model: "qwen-plus-latest"
      temperature: 0.3
      max_tokens: 8192
      prompt_template: "editor_review_nonreasoning"
      timeout: 180.0
      retry_attempts: 3
      stop: ["</editor_suggestions>"]

    translator_revision:
      provider: "tongyi"
      model: "qwen-plus-latest"
      temperature: 0.2
      max_tokens: 8192
      prompt_template: "translator_revision_nonreasoning"
      timeout: 180.0
      retry_attempts: 3
      stop: ["</revised_translation_notes>"]

  # Hybrid Workflow Configuration (Non-Reasoning → Reasoning → Non-Reasoning)
  hybrid_workflow:
    initial_translation:
      provider: "tongyi"
      model: "qwen-plus-latest"
      temperature: 0.7
      max_tokens: 8192
      prompt_template: "initial_translation_nonreasoning"
      timeout: 180.0
      retry_attempts: 3
      stop: ["</initial_translation_notes>"]

    # editor_review:
    #   provider: "deepseek"
    #   model: "deepseek-reasoner"
    #   temperature: 0.1
    #   max_tokens: 16384
    #   prompt_template: "editor_review_reasoning"
    #   timeout: 300.0
    #   retry_attempts: 2
    #   stop: ["</editor_suggestions>"]

    editor_review:
      provider: "siliconflow"
      model: "deepseek-ai/DeepSeek-V3.2-Exp"
      temperature: 0.1
      max_tokens: 16384
      prompt_template: "editor_review_reasoning"
      timeout: 300.0
      retry_attempts: 2
      stop: ["</editor_suggestions>"] 

    translator_revision:
      provider: "tongyi"
      model: "qwen-plus-latest"
      temperature: 0.2
      max_tokens: 8192
      prompt_template: "translator_revision_nonreasoning"
      timeout: 180.0
      retry_attempts: 3
      stop: ["</revised_translation_notes>"]

  
storage:
  output_dir: "outputs"
  format: "json"
  include_timestamp: true
  pretty_print: true
  workflow_mode_tag: true  # Include workflow mode in output filename

  # Default output directories
  wechat_articles_dir: "outputs/wechat_articles"
  cache_dir: "outputs/.cache"

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "vpsweb.log"
  max_file_size: 10485760  # 10MB
  backup_count: 5
  log_reasoning_tokens: true  # Track reasoning model token usage

# Performance monitoring
monitoring:
  track_latency: true
  track_token_usage: true
  track_cost: true  # Estimate API costs
  compare_workflows: true  # Enable A/B comparison

# System-wide settings
system:
  # Token management
  token_refresh_buffer: 300  # seconds (5 minutes) - refresh token before expiry
  default_token_expiry: 7200  # seconds (2 hours) - default WeChat token expiry

  # Preview lengths for logs and UI
  preview_lengths:
    input_preview: 100  # characters
    response_preview: 200  # characters
    editor_preview: 200  # characters

  # Default messages
  default_digest: "诗歌翻译作品，展现中英文学之美，传递文化精髓。"

  # Translation notes synthesis parameters
  translation_notes:
    reasoning:
      temperature: 0.1
      max_tokens: 2000
      timeout: 60  # seconds
    non_reasoning:
      temperature: 0.3
      max_tokens: 1500
      timeout: 45  # seconds
</code>



the four yaml Config files in config/. They are the configuration files for the 
workflow and the models.But I think they are confusing and inflexible to expand, and the 
configuration is kind of replicating themselves a lot. pls think hard how to improve them. let me know if there's anything unclear to you.