# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

**VPSWeb (Vox Poetica Studio Web)** is a professional AI-powered poetry translation platform that implements a collaborative Translatorâ†’Editorâ†’Translator workflow to produce high-fidelity translations between English and Chinese (and other languages).

**Tech Stack**: Python, Poetry, FastAPI, SQLAlchemy, Pydantic, AsyncIO, OpenAI-compatible APIs, YAML configuration, Tailwind CSS

# Development Guidelines
## Philosophy
### Core Beliefs
-Â **Incremental progress over big bangs**Â - Small changes that compile and pass tests
-Â **Learning from existing code**Â - Study and plan before implementing
-Â **Pragmatic over dogmatic**Â - Adapt to project reality
-Â **Clear intent over clever code**Â - Be boring and obvious
### Simplicity Means
-Â Single responsibility per function/class
-Â Avoid premature abstractions
-Â No clever tricks, choose the boring solution
-Â If you need to explain it, it's too complex

## Process
### 1. Planning & Staging
Use Brainstorming and Debating Guidelines to make non-trivial decisions in the plan.
Break complex work into 3-5 stages. Document inÂ `IMPLEMENTATION_PLAN.md`:
```markdown
## Stage N: [Name]
**Goal**: [Specific deliverable]
**Success Criteria**: [Testable outcomes]
**Tests**: [Specific test cases]
**Status**: [Not Started|In Progress|Complete]
```
-Â Update status as you progress
### 2. Implementation Flow
1.Â **Understand**Â - Study existing patterns in codebase
2.Â **Test**Â - Write test first (red)
3.Â **Implement**Â - Minimal code to pass (green)
4.Â **Refactor**Â - Clean up with tests passing
5.Â **Commit**Â - With clear message linking to plan
### 3. When Stuck (After 3 Attempts)
**CRITICAL**: Maximum 3 attempts per issue, then STOP.
1.Â **Document what failed**:Â  Â 
- What you triedÂ  Â 
- Specific error messagesÂ  Â 
- Why you think it failed
2.Â **Research alternatives**:Â  Â 
- Find 2-3 similar implementationsÂ  Â 
- Note different approaches used
3.Â **Question fundamentals**:Â  Â 
- Is this the right abstraction level?Â  Â 
- Can this be split into smaller problems?Â  Â 
- Is there a simpler approach entirely?
4.Â **Try different angle**:Â  Â 
- Different library/framework feature?Â  Â 
- Different architectural pattern?Â  Â 
- Remove abstraction instead of adding?
## Technical Standards
### Architecture Principles
-Â **Composition over inheritance**
Â - Use dependency injection
-Â **Interfaces over singletons**
Â - Enable testing and flexibility
-Â **Explicit over implicit**
Â - Clear data flow and dependencies
-Â **Test-driven when possible**
Â - Never disable tests, fix them
### Code Quality
-Â **Every commit must**:
Â  - Compile successfully
Â  - Pass all existing tests
Â  - Include tests for new functionality
  - Follow project formatting/linting
-Â **Before committing**:
Â  - Run formatters/linters
Â  - Self-review changes
Â  - Ensure commit message explains "why"
### Error Handling
-Â Fail fast with descriptive messages
-Â Include context for debugging
-Â Handle errors at appropriate level
-Â Never silently swallow exceptions
## Decision Framework
When multiple valid approaches exist, choose based on:
1.Â **Testability**Â - Can I easily test this?
2.Â **Readability**Â - Will someone understand this in 6 months?
3.Â **Consistency**Â - Does this match project patterns?
4.Â **Simplicity**Â - Is this the simplest solution that works?
5.Â **Reversibility**Â - How hard to change later?
## Project Integration
### Learning the Codebase
-Â Find 3 similar features/components
-Â Identify common patterns and conventions
-Â Use same libraries/utilities when possible
-Â Follow existing test patterns
### Tooling
-Â Use project's existing build system
-Â Use project's test framework
-Â Use project's formatter/linter settings
-Â Don't introduce new tools without strong justification
## Quality Gates
### Definition of Done
-Â [ ] Tests written and passing
-Â [ ] Code follows project conventions
-Â [ ] No linter/formatter warnings
-Â [ ] Commit messages are clear
-Â [ ] Implementation matches plan
-Â [ ] No TODOs without issue numbers
### Test Guidelines
-Â Test behavior, not implementation
-Â One assertion per test when possible
-Â Clear test names describing scenario
-Â Use existing test utilities/helpers
-Â Tests should be deterministic
## Important Reminders
**NEVER**:
-Â UseÂ `--no-verify`Â to bypass commit hooks
-Â Disable tests instead of fixing them
-Â Commit code that doesn't compile
-Â Make assumptions - verify with existing code
**ALWAYS**:
-Â Commit working code incrementally
-Â Update plan documentation as you go
-Â Learn from existing implementations
-Â Stop after 3 failed attempts and reassess
-Â Always use context7 when I need code generation, setup or configuration steps, or library/API documentation. This means you should automatically use the Context7 MCPtools to resolve library id and get library docs without me having to explicitly ask.

### Brainstorming and Debating Guidelines

#### **Effective Brainstorming Session**
**Purpose**: Generate diverse approaches before settling on a solution

**Process**:
1. **Divergent Thinking**: Generate as many approaches as possible without judgment
2. **Consider Multiple Dimensions**:
   - Technical approaches (different patterns, libraries, architectures)
   - Implementation strategies (phased vs. big-bang, monolithic vs. modular)
   - User experience considerations
   - Maintenance and scalability implications
3. **Document All Ideas**: Capture every approach with brief pros/cons
4. **Encourage Creativity**: Consider unconventional solutions that might reveal new insights

**Brainstorming Output Format**:
```markdown
## Brainstorming: [Decision Name]

### Approach 1: [Approach Name]
- **Description**: Brief description of the approach
- **Pros**: List of advantages
- **Cons**: List of disadvantages
- **Risks**: Potential risks and challenges
- **Best For**: Situations where this approach excels

### Approach 2: [Alternative Approach]
- **Description**: Brief description of the approach
- **Pros**: List of advantages
- **Cons**: List of disadvantages
- **Risks**: Potential risks and challenges
- **Best For**: Situations where this approach excels

### Approach 3: [Another Alternative]
- ... continue for all approaches considered
```

#### **Structured Debating Session**
**Purpose**: Critical evaluation of approaches through systematic comparison

**Process**:
1. **Establish Criteria**: Define evaluation criteria relevant to the decision
   - Technical complexity and risk
   - Implementation time and effort
   - Maintenance burden
   - Scalability and performance
   - Alignment with project goals
   - User impact

2. **Systematic Comparison**: Compare each approach against all criteria
3. **Challenge Assumptions**: Question underlying assumptions for each approach
4. **Identify Hidden Risks**: Look for risks that might not be immediately obvious
5. **Consider Context**: Evaluate approaches based on specific project constraints

**Debating Output Format**:
```markdown
## Debating Session: [Decision Name]

### Evaluation Criteria
- **Technical Risk**: [Description of what this means for the decision]
- **Implementation Complexity**: [Description]
- **Maintenance Impact**: [Description]
- **Scalability**: [Description]
- **User Experience**: [Description]

### Approach Comparison Matrix
| Approach | Technical Risk | Complexity | Maintenance | Scalability | UX | Overall Score |
|----------|----------------|------------|-------------|-------------|----|--------------|
| Approach 1 | High/Medium/Low | High/Medium/Low | High/Medium/Low | High/Medium/Low | High/Medium/Low | [Score] |
| Approach 2 | High/Medium/Low | High/Medium/Low | High/Medium/Low | High/Medium/Low | High/Medium/Low | [Score] |

### Key Debating Points
- **Point 1**: [Critical discussion point with resolution]
- **Point 2**: [Another critical discussion point with resolution]
- **Point 3**: [Additional point where approaches differ significantly]

### Risk Analysis
- **Approach 1 Risks**: [Specific risks with mitigation strategies]
- **Approach 2 Risks**: [Specific risks with mitigation strategies]

### Final Recommendation
- **Selected Approach**: [Which approach and why]
- **Key Reasons**: [Top 3 reasons for this choice]
- **Mitigation Strategy**: [How we'll address the chosen approach's risks]
```

#### **Integration with Plan Phase**
**Enhanced Plan Presentation**:
When presenting plan for user approval, include:
1. **Brainstorming Summary**: Brief overview of approaches considered
2. **Key Debating Points**: Most important trade-offs discussed
3. **Decision Rationale**: Clear explanation of why chosen approach is optimal
4. **Risk Mitigation**: How identified risks will be addressed

### 2. Decision Classification

**Trivial Decisions** (Direct Implementation):
- Simple bug fixes with clear solutions
- Adding comments or improving documentation
- Single-file refactorings that don't affect interfaces
- Configuration value updates

**Non-Trivial Decisions**:
- Adding new workflow steps or modes
- Changing API interfaces or data models
- Modifying core workflow orchestration
- Adding new LLM providers or integration patterns
- Changes affecting multiple configuration files
- Architectural refactoring
- New feature implementations

### Key Configuration Files
- `config/default.yaml`: Main workflow configuration
- `config/models.yaml`: Provider configurations
- `config/wechat.yaml`: WeChat Official Account integration
- `config/repository.yaml`: Central Repository and WebUI configurations

### Configuration Structure
- YAML format for readability
- Environment variable substitution using `${VAR_NAME}` syntax
- Pydantic model validation
- Support for workflow modes: reasoning, non_reasoning, hybrid

## Quick References

### Release Management
ðŸš¨ **CRITICAL**: All releases MUST follow the strict workflow in `VERSION_WORKFLOW.md`.
